

============================== 2022-10-13 15:36:33.314656 | 74955d92-896c-4df0-b018-e65466e66ab9 ==============================
[0m15:36:33.314673 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:36:33.315102 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m15:36:33.315277 [debug] [MainThread]: Tracking: tracking
[0m15:36:33.337780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c63e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c639a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c63790>]}
[0m15:36:34.073099 [debug] [MainThread]: Executing "git --help"
[0m15:36:34.086734 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:36:34.087797 [debug] [MainThread]: STDERR: "b''"
[0m15:36:34.097228 [debug] [MainThread]: Acquiring new databricks connection "debug"
[0m15:36:34.099977 [debug] [MainThread]: Using databricks connection "debug"
[0m15:36:34.100768 [debug] [MainThread]: On debug: select 1 as id
[0m15:36:34.101025 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:36:48.259077 [debug] [MainThread]: Databricks adapter: failed to connect: Error during request to server: [Errno 60] Operation timed out
[0m15:36:48.260044 [debug] [MainThread]: Databricks adapter: <class 'databricks.sql.exc.RequestError'>: Error during request to server: [Errno 60] Operation timed out
[0m15:36:48.260927 [debug] [MainThread]: Databricks adapter: attempt: 1/30
[0m15:36:48.261494 [debug] [MainThread]: Databricks adapter: bounded-retry-delay: None
[0m15:36:48.261817 [debug] [MainThread]: Databricks adapter: elapsed-seconds: 14.12965703010559/900.0
[0m15:36:48.262089 [debug] [MainThread]: Databricks adapter: error-message: [Errno 60] Operation timed out
[0m15:36:48.262341 [debug] [MainThread]: Databricks adapter: http-code: None
[0m15:36:48.262666 [debug] [MainThread]: Databricks adapter: method: OpenSession
[0m15:36:48.262937 [debug] [MainThread]: Databricks adapter: no-retry-reason: non-retryable error
[0m15:36:48.263216 [debug] [MainThread]: Databricks adapter: original-exception: [Errno 60] Operation timed out
[0m15:36:48.263501 [debug] [MainThread]: Databricks adapter: query-id: None
[0m15:36:48.263954 [debug] [MainThread]: Databricks adapter: session-id: None
[0m15:36:48.264444 [debug] [MainThread]: Databricks adapter: Error while running:
select 1 as id
[0m15:36:48.264739 [debug] [MainThread]: Databricks adapter: Database Error
  failed to connect
[0m15:36:48.266441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11830c1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec640>]}
[0m15:36:48.267854 [debug] [MainThread]: Flushing usage events
[0m15:36:49.088028 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-10-13 15:37:36.571874 | bec9ae3d-25bf-48df-9268-ba966e4f444d ==============================
[0m15:37:36.571886 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:37:36.572989 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'project_name': 'jaffle_shop', 'skip_profile_setup': False, 'which': 'init', 'indirect_selection': 'eager'}
[0m15:37:36.573345 [debug] [MainThread]: Tracking: tracking
[0m15:37:36.594823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3a7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3acd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3a5b0>]}
[0m15:37:36.598963 [info ] [MainThread]: Setting up your profile.
[0m15:37:41.445410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f5c48e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3a100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3a880>]}
[0m15:37:41.446750 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 15:39:05.134024 | ad3065fe-4ed4-4e16-b798-fca40e102a89 ==============================
[0m15:39:05.134036 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:39:05.134914 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m15:39:05.135097 [debug] [MainThread]: Tracking: tracking
[0m15:39:05.151841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113677e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136779a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113677790>]}
[0m15:39:05.862548 [debug] [MainThread]: Executing "git --help"
[0m15:39:05.872291 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m15:39:05.873523 [debug] [MainThread]: STDERR: "b''"
[0m15:39:05.880786 [debug] [MainThread]: Acquiring new databricks connection "debug"
[0m15:39:05.883486 [debug] [MainThread]: Using databricks connection "debug"
[0m15:39:05.884000 [debug] [MainThread]: On debug: select 1 as id
[0m15:39:05.884478 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:39:13.005509 [debug] [MainThread]: SQL status: OK in 7.12 seconds
[0m15:39:13.009024 [debug] [MainThread]: On debug: Close
[0m15:39:13.903970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123cea040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123cea940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123cea220>]}
[0m15:39:13.905194 [debug] [MainThread]: Flushing usage events
[0m15:39:14.736599 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-10-13 15:39:23.588131 | 01912b24-c57c-49cf-852c-fb39ae8a0738 ==============================
[0m15:39:23.588191 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:39:23.589373 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:39:23.589617 [debug] [MainThread]: Tracking: tracking
[0m15:39:23.614611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208e75e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208e77f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208e7ee0>]}
[0m15:39:23.637052 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m15:39:23.637629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120938d90>]}
[0m15:39:23.689717 [debug] [MainThread]: Parsing macros/statement.sql
[0m15:39:23.695791 [debug] [MainThread]: Parsing macros/catalog.sql
[0m15:39:23.698195 [debug] [MainThread]: Parsing macros/adapters.sql
[0m15:39:23.722088 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m15:39:23.730291 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m15:39:23.730924 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m15:39:23.734866 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m15:39:23.755342 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m15:39:23.762740 [debug] [MainThread]: Parsing macros/adapters.sql
[0m15:39:23.813726 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m15:39:23.816915 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m15:39:23.827697 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m15:39:23.828379 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m15:39:23.831932 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m15:39:23.859721 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m15:39:23.865306 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m15:39:23.873605 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m15:39:23.880096 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m15:39:23.880699 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m15:39:23.882194 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m15:39:23.887826 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m15:39:23.889971 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m15:39:23.904156 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m15:39:23.904913 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m15:39:23.905692 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m15:39:23.907293 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m15:39:23.911459 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m15:39:23.914181 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m15:39:23.916074 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m15:39:23.932387 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m15:39:23.946249 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m15:39:23.958665 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m15:39:23.963849 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m15:39:23.966008 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m15:39:23.968717 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m15:39:23.973410 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m15:39:23.995096 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m15:39:23.998469 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m15:39:24.014266 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m15:39:24.030789 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m15:39:24.038923 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m15:39:24.044677 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m15:39:24.052181 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m15:39:24.054349 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m15:39:24.058779 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m15:39:24.061225 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m15:39:24.068781 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m15:39:24.092400 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m15:39:24.095480 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m15:39:24.100095 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m15:39:24.102738 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m15:39:24.104634 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m15:39:24.106545 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m15:39:24.111324 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m15:39:24.113632 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m15:39:24.123693 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m15:39:24.136476 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m15:39:24.137550 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m15:39:24.139671 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m15:39:24.141467 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m15:39:24.145283 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m15:39:24.148214 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m15:39:24.149686 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m15:39:24.151353 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m15:39:24.152659 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m15:39:24.156258 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m15:39:24.159554 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m15:39:24.160938 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m15:39:24.163959 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m15:39:24.165264 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m15:39:24.166496 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m15:39:24.167535 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m15:39:24.168445 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m15:39:24.174406 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m15:39:24.175422 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m15:39:24.177122 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m15:39:24.179436 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m15:39:24.180774 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m15:39:24.183434 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m15:39:24.186392 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m15:39:24.206843 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m15:39:24.210363 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m15:39:24.225512 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m15:39:24.230538 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m15:39:24.239375 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m15:39:24.251398 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m15:39:24.659602 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m15:39:24.674443 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:39:24.677865 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m15:39:24.724618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a7a0d0>]}
[0m15:39:24.732742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120938100>]}
[0m15:39:24.733150 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m15:39:24.733414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a3d760>]}
[0m15:39:24.735149 [info ] [MainThread]: 
[0m15:39:24.735821 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:39:24.736878 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m15:39:24.748865 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m15:39:24.749194 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:39:24.749353 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:39:27.530393 [debug] [ThreadPool]: SQL status: OK in 2.78 seconds
[0m15:39:27.549982 [debug] [ThreadPool]: On list_schemas: Close
[0m15:39:28.420510 [debug] [ThreadPool]: Acquiring new databricks connection "create__jaffle_shop_dbt"
[0m15:39:28.421060 [debug] [ThreadPool]: Acquiring new databricks connection "create__jaffle_shop_dbt"
[0m15:39:28.421373 [debug] [ThreadPool]: Creating schema "_ReferenceKey(database=None, schema='jaffle_shop_dbt', identifier=None)"
[0m15:39:28.428257 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:39:28.428615 [debug] [ThreadPool]: Using databricks connection "create__jaffle_shop_dbt"
[0m15:39:28.428807 [debug] [ThreadPool]: On create__jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "create__jaffle_shop_dbt"} */
create schema if not exists jaffle_shop_dbt
  
[0m15:39:28.429134 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:39:30.397343 [debug] [ThreadPool]: SQL status: OK in 1.97 seconds
[0m15:39:30.400512 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:39:30.400958 [debug] [ThreadPool]: On create__jaffle_shop_dbt: ROLLBACK
[0m15:39:30.401271 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:39:30.401560 [debug] [ThreadPool]: On create__jaffle_shop_dbt: Close
[0m15:39:30.875382 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m15:39:30.889536 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:39:30.890020 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m15:39:30.890401 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m15:39:30.890684 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:39:31.952707 [debug] [ThreadPool]: SQL status: OK in 1.06 seconds
[0m15:39:31.957580 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m15:39:31.957956 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:39:31.958230 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m15:39:32.413611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ac1520>]}
[0m15:39:32.414438 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:39:32.414780 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:39:32.415423 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:39:32.415941 [info ] [MainThread]: 
[0m15:39:32.430386 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m15:39:32.430900 [info ] [Thread-1  ]: 1 of 3 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m15:39:32.431631 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m15:39:32.431927 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m15:39:32.432233 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m15:39:32.435500 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m15:39:32.436948 [debug] [Thread-1  ]: finished collecting timing info
[0m15:39:32.437262 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m15:39:32.473000 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m15:39:32.473808 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:39:32.474094 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m15:39:32.474218 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m15:39:32.474335 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:39:34.958474 [debug] [Thread-1  ]: SQL status: OK in 2.48 seconds
[0m15:39:34.979937 [debug] [Thread-1  ]: finished collecting timing info
[0m15:39:34.980282 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m15:39:34.980483 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:39:34.980653 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m15:39:35.438620 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120bdfaf0>]}
[0m15:39:35.448605 [info ] [Thread-1  ]: 1 of 3 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 3.01s]
[0m15:39:35.450508 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m15:39:35.451503 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m15:39:35.452881 [info ] [Thread-1  ]: 2 of 3 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m15:39:35.454733 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m15:39:35.455269 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m15:39:35.455807 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m15:39:35.460808 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m15:39:35.461774 [debug] [Thread-1  ]: finished collecting timing info
[0m15:39:35.462394 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m15:39:35.468411 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m15:39:35.469295 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:39:35.469708 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m15:39:35.469933 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m15:39:35.470142 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:39:36.966357 [debug] [Thread-1  ]: SQL status: OK in 1.5 seconds
[0m15:39:36.969554 [debug] [Thread-1  ]: finished collecting timing info
[0m15:39:36.969975 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m15:39:36.970267 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:39:36.970526 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m15:39:37.448600 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ba8af0>]}
[0m15:39:37.449786 [info ] [Thread-1  ]: 2 of 3 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 1.99s]
[0m15:39:37.450908 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m15:39:37.453080 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m15:39:37.454110 [info ] [Thread-1  ]: 3 of 3 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m15:39:37.455450 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m15:39:37.455846 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m15:39:37.456197 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m15:39:37.463451 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m15:39:37.464715 [debug] [Thread-1  ]: finished collecting timing info
[0m15:39:37.465056 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m15:39:37.521130 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m15:39:37.521837 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:39:37.521996 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m15:39:37.522122 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (

    select * from jaffle_shop_dbt.stg_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
[0m15:39:37.522241 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:40:05.286393 [debug] [Thread-1  ]: SQL status: OK in 27.76 seconds
[0m15:40:05.807976 [debug] [Thread-1  ]: finished collecting timing info
[0m15:40:05.808379 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m15:40:05.808620 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:40:05.808840 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m15:40:06.277473 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01912b24-c57c-49cf-852c-fb39ae8a0738', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c498b0>]}
[0m15:40:06.278924 [info ] [Thread-1  ]: 3 of 3 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 28.82s]
[0m15:40:06.280026 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m15:40:06.282772 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:40:06.283338 [debug] [MainThread]: On master: ROLLBACK
[0m15:40:06.283748 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:40:06.816856 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:40:06.817474 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:40:06.817838 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:40:06.818202 [debug] [MainThread]: On master: ROLLBACK
[0m15:40:06.818531 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:40:06.818851 [debug] [MainThread]: On master: Close
[0m15:40:07.281262 [info ] [MainThread]: 
[0m15:40:07.282573 [info ] [MainThread]: Finished running 2 view models, 1 table model in 0 hours 0 minutes and 42.55 seconds (42.55s).
[0m15:40:07.283665 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:40:07.284399 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m15:40:07.303461 [info ] [MainThread]: 
[0m15:40:07.304270 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:40:07.305022 [info ] [MainThread]: 
[0m15:40:07.305492 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m15:40:07.306066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b90250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b902e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120bba700>]}
[0m15:40:07.306630 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 15:59:52.704947 | 0e53d0c5-2a15-4758-b862-fbbbdaaa0230 ==============================
[0m15:59:52.705020 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:52.707220 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:59:52.707490 [debug] [MainThread]: Tracking: tracking
[0m15:59:52.730833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c39730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c39a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c396a0>]}
[0m15:59:52.840988 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m15:59:52.841674 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m15:59:52.841984 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/marts/core/fct_orders.sql
[0m15:59:52.870816 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m15:59:52.893116 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m15:59:52.911601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e6c0d0>]}
[0m15:59:52.926222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e061c0>]}
[0m15:59:52.926763 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m15:59:52.927325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e06100>]}
[0m15:59:52.929084 [info ] [MainThread]: 
[0m15:59:52.929835 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:59:52.931881 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m15:59:52.947556 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m15:59:52.948163 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:59:52.948627 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:54.527846 [debug] [ThreadPool]: SQL status: OK in 1.58 seconds
[0m15:59:54.536198 [debug] [ThreadPool]: On list_schemas: Close
[0m15:59:55.395977 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m15:59:55.403224 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:55.403484 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m15:59:55.403647 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m15:59:55.403786 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:59:56.849444 [debug] [ThreadPool]: SQL status: OK in 1.45 seconds
[0m15:59:56.854873 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m15:59:56.855284 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:59:56.855575 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m15:59:57.311315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121dce310>]}
[0m15:59:57.311886 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:57.312199 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:59:57.312720 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:59:57.313188 [info ] [MainThread]: 
[0m15:59:57.319242 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m15:59:57.319692 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m15:59:57.320308 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m15:59:57.320523 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m15:59:57.320724 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m15:59:57.322725 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m15:59:57.323309 [debug] [Thread-1  ]: finished collecting timing info
[0m15:59:57.323823 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m15:59:57.355929 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m15:59:57.356567 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:57.356805 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m15:59:57.356968 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m15:59:57.357089 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:59:59.173371 [debug] [Thread-1  ]: SQL status: OK in 1.82 seconds
[0m15:59:59.204148 [debug] [Thread-1  ]: finished collecting timing info
[0m15:59:59.204448 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m15:59:59.204634 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:59:59.204790 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m15:59:59.674613 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e7c250>]}
[0m15:59:59.675724 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.35s]
[0m15:59:59.676588 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m15:59:59.677331 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m15:59:59.678483 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m15:59:59.679803 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m15:59:59.680348 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m15:59:59.680685 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m15:59:59.684541 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m15:59:59.685493 [debug] [Thread-1  ]: finished collecting timing info
[0m15:59:59.685819 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m15:59:59.690835 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m15:59:59.691576 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:59.691825 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m15:59:59.692018 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m15:59:59.692204 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:01.325646 [debug] [Thread-1  ]: SQL status: OK in 1.63 seconds
[0m16:00:01.329224 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:01.329604 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:00:01.329841 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:01.330039 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:00:01.797247 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f4f820>]}
[0m16:00:01.797765 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.12s]
[0m16:00:01.798204 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:00:01.798606 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:00:01.799199 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:00:01.799745 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:00:01.799934 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:00:01.800080 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:00:01.801945 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:00:01.802836 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:01.803116 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:00:01.809253 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:00:01.810426 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:01.810834 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:00:01.811038 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:00:01.811202 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:03.165893 [debug] [Thread-1  ]: SQL status: OK in 1.35 seconds
[0m16:00:03.170327 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:03.170840 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:00:03.171163 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:03.171438 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:00:03.629309 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e72e80>]}
[0m16:00:03.630518 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 1.83s]
[0m16:00:03.631462 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:00:03.632213 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:00:03.633432 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:00:03.634421 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:00:03.634767 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:00:03.635176 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:00:03.640584 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:00:03.641369 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:03.641671 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:00:03.693659 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:00:03.694288 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:03.694540 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:00:03.694666 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (

    select * from jaffle_shop_dbt.stg_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
[0m16:00:03.694797 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:09.216117 [debug] [Thread-1  ]: SQL status: OK in 5.52 seconds
[0m16:00:09.239199 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:09.239519 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:00:09.239715 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:09.239884 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:00:09.709692 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121fa2e80>]}
[0m16:00:09.710920 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 6.08s]
[0m16:00:09.712069 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:00:09.712957 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:00:09.714517 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:00:09.716389 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:00:09.716978 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:00:09.717343 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:00:09.731846 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:00:09.732750 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:09.733056 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:00:09.740085 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:00:09.740739 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:09.740947 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:00:09.741115 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)
[0m16:00:09.741306 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:10.876064 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)
[0m16:00:10.876462 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
mismatched input '<EOF>' expecting {'(', 'DESC', 'DESCRIBE', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES'}(line 43, pos 1)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)
-^^^

[0m16:00:10.876927 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '<EOF>' expecting {'(', 'DESC', 'DESCRIBE', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES'}(line 43, pos 1)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)
-^^^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input '<EOF>' expecting {'(', 'DESC', 'DESCRIBE', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES'}(line 43, pos 1)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)
-^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:298)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:159)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:88)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:106)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:337)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:337)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m16:00:10.877301 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xd2\xbc\xe5\x1f$xI\x1e\xa7\xe4\x81\x05\xecLb\x81'
[0m16:00:10.877747 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:10.878007 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:00:10.878215 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:10.878547 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:00:11.385878 [debug] [Thread-1  ]: Runtime Error in model fct_orders (models/marts/core/fct_orders.sql)
  
  mismatched input '<EOF>' expecting {'(', 'DESC', 'DESCRIBE', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES'}(line 43, pos 1)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */
  
        create or replace table jaffle_shop_dbt.fct_orders
      
      
      using delta
      
      
      
      
      
      
      as
        with orders as (
  
      select * from jaffle_shop_dbt.stg_orders
  
  ),
  payments as (
  
      select * from jaffle_shop_dbt.stg_payments
  
  ),
  orders_customer as (
      select 
          order_id,
          customer_id
      from orders
  ),
  orders_payments as (
      select 
          order_id,
          amount
      from payments
  ),
  final as (
      select 
          orders_customer.order_id, 
          orders_customer.customer_id,
          orders_payments.amount
      from orders_payment
      left join orders_customer using (customer_id)
  )
  -^^^
  
[0m16:00:11.386895 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e53d0c5-2a15-4758-b862-fbbbdaaa0230', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f546d0>]}
[0m16:00:11.388053 [error] [Thread-1  ]: 5 of 5 ERROR creating table model jaffle_shop_dbt.fct_orders ................... [[31mERROR[0m in 1.67s]
[0m16:00:11.388939 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:00:11.391372 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:00:11.391823 [debug] [MainThread]: On master: ROLLBACK
[0m16:00:11.392140 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:00:11.859116 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:00:11.859741 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:11.860110 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:00:11.860394 [debug] [MainThread]: On master: ROLLBACK
[0m16:00:11.860767 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:00:11.861125 [debug] [MainThread]: On master: Close
[0m16:00:12.319263 [info ] [MainThread]: 
[0m16:00:12.320086 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 19.39 seconds (19.39s).
[0m16:00:12.321398 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:00:12.322091 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m16:00:12.339712 [info ] [MainThread]: 
[0m16:00:12.340141 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:00:12.340558 [info ] [MainThread]: 
[0m16:00:12.340987 [error] [MainThread]: [33mRuntime Error in model fct_orders (models/marts/core/fct_orders.sql)[0m
[0m16:00:12.341318 [error] [MainThread]:   
[0m16:00:12.341635 [error] [MainThread]:   mismatched input '<EOF>' expecting {'(', 'DESC', 'DESCRIBE', 'FROM', 'MAP', 'REDUCE', 'SELECT', 'TABLE', 'VALUES'}(line 43, pos 1)
[0m16:00:12.341932 [error] [MainThread]:   
[0m16:00:12.342185 [error] [MainThread]:   == SQL ==
[0m16:00:12.342601 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */
[0m16:00:12.343294 [error] [MainThread]:   
[0m16:00:12.343832 [error] [MainThread]:         create or replace table jaffle_shop_dbt.fct_orders
[0m16:00:12.344389 [error] [MainThread]:       
[0m16:00:12.344807 [error] [MainThread]:       
[0m16:00:12.345232 [error] [MainThread]:       using delta
[0m16:00:12.345471 [error] [MainThread]:       
[0m16:00:12.345701 [error] [MainThread]:       
[0m16:00:12.345931 [error] [MainThread]:       
[0m16:00:12.346346 [error] [MainThread]:       
[0m16:00:12.346624 [error] [MainThread]:       
[0m16:00:12.346865 [error] [MainThread]:       
[0m16:00:12.347092 [error] [MainThread]:       as
[0m16:00:12.347317 [error] [MainThread]:         with orders as (
[0m16:00:12.347545 [error] [MainThread]:   
[0m16:00:12.347778 [error] [MainThread]:       select * from jaffle_shop_dbt.stg_orders
[0m16:00:12.348026 [error] [MainThread]:   
[0m16:00:12.348259 [error] [MainThread]:   ),
[0m16:00:12.348486 [error] [MainThread]:   payments as (
[0m16:00:12.348710 [error] [MainThread]:   
[0m16:00:12.348961 [error] [MainThread]:       select * from jaffle_shop_dbt.stg_payments
[0m16:00:12.349171 [error] [MainThread]:   
[0m16:00:12.349385 [error] [MainThread]:   ),
[0m16:00:12.349609 [error] [MainThread]:   orders_customer as (
[0m16:00:12.349822 [error] [MainThread]:       select 
[0m16:00:12.350031 [error] [MainThread]:           order_id,
[0m16:00:12.350244 [error] [MainThread]:           customer_id
[0m16:00:12.350524 [error] [MainThread]:       from orders
[0m16:00:12.350951 [error] [MainThread]:   ),
[0m16:00:12.351222 [error] [MainThread]:   orders_payments as (
[0m16:00:12.351602 [error] [MainThread]:       select 
[0m16:00:12.351845 [error] [MainThread]:           order_id,
[0m16:00:12.352163 [error] [MainThread]:           amount
[0m16:00:12.352518 [error] [MainThread]:       from payments
[0m16:00:12.352924 [error] [MainThread]:   ),
[0m16:00:12.353346 [error] [MainThread]:   final as (
[0m16:00:12.353593 [error] [MainThread]:       select 
[0m16:00:12.353936 [error] [MainThread]:           orders_customer.order_id, 
[0m16:00:12.354375 [error] [MainThread]:           orders_customer.customer_id,
[0m16:00:12.354788 [error] [MainThread]:           orders_payments.amount
[0m16:00:12.355032 [error] [MainThread]:       from orders_payment
[0m16:00:12.355407 [error] [MainThread]:       left join orders_customer using (customer_id)
[0m16:00:12.355635 [error] [MainThread]:   )
[0m16:00:12.355832 [error] [MainThread]:   -^^^
[0m16:00:12.356030 [error] [MainThread]:   
[0m16:00:12.356255 [info ] [MainThread]: 
[0m16:00:12.356487 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m16:00:12.356937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f82970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ef1d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f11820>]}
[0m16:00:12.357346 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:00:53.329023 | 88b568ad-915a-4dd0-9682-44d0988f4fe8 ==============================
[0m16:00:53.329079 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:00:53.330217 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:00:53.330685 [debug] [MainThread]: Tracking: tracking
[0m16:00:53.349140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a5195e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a5197f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a519ee0>]}
[0m16:00:53.404188 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:00:53.404819 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m16:00:53.421846 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m16:00:53.447767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a75e0d0>]}
[0m16:00:53.456172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a6f0280>]}
[0m16:00:53.456540 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:00:53.456826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a6f01c0>]}
[0m16:00:53.458292 [info ] [MainThread]: 
[0m16:00:53.458895 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:00:53.459916 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:00:53.472122 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:00:53.472462 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:00:53.472644 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:00:54.637751 [debug] [ThreadPool]: SQL status: OK in 1.17 seconds
[0m16:00:54.647293 [debug] [ThreadPool]: On list_schemas: Close
[0m16:00:55.106320 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:00:55.119095 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:55.119469 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:00:55.119729 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:00:55.119954 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:00:56.277981 [debug] [ThreadPool]: SQL status: OK in 1.16 seconds
[0m16:00:56.284259 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:00:56.284652 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:00:56.284939 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:00:56.751674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a6c4220>]}
[0m16:00:56.752635 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:56.753171 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:00:56.754097 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:00:56.754790 [info ] [MainThread]: 
[0m16:00:56.763153 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:00:56.763907 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:00:56.765049 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:00:56.765439 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:00:56.765756 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:00:56.768783 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:00:56.769641 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:56.769971 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:00:56.806131 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:00:56.806806 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:56.806990 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:00:56.807128 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:00:56.807261 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:58.348408 [debug] [Thread-1  ]: SQL status: OK in 1.54 seconds
[0m16:00:58.379975 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:58.380311 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:00:58.380511 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:58.380668 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:00:58.845843 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a81bdc0>]}
[0m16:00:58.846875 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.08s]
[0m16:00:58.847889 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:00:58.848396 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:00:58.849181 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:00:58.850812 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:00:58.851300 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:00:58.851624 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:00:58.862931 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:00:58.863774 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:58.864244 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:00:58.869573 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:00:58.870297 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:58.870749 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:00:58.871010 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:00:58.871201 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:01:00.508943 [debug] [Thread-1  ]: SQL status: OK in 1.64 seconds
[0m16:01:00.514546 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:00.515035 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:01:00.515331 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:01:00.515732 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:01:00.973448 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7ef430>]}
[0m16:01:00.974363 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.12s]
[0m16:01:00.975629 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:01:00.976561 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:01:00.977856 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:01:00.978837 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:01:00.979203 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:01:00.979476 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:01:00.982630 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:01:00.983395 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:00.983782 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:01:00.988442 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:01:00.989102 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:00.989466 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:01:00.989692 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:01:00.989893 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:01:02.558082 [debug] [Thread-1  ]: SQL status: OK in 1.57 seconds
[0m16:01:02.564086 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:02.564583 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:01:02.564934 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:01:02.565210 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:01:03.030293 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a61d400>]}
[0m16:01:03.031032 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 2.05s]
[0m16:01:03.031765 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:01:03.032308 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:01:03.033463 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:01:03.034406 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:01:03.034658 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:01:03.034873 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:01:03.038729 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:01:03.039405 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:03.039711 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:01:03.090523 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:01:03.091140 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:03.091367 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:01:03.091494 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (

    select * from jaffle_shop_dbt.stg_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
[0m16:01:03.091613 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:01:08.394266 [debug] [Thread-1  ]: SQL status: OK in 5.3 seconds
[0m16:01:08.416111 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:08.416566 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:01:08.416846 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:01:08.417097 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:01:08.878235 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7ffb50>]}
[0m16:01:08.879120 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 5.84s]
[0m16:01:08.880013 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:01:08.880605 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:01:08.881655 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:01:08.883128 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:01:08.883576 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:01:08.883851 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:01:08.888502 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:01:08.889276 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:08.889616 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:01:08.902628 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:01:08.903609 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:08.903926 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:01:08.904106 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)

select * from final
[0m16:01:08.904262 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:01:10.041230 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payment
    left join orders_customer using (customer_id)
)

select * from final
[0m16:01:10.041805 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: orders_payment; line 41 pos 9
[0m16:01:10.042113 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: orders_payment; line 41 pos 9
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: orders_payment; line 41 pos 9
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m16:01:10.042385 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"\xf6'K\xe9acBj\xad\x8b^\x1bY1\xb1]"
[0m16:01:10.042900 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:10.043231 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:01:10.043507 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:01:10.043761 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:01:10.595785 [debug] [Thread-1  ]: Runtime Error in model fct_orders (models/marts/core/fct_orders.sql)
  Table or view not found: orders_payment; line 41 pos 9
[0m16:01:10.596499 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '88b568ad-915a-4dd0-9682-44d0988f4fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a8b93a0>]}
[0m16:01:10.597171 [error] [Thread-1  ]: 5 of 5 ERROR creating table model jaffle_shop_dbt.fct_orders ................... [[31mERROR[0m in 1.71s]
[0m16:01:10.597956 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:01:10.600418 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:01:10.600902 [debug] [MainThread]: On master: ROLLBACK
[0m16:01:10.601277 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:01:11.069477 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:01:11.070277 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:11.070685 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:01:11.071077 [debug] [MainThread]: On master: ROLLBACK
[0m16:01:11.071739 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:01:11.072230 [debug] [MainThread]: On master: Close
[0m16:01:11.522460 [info ] [MainThread]: 
[0m16:01:11.523075 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 18.06 seconds (18.06s).
[0m16:01:11.523865 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:01:11.524401 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m16:01:11.536265 [info ] [MainThread]: 
[0m16:01:11.536691 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:01:11.537077 [info ] [MainThread]: 
[0m16:01:11.537370 [error] [MainThread]: [33mRuntime Error in model fct_orders (models/marts/core/fct_orders.sql)[0m
[0m16:01:11.537656 [error] [MainThread]:   Table or view not found: orders_payment; line 41 pos 9
[0m16:01:11.537952 [info ] [MainThread]: 
[0m16:01:11.538235 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m16:01:11.538579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7e1fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7e1790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a8b9130>]}
[0m16:01:11.538850 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:01:51.135730 | 6bb545bc-f49c-4c79-909d-6bb489aa5aba ==============================
[0m16:01:51.135762 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:01:51.136763 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:01:51.136979 [debug] [MainThread]: Tracking: tracking
[0m16:01:51.154737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224def10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224de670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224de8b0>]}
[0m16:01:51.210115 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:01:51.210792 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m16:01:51.226895 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m16:01:51.249232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12270c0d0>]}
[0m16:01:51.258037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1226ad1c0>]}
[0m16:01:51.258477 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:01:51.258772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1226ad100>]}
[0m16:01:51.260165 [info ] [MainThread]: 
[0m16:01:51.260807 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:01:51.261827 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:01:51.272911 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:01:51.273349 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:01:51.273672 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:01:52.404340 [debug] [ThreadPool]: SQL status: OK in 1.13 seconds
[0m16:01:52.417312 [debug] [ThreadPool]: On list_schemas: Close
[0m16:01:52.883130 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:01:52.893595 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:52.893980 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:01:52.894235 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:01:52.894461 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:01:54.166959 [debug] [ThreadPool]: SQL status: OK in 1.27 seconds
[0m16:01:54.173207 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:01:54.173595 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:01:54.173805 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:01:54.631232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122673f40>]}
[0m16:01:54.632061 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:54.632399 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:01:54.633081 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:01:54.633631 [info ] [MainThread]: 
[0m16:01:54.639529 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:01:54.640135 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:01:54.640998 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:01:54.641319 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:01:54.641629 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:01:54.644501 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:01:54.645294 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:54.645650 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:01:54.686704 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:01:54.687488 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:54.687749 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:01:54.687888 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:01:54.688015 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:01:56.415165 [debug] [Thread-1  ]: SQL status: OK in 1.73 seconds
[0m16:01:56.440272 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:56.440589 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:01:56.440778 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:01:56.440936 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:01:56.961330 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1227c9cd0>]}
[0m16:01:56.962031 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.32s]
[0m16:01:56.962627 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:01:56.962985 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:01:56.963865 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:01:56.965225 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:01:56.965579 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:01:56.965816 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:01:56.973033 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:01:56.973638 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:56.974097 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:01:56.978224 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:01:56.978911 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:56.979119 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:01:56.979286 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:01:56.979416 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:01:58.643910 [debug] [Thread-1  ]: SQL status: OK in 1.66 seconds
[0m16:01:58.650243 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:58.650760 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:01:58.651094 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:01:58.651392 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:01:59.183028 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12279d3a0>]}
[0m16:01:59.183657 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.22s]
[0m16:01:59.184220 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:01:59.184562 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:01:59.185311 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:01:59.185945 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:01:59.186155 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:01:59.186350 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:01:59.188268 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:01:59.188808 [debug] [Thread-1  ]: finished collecting timing info
[0m16:01:59.189005 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:01:59.192958 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:01:59.193513 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:01:59.193686 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:01:59.193832 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:01:59.193973 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:00.686467 [debug] [Thread-1  ]: SQL status: OK in 1.49 seconds
[0m16:02:00.689275 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:00.689600 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:02:00.689786 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:00.689944 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:02:01.137587 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224de490>]}
[0m16:02:01.138178 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 1.95s]
[0m16:02:01.138710 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:02:01.139082 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:02:01.139644 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:02:01.140468 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:02:01.140680 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:02:01.140856 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:02:01.145182 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:02:01.145783 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:01.146023 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:02:01.201796 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:02:01.202489 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:01.202700 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:02:01.202861 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (

    select * from jaffle_shop_dbt.stg_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
[0m16:02:01.203012 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:05.533208 [debug] [Thread-1  ]: SQL status: OK in 4.33 seconds
[0m16:02:05.547890 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:05.548206 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:02:05.548388 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:05.548543 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:02:06.014511 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12267cb20>]}
[0m16:02:06.015360 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 4.87s]
[0m16:02:06.016304 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:02:06.017001 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:02:06.018201 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:02:06.019656 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:02:06.020001 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:02:06.020332 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:02:06.026531 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:02:06.027255 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:06.027549 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:02:06.039438 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:02:06.040378 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:06.040615 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:02:06.040784 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (customer_id)
)

select * from final
[0m16:02:06.040960 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:07.308013 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (customer_id)
)

select * from final
[0m16:02:07.308737 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: USING column `customer_id` cannot be resolved on the left side of the join. The left-side columns: [order_id, amount]
[0m16:02:07.309144 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: USING column `customer_id` cannot be resolved on the left side of the join. The left-side columns: [order_id, amount]
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: USING column `customer_id` cannot be resolved on the left side of the join. The left-side columns: [order_id, amount]
	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedUsingColForJoinError(QueryCompilationErrors.scala:138)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$commonNaturalJoinProcessing$3(Analyzer.scala:4563)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$commonNaturalJoinProcessing$1(Analyzer.scala:4563)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:4561)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$56.applyOrElse(Analyzer.scala:4426)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$56.applyOrElse(Analyzer.scala:4423)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1226)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1225)
	at org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1226)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1225)
	at org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1226)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1225)
	at org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1255)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1252)
	at org.apache.spark.sql.catalyst.plans.logical.ReplaceTableAsSelect.mapChildren(v2Commands.scala:322)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:4423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:4421)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m16:02:07.309453 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'l\xa2s\xd2WVN\xbc\xbc\xf3bA\x96{\xe3]'
[0m16:02:07.309968 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:07.310327 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:02:07.310635 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:07.310917 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:02:07.763257 [debug] [Thread-1  ]: Runtime Error in model fct_orders (models/marts/core/fct_orders.sql)
  USING column `customer_id` cannot be resolved on the left side of the join. The left-side columns: [order_id, amount]
[0m16:02:07.764115 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6bb545bc-f49c-4c79-909d-6bb489aa5aba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122869a00>]}
[0m16:02:07.765004 [error] [Thread-1  ]: 5 of 5 ERROR creating table model jaffle_shop_dbt.fct_orders ................... [[31mERROR[0m in 1.74s]
[0m16:02:07.766207 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:02:07.769315 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:02:07.769857 [debug] [MainThread]: On master: ROLLBACK
[0m16:02:07.770199 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:02:08.243014 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:02:08.243602 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:08.243995 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:02:08.244373 [debug] [MainThread]: On master: ROLLBACK
[0m16:02:08.244709 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:02:08.245037 [debug] [MainThread]: On master: Close
[0m16:02:08.708084 [info ] [MainThread]: 
[0m16:02:08.709037 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 17.45 seconds (17.45s).
[0m16:02:08.710513 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:02:08.711285 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m16:02:08.727423 [info ] [MainThread]: 
[0m16:02:08.727985 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:02:08.728421 [info ] [MainThread]: 
[0m16:02:08.728791 [error] [MainThread]: [33mRuntime Error in model fct_orders (models/marts/core/fct_orders.sql)[0m
[0m16:02:08.729119 [error] [MainThread]:   USING column `customer_id` cannot be resolved on the left side of the join. The left-side columns: [order_id, amount]
[0m16:02:08.729451 [info ] [MainThread]: 
[0m16:02:08.729768 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m16:02:08.730225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122791d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122791400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122791e50>]}
[0m16:02:08.730580 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:02:39.511148 | 47060ecb-57d1-4530-82eb-ddc10e839d8d ==============================
[0m16:02:39.511224 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:02:39.512673 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:02:39.513179 [debug] [MainThread]: Tracking: tracking
[0m16:02:39.532476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173d1b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173d61f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173d61c0>]}
[0m16:02:39.587735 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:02:39.588293 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m16:02:39.604878 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m16:02:39.634250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1176160d0>]}
[0m16:02:39.643449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175b41f0>]}
[0m16:02:39.644297 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:02:39.644978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175b4130>]}
[0m16:02:39.646685 [info ] [MainThread]: 
[0m16:02:39.647557 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:02:39.649142 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:02:39.662869 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:02:39.663166 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:02:39.663333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:40.687454 [debug] [ThreadPool]: SQL status: OK in 1.02 seconds
[0m16:02:40.696000 [debug] [ThreadPool]: On list_schemas: Close
[0m16:02:41.149301 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:02:41.160948 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:41.161421 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:02:41.161862 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:02:41.162287 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:02:42.283041 [debug] [ThreadPool]: SQL status: OK in 1.12 seconds
[0m16:02:42.288196 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:02:42.288573 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:02:42.288829 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:02:42.734007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11757c700>]}
[0m16:02:42.734541 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:42.734786 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:02:42.735249 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:02:42.735625 [info ] [MainThread]: 
[0m16:02:42.739725 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:02:42.740256 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:02:42.741125 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:02:42.741419 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:02:42.741650 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:02:42.743709 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:02:42.744261 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:42.744474 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:02:42.780588 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:02:42.781283 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:42.781491 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:02:42.781645 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:02:42.781789 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:44.262125 [debug] [Thread-1  ]: SQL status: OK in 1.48 seconds
[0m16:02:44.289462 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:44.289790 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:02:44.289975 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:44.290130 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:02:44.750754 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1176d0d30>]}
[0m16:02:44.752390 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.01s]
[0m16:02:44.753704 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:02:44.754863 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:02:44.755964 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:02:44.757230 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:02:44.757681 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:02:44.758066 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:02:44.769187 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:02:44.770097 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:44.770446 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:02:44.776154 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:02:44.776935 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:44.777207 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:02:44.777381 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:02:44.777547 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:46.248789 [debug] [Thread-1  ]: SQL status: OK in 1.47 seconds
[0m16:02:46.255460 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:46.256195 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:02:46.256573 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:46.256858 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:02:46.713384 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117666400>]}
[0m16:02:46.714267 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 1.96s]
[0m16:02:46.715606 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:02:46.716448 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:02:46.717208 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:02:46.718897 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:02:46.719655 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:02:46.720155 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:02:46.723117 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:02:46.723838 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:46.724298 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:02:46.729313 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:02:46.730006 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:46.730235 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:02:46.730423 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:02:46.730621 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:48.220107 [debug] [Thread-1  ]: SQL status: OK in 1.49 seconds
[0m16:02:48.225291 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:48.225737 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:02:48.226075 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:48.226258 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:02:48.675751 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175566d0>]}
[0m16:02:48.676257 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 1.96s]
[0m16:02:48.676808 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:02:48.677188 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:02:48.677840 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:02:48.678938 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:02:48.679309 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:02:48.679618 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:02:48.684198 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:02:48.684659 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:48.684931 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:02:48.743537 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:02:48.744121 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:48.744433 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:02:48.744578 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (

    select * from jaffle_shop_dbt.stg_orders

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
[0m16:02:48.744703 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:52.846275 [debug] [Thread-1  ]: SQL status: OK in 4.1 seconds
[0m16:02:52.870337 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:52.870741 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:02:52.870972 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:52.871154 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:02:53.335968 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117647310>]}
[0m16:02:53.336925 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 4.66s]
[0m16:02:53.338060 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:02:53.338927 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:02:53.339807 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:02:53.340681 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:02:53.340951 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:02:53.341201 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:02:53.346224 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:02:53.346991 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:53.347446 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:02:53.363379 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:02:53.363989 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:53.364183 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:02:53.364327 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m16:02:53.364464 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:02:57.738871 [debug] [Thread-1  ]: SQL status: OK in 4.37 seconds
[0m16:02:57.742747 [debug] [Thread-1  ]: finished collecting timing info
[0m16:02:57.743105 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:02:57.743324 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:02:57.743515 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:02:58.247181 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47060ecb-57d1-4530-82eb-ddc10e839d8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11776b4f0>]}
[0m16:02:58.248617 [info ] [Thread-1  ]: 5 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.91s]
[0m16:02:58.249636 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:02:58.252952 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:02:58.253510 [debug] [MainThread]: On master: ROLLBACK
[0m16:02:58.253918 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:02:58.732676 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:02:58.733165 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:02:58.733485 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:02:58.733783 [debug] [MainThread]: On master: ROLLBACK
[0m16:02:58.734042 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:02:58.734291 [debug] [MainThread]: On master: Close
[0m16:02:59.199432 [info ] [MainThread]: 
[0m16:02:59.200335 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 19.55 seconds (19.55s).
[0m16:02:59.201304 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:02:59.201786 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m16:02:59.219187 [info ] [MainThread]: 
[0m16:02:59.219715 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:02:59.220128 [info ] [MainThread]: 
[0m16:02:59.220463 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m16:02:59.221158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117756730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117756cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117756d90>]}
[0m16:02:59.221555 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:41:24.138149 | 2338b6d2-0216-47d3-928f-8a8541dd437f ==============================
[0m16:41:24.138226 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:41:24.139856 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m16:41:24.140304 [debug] [MainThread]: Tracking: tracking
[0m16:41:24.168148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207f55e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207f57f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207f5ee0>]}
[0m16:41:24.268353 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:41:24.268941 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m16:41:24.284242 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m16:41:24.310972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2338b6d2-0216-47d3-928f-8a8541dd437f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a360d0>]}
[0m16:41:24.320823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2338b6d2-0216-47d3-928f-8a8541dd437f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1209d5280>]}
[0m16:41:24.321545 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:41:24.321954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2338b6d2-0216-47d3-928f-8a8541dd437f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11020b790>]}
[0m16:41:24.323438 [info ] [MainThread]: 
[0m16:41:24.324078 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:41:24.325039 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:41:24.337413 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:41:24.337716 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:41:24.337922 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:41:24.338094 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:41:26.093866 [debug] [ThreadPool]: SQL status: OK in 1.76 seconds
[0m16:41:26.105633 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:41:26.105964 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:41:26.106201 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:41:26.605402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2338b6d2-0216-47d3-928f-8a8541dd437f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1209a15b0>]}
[0m16:41:26.606475 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:41:26.607049 [info ] [MainThread]: 
[0m16:41:26.615153 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:41:26.615958 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:41:26.616473 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:41:26.616808 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:41:26.619624 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:41:26.620330 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.620667 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:41:26.620930 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.621578 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:41:26.621860 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:41:26.622528 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:41:26.622797 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:41:26.623018 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:41:26.625462 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:41:26.626521 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.626834 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:41:26.627048 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.627623 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:41:26.627887 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:41:26.628520 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:41:26.628762 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:41:26.628963 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:41:26.631285 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:41:26.633189 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.633665 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:41:26.634043 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.635105 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:41:26.635782 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:41:26.636735 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:41:26.637077 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:41:26.637380 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:41:26.642658 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:41:26.643297 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.643758 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:41:26.643980 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.644570 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:41:26.644881 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:41:26.645377 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:41:26.645597 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:41:26.645751 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:41:26.649059 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:41:26.649651 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.649906 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:41:26.650053 [debug] [Thread-1  ]: finished collecting timing info
[0m16:41:26.650477 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:41:26.651593 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:41:26.651798 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m16:41:26.661911 [info ] [MainThread]: Done.
[0m16:41:26.666408 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m16:41:26.666758 [info ] [MainThread]: Building catalog
[0m16:41:26.668494 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop_dbt"
[0m16:41:26.668853 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.dim_customers
[0m16:41:26.669166 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.fct_orders
[0m16:41:26.669363 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_customers
[0m16:41:26.669546 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_orders
[0m16:41:26.669727 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_payments
[0m16:41:26.681165 [info ] [MainThread]: Catalog written to /Users/tcsilva/workspace/pessoal/estudos/dbt/jaffle_shop/target/catalog.json
[0m16:41:26.681882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207f55e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120af6040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120af62b0>]}
[0m16:41:26.682303 [debug] [MainThread]: Flushing usage events
[0m16:41:27.353767 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m16:41:27.354194 [debug] [MainThread]: Connection 'jaffle_shop_dbt' was properly closed.


============================== 2022-10-13 16:41:33.970696 | 123ef5af-4478-42c5-a259-05e36280c424 ==============================
[0m16:41:33.970742 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:41:33.972256 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m16:41:33.972763 [debug] [MainThread]: Tracking: tracking
[0m16:41:33.994376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cab1a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cab12e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cac1520>]}
[0m16:41:33.999291 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m16:41:33.999932 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m16:41:34.000338 [info ] [MainThread]: 
[0m16:41:34.000647 [info ] [MainThread]: 
[0m16:41:34.001106 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-10-13 16:44:18.345920 | bd20daba-546c-4eae-aa1d-8867b13daf16 ==============================
[0m16:44:18.346036 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:44:18.347350 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:44:18.347873 [debug] [MainThread]: Tracking: tracking
[0m16:44:18.368593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116b05730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116b05a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116b056a0>]}
[0m16:44:18.447752 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:44:18.448336 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/dim_customers.sql
[0m16:44:18.471069 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m16:44:18.523864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d350d0>]}
[0m16:44:18.540220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cd6280>]}
[0m16:44:18.540656 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:44:18.541020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116c63250>]}
[0m16:44:18.543424 [info ] [MainThread]: 
[0m16:44:18.544905 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:44:18.546924 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:44:18.566485 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:44:18.566989 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:44:18.567346 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:44:20.178549 [debug] [ThreadPool]: SQL status: OK in 1.61 seconds
[0m16:44:20.186837 [debug] [ThreadPool]: On list_schemas: Close
[0m16:44:21.094660 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:44:21.100857 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:21.101300 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:44:21.101534 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:44:21.101824 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:44:22.232013 [debug] [ThreadPool]: SQL status: OK in 1.13 seconds
[0m16:44:22.235775 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:44:22.236022 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:44:22.236184 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:44:22.687339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116ca51c0>]}
[0m16:44:22.687775 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:22.688318 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:44:22.688867 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:44:22.689196 [info ] [MainThread]: 
[0m16:44:22.694310 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:44:22.694717 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:44:22.695267 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:44:22.695488 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:44:22.695645 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:44:22.697899 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:44:22.698522 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:22.698855 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:44:22.740490 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:44:22.741276 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:22.741563 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:44:22.741706 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:44:22.741830 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:44:24.321738 [debug] [Thread-1  ]: SQL status: OK in 1.58 seconds
[0m16:44:24.349660 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:24.349991 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:44:24.350166 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:44:24.350305 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:44:24.806552 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116df2e80>]}
[0m16:44:24.807571 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.11s]
[0m16:44:24.808587 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:44:24.809097 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:44:24.810419 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:44:24.811928 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:44:24.812625 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:44:24.813162 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:44:24.824801 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:44:24.825510 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:24.825841 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:44:24.833068 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:44:24.833921 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:24.834229 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:44:24.834433 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:44:24.834611 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:44:26.490344 [debug] [Thread-1  ]: SQL status: OK in 1.66 seconds
[0m16:44:26.493668 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:26.494008 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:44:26.494227 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:44:26.494402 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:44:26.945330 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116d302b0>]}
[0m16:44:26.945956 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.13s]
[0m16:44:26.946565 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:44:26.946930 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:44:26.947282 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:44:26.947934 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:44:26.948278 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:44:26.948567 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:44:26.950708 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:44:26.951330 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:26.951556 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:44:26.955889 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:44:26.956617 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:26.956828 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:44:26.957033 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:44:26.957183 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:44:28.485966 [debug] [Thread-1  ]: SQL status: OK in 1.53 seconds
[0m16:44:28.489990 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:28.490434 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:44:28.490771 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:44:28.491004 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:44:28.947137 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116da7460>]}
[0m16:44:28.947831 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 2.00s]
[0m16:44:28.948508 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:44:28.949726 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:44:28.950178 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:44:28.950860 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:44:28.951104 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:44:28.951316 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:44:28.955145 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:44:28.955819 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:28.956049 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:44:29.007354 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:44:29.008164 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:29.008495 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:44:29.008804 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m16:44:29.009156 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:44:32.888296 [debug] [Thread-1  ]: SQL status: OK in 3.88 seconds
[0m16:44:32.906332 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:32.906653 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:44:32.906837 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:44:32.906987 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:44:33.381421 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116dd9df0>]}
[0m16:44:33.382626 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.43s]
[0m16:44:33.383671 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:44:33.385340 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:44:33.386801 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:44:33.388082 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:44:33.388544 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:44:33.388892 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:44:33.394544 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:44:33.395418 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:33.395709 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:44:33.408962 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:44:33.409663 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:33.409858 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:44:33.410019 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        customer_orders.lifetime_value
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:44:33.410177 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:44:34.492970 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders,
        customer_orders.lifetime_value
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:44:34.493606 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
[0m16:44:34.494001 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m16:44:34.494400 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'g\x93#\x1c\xb5\x94KT\xb4\xd9\x1e\xc8\xfe\xaf2\xaa'
[0m16:44:34.495083 [debug] [Thread-1  ]: finished collecting timing info
[0m16:44:34.495461 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:44:34.495749 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:44:34.496019 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:44:34.949731 [debug] [Thread-1  ]: Runtime Error in model dim_customers (models/marts/core/dim_customers.sql)
  Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
[0m16:44:34.950421 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd20daba-546c-4eae-aa1d-8867b13daf16', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cee4c0>]}
[0m16:44:34.951080 [error] [Thread-1  ]: 5 of 5 ERROR creating table model jaffle_shop_dbt.dim_customers ................ [[31mERROR[0m in 1.56s]
[0m16:44:34.951854 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:44:34.954657 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:44:34.955246 [debug] [MainThread]: On master: ROLLBACK
[0m16:44:34.955597 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:44:35.462598 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:44:35.463421 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:44:35.463797 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:44:35.464183 [debug] [MainThread]: On master: ROLLBACK
[0m16:44:35.464649 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:44:35.464990 [debug] [MainThread]: On master: Close
[0m16:44:35.925286 [info ] [MainThread]: 
[0m16:44:35.926168 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 17.38 seconds (17.38s).
[0m16:44:35.927316 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:44:35.927987 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m16:44:35.943357 [info ] [MainThread]: 
[0m16:44:35.943846 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:44:35.944281 [info ] [MainThread]: 
[0m16:44:35.944648 [error] [MainThread]: [33mRuntime Error in model dim_customers (models/marts/core/dim_customers.sql)[0m
[0m16:44:35.945437 [error] [MainThread]:   Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
[0m16:44:35.946120 [info ] [MainThread]: 
[0m16:44:35.946568 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m16:44:35.947118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e930a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e93970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116db8c10>]}
[0m16:44:35.947508 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:45:35.668451 | 3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c ==============================
[0m16:45:35.668533 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:45:35.670001 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:45:35.670689 [debug] [MainThread]: Tracking: tracking
[0m16:45:35.690159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1227e5730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1227e5a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1227e56a0>]}
[0m16:45:35.746264 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:45:35.746848 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/dim_customers.sql
[0m16:45:35.760204 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m16:45:35.784987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122a160d0>]}
[0m16:45:35.793974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1229b6280>]}
[0m16:45:35.794469 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:45:35.794778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122944250>]}
[0m16:45:35.796230 [info ] [MainThread]: 
[0m16:45:35.797010 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:45:35.798072 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:45:35.808179 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:45:35.808471 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:45:35.808632 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:45:36.876551 [debug] [ThreadPool]: SQL status: OK in 1.07 seconds
[0m16:45:36.890175 [debug] [ThreadPool]: On list_schemas: Close
[0m16:45:37.355747 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:45:37.368102 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:37.368432 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:45:37.368663 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:45:37.368868 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:45:38.533336 [debug] [ThreadPool]: SQL status: OK in 1.16 seconds
[0m16:45:38.540402 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:45:38.540858 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:45:38.541151 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:45:38.999151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1229851c0>]}
[0m16:45:39.000065 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:39.000476 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:45:39.001240 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:45:39.001887 [info ] [MainThread]: 
[0m16:45:39.008184 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:45:39.009006 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:45:39.009820 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:45:39.010104 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:45:39.010375 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:45:39.012952 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:45:39.013692 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:39.014024 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:45:39.053139 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:45:39.053763 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:39.054017 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:45:39.054189 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:45:39.054377 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:45:40.491624 [debug] [Thread-1  ]: SQL status: OK in 1.44 seconds
[0m16:45:40.512709 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:40.513011 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:45:40.513186 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:45:40.513340 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:45:40.993645 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ad2e80>]}
[0m16:45:40.994414 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 1.98s]
[0m16:45:40.995306 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:45:40.995833 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:45:40.996638 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:45:40.997310 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:45:40.997607 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:45:40.997809 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:45:41.006121 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:45:41.006739 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:41.006984 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:45:41.010435 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:45:41.011156 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:41.011373 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:45:41.011515 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:45:41.011645 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:45:42.559179 [debug] [Thread-1  ]: SQL status: OK in 1.55 seconds
[0m16:45:42.561569 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:42.561873 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:45:42.562055 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:45:42.562193 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:45:43.033258 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122a0a2b0>]}
[0m16:45:43.034057 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.04s]
[0m16:45:43.034847 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:45:43.035417 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:45:43.036136 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:45:43.037052 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:45:43.037351 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:45:43.037595 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:45:43.039928 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:45:43.040664 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:43.040935 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:45:43.044966 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:45:43.045571 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:43.045832 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:45:43.046023 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:45:43.046183 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:45:44.491863 [debug] [Thread-1  ]: SQL status: OK in 1.45 seconds
[0m16:45:44.494470 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:44.494773 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:45:44.494972 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:45:44.495375 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:45:44.950976 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122a87460>]}
[0m16:45:44.951599 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 1.91s]
[0m16:45:44.952117 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:45:44.955655 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:45:44.956058 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:45:44.956560 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:45:44.956733 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:45:44.957021 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:45:44.959872 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:45:44.962026 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:44.962437 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:45:45.022155 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:45:45.022808 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:45.023122 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:45:45.023520 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m16:45:45.023736 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:45:48.924807 [debug] [Thread-1  ]: SQL status: OK in 3.9 seconds
[0m16:45:48.936360 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:48.936703 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:45:48.936860 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:45:48.936986 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:45:49.383051 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ab9df0>]}
[0m16:45:49.383621 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.43s]
[0m16:45:49.384205 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:45:49.384974 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:45:49.385381 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:45:49.385986 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:45:49.386205 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:45:49.386351 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:45:49.389554 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:45:49.390144 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:49.390381 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:45:49.400629 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:45:49.401231 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:49.401446 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:45:49.401618 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:45:49.401790 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:45:50.474134 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:45:50.474452 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
mismatched input 'left' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'QUALIFY', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
    from customers
    left join customer_orders using (customer_id)
----^^^
)
select * from final

[0m16:45:50.474671 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'left' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'QUALIFY', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
    from customers
    left join customer_orders using (customer_id)
----^^^
)
select * from final

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'left' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'QUALIFY', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 4)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
    from customers
    left join customer_orders using (customer_id)
----^^^
)
select * from final

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:298)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:159)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:88)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:106)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:337)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:337)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m16:45:50.474850 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x04\xfa\\\xa7\x80YKZ\x95\n\xd0\xe0\tLZ\x07'
[0m16:45:50.475292 [debug] [Thread-1  ]: finished collecting timing info
[0m16:45:50.475598 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:45:50.475936 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:45:50.476106 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:45:50.939806 [debug] [Thread-1  ]: Runtime Error in model dim_customers (models/marts/core/dim_customers.sql)
  
  mismatched input 'left' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'QUALIFY', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 4)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */
  
        create or replace table jaffle_shop_dbt.dim_customers
      
      
      using delta
      
      
      
      
      
      
      as
        with 
  
  customers as (
  
      select * from jaffle_shop_dbt.stg_customers
  
  ),
  
  orders as (
      select * from jaffle_shop_dbt.fct_orders
  ),
  
  customer_orders as (
      select
          customer_id,
          min(order_date) as first_order_date,
          max(order_date) as most_recent_order_date,
          count(order_id) as number_of_orders,
          sum(amount) as lifetime_value
      from orders
      group by 1
  ),
  final as (
      select
          customers.customer_id,
          customers.first_name,
          customers.last_name,
      from customers
      left join customer_orders using (customer_id)
  ----^^^
  )
  select * from final
  
[0m16:45:50.940232 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d8f45f6-f6a5-49f9-8eef-4a8520ac0e8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1229cc760>]}
[0m16:45:50.940635 [error] [Thread-1  ]: 5 of 5 ERROR creating table model jaffle_shop_dbt.dim_customers ................ [[31mERROR[0m in 1.55s]
[0m16:45:50.941067 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:45:50.942762 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:45:50.943009 [debug] [MainThread]: On master: ROLLBACK
[0m16:45:50.943188 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:45:51.404646 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:45:51.404927 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:45:51.405082 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:45:51.405410 [debug] [MainThread]: On master: ROLLBACK
[0m16:45:51.405559 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:45:51.405693 [debug] [MainThread]: On master: Close
[0m16:45:51.868157 [info ] [MainThread]: 
[0m16:45:51.868654 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 16.07 seconds (16.07s).
[0m16:45:51.869445 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:45:51.869805 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m16:45:51.881392 [info ] [MainThread]: 
[0m16:45:51.881784 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:45:51.882176 [info ] [MainThread]: 
[0m16:45:51.882447 [error] [MainThread]: [33mRuntime Error in model dim_customers (models/marts/core/dim_customers.sql)[0m
[0m16:45:51.882769 [error] [MainThread]:   
[0m16:45:51.882988 [error] [MainThread]:   mismatched input 'left' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'QUALIFY', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 42, pos 4)
[0m16:45:51.883208 [error] [MainThread]:   
[0m16:45:51.883445 [error] [MainThread]:   == SQL ==
[0m16:45:51.883726 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */
[0m16:45:51.883959 [error] [MainThread]:   
[0m16:45:51.884346 [error] [MainThread]:         create or replace table jaffle_shop_dbt.dim_customers
[0m16:45:51.884657 [error] [MainThread]:       
[0m16:45:51.884860 [error] [MainThread]:       
[0m16:45:51.885078 [error] [MainThread]:       using delta
[0m16:45:51.885267 [error] [MainThread]:       
[0m16:45:51.885615 [error] [MainThread]:       
[0m16:45:51.886409 [error] [MainThread]:       
[0m16:45:51.886942 [error] [MainThread]:       
[0m16:45:51.887275 [error] [MainThread]:       
[0m16:45:51.887562 [error] [MainThread]:       
[0m16:45:51.887928 [error] [MainThread]:       as
[0m16:45:51.888422 [error] [MainThread]:         with 
[0m16:45:51.888810 [error] [MainThread]:   
[0m16:45:51.889299 [error] [MainThread]:   customers as (
[0m16:45:51.889767 [error] [MainThread]:   
[0m16:45:51.890260 [error] [MainThread]:       select * from jaffle_shop_dbt.stg_customers
[0m16:45:51.890810 [error] [MainThread]:   
[0m16:45:51.891274 [error] [MainThread]:   ),
[0m16:45:51.891582 [error] [MainThread]:   
[0m16:45:51.892275 [error] [MainThread]:   orders as (
[0m16:45:51.892644 [error] [MainThread]:       select * from jaffle_shop_dbt.fct_orders
[0m16:45:51.892893 [error] [MainThread]:   ),
[0m16:45:51.893167 [error] [MainThread]:   
[0m16:45:51.893436 [error] [MainThread]:   customer_orders as (
[0m16:45:51.893768 [error] [MainThread]:       select
[0m16:45:51.894208 [error] [MainThread]:           customer_id,
[0m16:45:51.894559 [error] [MainThread]:           min(order_date) as first_order_date,
[0m16:45:51.894913 [error] [MainThread]:           max(order_date) as most_recent_order_date,
[0m16:45:51.895341 [error] [MainThread]:           count(order_id) as number_of_orders,
[0m16:45:51.895902 [error] [MainThread]:           sum(amount) as lifetime_value
[0m16:45:51.896408 [error] [MainThread]:       from orders
[0m16:45:51.896761 [error] [MainThread]:       group by 1
[0m16:45:51.897257 [error] [MainThread]:   ),
[0m16:45:51.897521 [error] [MainThread]:   final as (
[0m16:45:51.897771 [error] [MainThread]:       select
[0m16:45:51.898035 [error] [MainThread]:           customers.customer_id,
[0m16:45:51.898251 [error] [MainThread]:           customers.first_name,
[0m16:45:51.898480 [error] [MainThread]:           customers.last_name,
[0m16:45:51.898700 [error] [MainThread]:       from customers
[0m16:45:51.898917 [error] [MainThread]:       left join customer_orders using (customer_id)
[0m16:45:51.899153 [error] [MainThread]:   ----^^^
[0m16:45:51.899366 [error] [MainThread]:   )
[0m16:45:51.899579 [error] [MainThread]:   select * from final
[0m16:45:51.899783 [error] [MainThread]:   
[0m16:45:51.900027 [info ] [MainThread]: 
[0m16:45:51.900235 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m16:45:51.900527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b732e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b73970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b74c40>]}
[0m16:45:51.900766 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:46:42.713016 | eacc93ec-e3aa-4bff-a13f-5c5874019713 ==============================
[0m16:46:42.713136 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:46:42.714672 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:46:42.714905 [debug] [MainThread]: Tracking: tracking
[0m16:46:42.733619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1181b5100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1181b6f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1181b6b50>]}
[0m16:46:42.820145 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:46:42.820816 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/dim_customers.sql
[0m16:46:42.837141 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m16:46:42.861825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183fa0d0>]}
[0m16:46:42.870433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11839a1f0>]}
[0m16:46:42.870801 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:46:42.871115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183291c0>]}
[0m16:46:42.872534 [info ] [MainThread]: 
[0m16:46:42.873120 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:46:42.874447 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:46:42.887822 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:46:42.888124 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:46:42.888288 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:46:44.349428 [debug] [ThreadPool]: SQL status: OK in 1.46 seconds
[0m16:46:44.360494 [debug] [ThreadPool]: On list_schemas: Close
[0m16:46:44.918994 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:46:44.928048 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:44.928339 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:46:44.928522 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:46:44.928683 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:46:46.095111 [debug] [ThreadPool]: SQL status: OK in 1.17 seconds
[0m16:46:46.101123 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:46:46.101519 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:46:46.101774 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:46:46.646408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11835ffd0>]}
[0m16:46:46.647296 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:46.647809 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:46:46.648674 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:46:46.649404 [info ] [MainThread]: 
[0m16:46:46.657520 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:46:46.658115 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:46:46.658943 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:46:46.659245 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:46:46.659522 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:46:46.662284 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:46:46.663082 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:46.663410 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:46:46.702014 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:46:46.702601 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:46.702823 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:46:46.702949 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:46:46.703070 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:46:48.174911 [debug] [Thread-1  ]: SQL status: OK in 1.47 seconds
[0m16:46:48.197184 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:48.197471 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:46:48.197630 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:46:48.197769 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:46:48.646310 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184b5df0>]}
[0m16:46:48.647180 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 1.99s]
[0m16:46:48.648038 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:46:48.648456 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:46:48.649162 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:46:48.650430 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:46:48.650833 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:46:48.651080 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:46:48.659592 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:46:48.660326 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:48.660594 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:46:48.666098 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:46:48.667015 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:48.667274 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:46:48.667467 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:46:48.667640 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:46:50.278970 [debug] [Thread-1  ]: SQL status: OK in 1.61 seconds
[0m16:46:50.284207 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:50.284724 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:46:50.285051 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:46:50.285355 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:46:50.729774 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183f64f0>]}
[0m16:46:50.730749 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.08s]
[0m16:46:50.731912 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:46:50.732598 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:46:50.733224 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:46:50.734499 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:46:50.734842 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:46:50.735097 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:46:50.737219 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:46:50.737823 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:50.738066 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:46:50.742326 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:46:50.743498 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:50.743739 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:46:50.743905 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:46:50.744062 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:46:52.176436 [debug] [Thread-1  ]: SQL status: OK in 1.43 seconds
[0m16:46:52.179941 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:52.180264 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:46:52.180567 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:46:52.180750 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:46:52.637043 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11849bf70>]}
[0m16:46:52.638057 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 1.90s]
[0m16:46:52.639369 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:46:52.641082 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:46:52.641796 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:46:52.642963 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:46:52.643376 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:46:52.643693 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:46:52.648607 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:46:52.649431 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:52.649743 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:46:52.704418 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:46:52.705023 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:52.705243 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:46:52.705372 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m16:46:52.705497 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:46:56.334397 [debug] [Thread-1  ]: SQL status: OK in 3.63 seconds
[0m16:46:56.354411 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:56.354803 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:46:56.355028 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:46:56.355205 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:46:56.816342 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11852dfa0>]}
[0m16:46:56.817864 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.17s]
[0m16:46:56.819279 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:46:56.821438 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:46:56.822413 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:46:56.824351 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:46:56.824948 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:46:56.825298 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:46:56.830394 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:46:56.831232 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:56.831691 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:46:56.846664 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:46:56.847363 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:56.847585 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:46:56.847741 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:46:56.847890 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:46:57.963879 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:46:57.964757 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
[0m16:46:57.965387 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m16:46:57.965750 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'uA\xa5\xcc\xd2BE\xb9\xbe\xbc\xf9\xa1\xd4\xa8n\xc0'
[0m16:46:57.966724 [debug] [Thread-1  ]: finished collecting timing info
[0m16:46:57.967260 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:46:57.967656 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:46:57.968011 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:46:58.425660 [debug] [Thread-1  ]: Runtime Error in model dim_customers (models/marts/core/dim_customers.sql)
  Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
[0m16:46:58.426686 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'eacc93ec-e3aa-4bff-a13f-5c5874019713', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118497af0>]}
[0m16:46:58.427759 [error] [Thread-1  ]: 5 of 5 ERROR creating table model jaffle_shop_dbt.dim_customers ................ [[31mERROR[0m in 1.60s]
[0m16:46:58.429096 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:46:58.432189 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:46:58.432685 [debug] [MainThread]: On master: ROLLBACK
[0m16:46:58.433009 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:46:58.903533 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:46:58.904278 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:58.904661 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:46:58.905029 [debug] [MainThread]: On master: ROLLBACK
[0m16:46:58.905367 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:46:58.905690 [debug] [MainThread]: On master: Close
[0m16:46:59.347095 [info ] [MainThread]: 
[0m16:46:59.347648 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 16.47 seconds (16.47s).
[0m16:46:59.348064 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:46:59.348293 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m16:46:59.357769 [info ] [MainThread]: 
[0m16:46:59.358161 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:46:59.358601 [info ] [MainThread]: 
[0m16:46:59.358875 [error] [MainThread]: [33mRuntime Error in model dim_customers (models/marts/core/dim_customers.sql)[0m
[0m16:46:59.359353 [error] [MainThread]:   Column 'order_date' does not exist. Did you mean one of the following? [orders.amount, orders.order_id, orders.customer_id]; line 29 pos 12
[0m16:46:59.359624 [info ] [MainThread]: 
[0m16:46:59.360043 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m16:46:59.360441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11843cdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11843c7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118557190>]}
[0m16:46:59.360728 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:47:17.242999 | 2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b ==============================
[0m16:47:17.243044 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:47:17.244734 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m16:47:17.245248 [debug] [MainThread]: Tracking: tracking
[0m16:47:17.263681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9155e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9157f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e915ee0>]}
[0m16:47:17.352605 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:47:17.353130 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/dim_customers.sql
[0m16:47:17.369686 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m16:47:17.396041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb580d0>]}
[0m16:47:17.403934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eaf92b0>]}
[0m16:47:17.404331 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:47:17.404648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea88280>]}
[0m16:47:17.406044 [info ] [MainThread]: 
[0m16:47:17.406564 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:47:17.407565 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m16:47:17.417824 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m16:47:17.418097 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m16:47:17.418263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:18.396025 [debug] [ThreadPool]: SQL status: OK in 0.98 seconds
[0m16:47:18.404790 [debug] [ThreadPool]: On list_schemas: Close
[0m16:47:18.852438 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:47:18.862417 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:18.862803 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:47:18.863036 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:47:18.863248 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:47:20.004560 [debug] [ThreadPool]: SQL status: OK in 1.14 seconds
[0m16:47:20.011096 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:47:20.011548 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:47:20.011891 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:47:20.465860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eac9d00>]}
[0m16:47:20.466790 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:20.467312 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:47:20.468143 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:47:20.468902 [info ] [MainThread]: 
[0m16:47:20.477572 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:47:20.478240 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m16:47:20.479189 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:47:20.479574 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:47:20.479973 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:47:20.482806 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:47:20.483586 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:20.483982 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:47:20.520818 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m16:47:20.521436 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:20.521606 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m16:47:20.521738 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m16:47:20.521862 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:47:22.137451 [debug] [Thread-1  ]: SQL status: OK in 1.62 seconds
[0m16:47:22.161523 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:22.161975 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m16:47:22.162191 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:47:22.162353 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m16:47:22.620807 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ec16eb0>]}
[0m16:47:22.622186 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.14s]
[0m16:47:22.623559 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:47:22.624542 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:47:22.625728 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m16:47:22.627062 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:47:22.627623 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:47:22.628019 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:47:22.638950 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:47:22.639790 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:22.640378 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:47:22.645900 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m16:47:22.646606 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:22.646823 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m16:47:22.647002 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m16:47:22.647176 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:47:24.109577 [debug] [Thread-1  ]: SQL status: OK in 1.46 seconds
[0m16:47:24.116270 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:24.117076 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m16:47:24.117454 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:47:24.117745 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m16:47:24.572584 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebeb520>]}
[0m16:47:24.573736 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 1.95s]
[0m16:47:24.574844 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:47:24.575667 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:47:24.577017 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m16:47:24.578589 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:47:24.579094 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:47:24.579484 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:47:24.582848 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:47:24.583791 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:24.584188 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:47:24.589963 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m16:47:24.590774 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:24.591121 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m16:47:24.591408 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m16:47:24.591621 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:47:26.129136 [debug] [Thread-1  ]: SQL status: OK in 1.54 seconds
[0m16:47:26.133568 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:26.134138 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m16:47:26.134414 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:47:26.134621 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m16:47:26.601937 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebeb640>]}
[0m16:47:26.603174 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 2.02s]
[0m16:47:26.604144 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:47:26.605526 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:47:26.606462 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m16:47:26.607687 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:47:26.608109 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:47:26.608480 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:47:26.614680 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:47:26.616354 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:26.616842 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:47:26.670987 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m16:47:26.671579 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:26.671904 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m16:47:26.672168 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m16:47:26.672309 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:47:30.183468 [debug] [Thread-1  ]: SQL status: OK in 3.51 seconds
[0m16:47:30.203676 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:30.204035 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m16:47:30.204249 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:47:30.204421 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m16:47:30.672012 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebf93a0>]}
[0m16:47:30.673420 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.06s]
[0m16:47:30.674805 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:47:30.676793 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:47:30.677597 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m16:47:30.678769 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:47:30.679162 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:47:30.679481 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:47:30.685039 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:47:30.686113 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:30.686689 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:47:30.700831 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m16:47:30.701515 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:30.701713 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m16:47:30.701869 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m16:47:30.702020 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:47:34.740995 [debug] [Thread-1  ]: SQL status: OK in 4.04 seconds
[0m16:47:34.746507 [debug] [Thread-1  ]: finished collecting timing info
[0m16:47:34.746974 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m16:47:34.747301 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:47:34.747532 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m16:47:35.237112 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f59cbcc-6b5f-4943-b1cc-0d55d9e8fe4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ecaf340>]}
[0m16:47:35.238135 [info ] [Thread-1  ]: 5 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 4.56s]
[0m16:47:35.239461 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:47:35.241823 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:47:35.242294 [debug] [MainThread]: On master: ROLLBACK
[0m16:47:35.242591 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:47:35.721121 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:47:35.721621 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:35.722078 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:47:35.722577 [debug] [MainThread]: On master: ROLLBACK
[0m16:47:35.722967 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:47:35.723298 [debug] [MainThread]: On master: Close
[0m16:47:36.223609 [info ] [MainThread]: 
[0m16:47:36.224604 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 18.82 seconds (18.82s).
[0m16:47:36.225633 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:47:36.226192 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m16:47:36.243186 [info ] [MainThread]: 
[0m16:47:36.243769 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:47:36.244526 [info ] [MainThread]: 
[0m16:47:36.245313 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m16:47:36.245848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebdca60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebdcb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebdce50>]}
[0m16:47:36.246200 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 16:48:28.383454 | 964b57aa-4743-4dba-96bb-ba54abc99791 ==============================
[0m16:48:28.383499 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:48:28.384652 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m16:48:28.385154 [debug] [MainThread]: Tracking: tracking
[0m16:48:28.402398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120dc1280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120dc1a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120dc16a0>]}
[0m16:48:28.453937 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:48:28.454193 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:48:28.461790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '964b57aa-4743-4dba-96bb-ba54abc99791', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f6b0d0>]}
[0m16:48:28.469844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '964b57aa-4743-4dba-96bb-ba54abc99791', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120eb64c0>]}
[0m16:48:28.470202 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m16:48:28.470504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '964b57aa-4743-4dba-96bb-ba54abc99791', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120eb6550>]}
[0m16:48:28.472054 [info ] [MainThread]: 
[0m16:48:28.472706 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:48:28.473575 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:48:28.485288 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:48:28.485648 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:48:28.486057 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:48:28.486294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:48:29.833410 [debug] [ThreadPool]: SQL status: OK in 1.35 seconds
[0m16:48:29.845606 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:48:29.845990 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:48:29.846246 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:48:30.303350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '964b57aa-4743-4dba-96bb-ba54abc99791', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120eb6190>]}
[0m16:48:30.305197 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:48:30.306301 [info ] [MainThread]: 
[0m16:48:30.315923 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:48:30.316863 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:48:30.317419 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:48:30.317787 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:48:30.321550 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:48:30.322477 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.322848 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:48:30.323150 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.323927 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:48:30.324286 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:48:30.325022 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:48:30.325377 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:48:30.325659 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:48:30.328758 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:48:30.330071 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.330746 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:48:30.331040 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.331692 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:48:30.331981 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:48:30.332600 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:48:30.332919 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:48:30.333184 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:48:30.337594 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:48:30.338256 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.338495 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:48:30.338691 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.339208 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:48:30.339807 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:48:30.340282 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:48:30.340488 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:48:30.340819 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:48:30.344363 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:48:30.344921 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.345200 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:48:30.345390 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.345870 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:48:30.346630 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:48:30.347764 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:48:30.348118 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:48:30.348315 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:48:30.351252 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:48:30.351737 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.351910 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:48:30.352055 [debug] [Thread-1  ]: finished collecting timing info
[0m16:48:30.352445 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:48:30.353309 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:48:30.353474 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m16:48:30.360270 [info ] [MainThread]: Done.
[0m16:48:30.364258 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m16:48:30.364564 [info ] [MainThread]: Building catalog
[0m16:48:30.366321 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop_dbt"
[0m16:48:30.366588 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.dim_customers
[0m16:48:30.366838 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.fct_orders
[0m16:48:30.367032 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_customers
[0m16:48:30.367215 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_orders
[0m16:48:30.367399 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_payments
[0m16:48:30.378464 [info ] [MainThread]: Catalog written to /Users/tcsilva/workspace/pessoal/estudos/dbt/jaffle_shop/target/catalog.json
[0m16:48:30.379387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12101d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12101dac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12100e3d0>]}
[0m16:48:30.379771 [debug] [MainThread]: Flushing usage events
[0m16:48:31.034467 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m16:48:31.034938 [debug] [MainThread]: Connection 'jaffle_shop_dbt' was properly closed.


============================== 2022-10-13 16:48:37.857336 | 09ac7388-747a-4419-b140-8bb1dc8dc6fd ==============================
[0m16:48:37.857415 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:48:37.859384 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m16:48:37.859946 [debug] [MainThread]: Tracking: tracking
[0m16:48:37.880196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114df50a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114df6be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114df6f10>]}
[0m16:48:37.885695 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m16:48:37.886248 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m16:48:37.886738 [info ] [MainThread]: 
[0m16:48:37.887014 [info ] [MainThread]: 
[0m16:48:37.887563 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-10-13 16:58:05.605588 | 2af11a42-8b0f-4c83-8baf-72f33887e00a ==============================
[0m16:58:05.605622 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:58:05.606621 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m16:58:05.606974 [debug] [MainThread]: Tracking: tracking
[0m16:58:05.625101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d4b1100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d4b2f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d4b2b50>]}
[0m16:58:05.716552 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 0 files changed.
[0m16:58:05.717063 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m16:58:05.717309 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m16:58:05.746832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2af11a42-8b0f-4c83-8baf-72f33887e00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6ca0d0>]}
[0m16:58:05.755134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2af11a42-8b0f-4c83-8baf-72f33887e00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6676a0>]}
[0m16:58:05.755576 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m16:58:05.755971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2af11a42-8b0f-4c83-8baf-72f33887e00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d667280>]}
[0m16:58:05.757393 [info ] [MainThread]: 
[0m16:58:05.757969 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:58:05.759111 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m16:58:05.769758 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:05.770134 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m16:58:05.770469 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m16:58:05.770717 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:07.533984 [debug] [ThreadPool]: SQL status: OK in 1.76 seconds
[0m16:58:07.549470 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m16:58:07.549850 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:58:07.550175 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m16:58:08.316135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2af11a42-8b0f-4c83-8baf-72f33887e00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d667220>]}
[0m16:58:08.317339 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:58:08.318058 [info ] [MainThread]: 
[0m16:58:08.326165 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m16:58:08.326906 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m16:58:08.327202 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m16:58:08.327468 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m16:58:08.330408 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m16:58:08.331234 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.331574 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m16:58:08.331875 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.332688 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m16:58:08.332976 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m16:58:08.333408 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m16:58:08.333745 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m16:58:08.334105 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m16:58:08.336961 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m16:58:08.337941 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.338251 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m16:58:08.338546 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.339359 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m16:58:08.339684 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m16:58:08.340217 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m16:58:08.340463 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m16:58:08.340661 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m16:58:08.342912 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m16:58:08.343673 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.344001 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m16:58:08.344247 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.344958 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m16:58:08.345648 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m16:58:08.346210 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m16:58:08.346413 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m16:58:08.346590 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m16:58:08.349930 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m16:58:08.350541 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.350755 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m16:58:08.350931 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.351382 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m16:58:08.351941 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m16:58:08.352354 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m16:58:08.352602 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m16:58:08.352805 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m16:58:08.356354 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m16:58:08.356914 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.357114 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m16:58:08.357362 [debug] [Thread-1  ]: finished collecting timing info
[0m16:58:08.357963 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m16:58:08.358921 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:58:08.359096 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m16:58:08.367179 [info ] [MainThread]: Done.
[0m16:58:08.371486 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m16:58:08.371833 [info ] [MainThread]: Building catalog
[0m16:58:08.373767 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop_dbt"
[0m16:58:08.374354 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.dim_customers
[0m16:58:08.374663 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.fct_orders
[0m16:58:08.374864 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_customers
[0m16:58:08.375046 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_orders
[0m16:58:08.375232 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_payments
[0m16:58:08.378125 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop"
[0m16:58:08.378558 [debug] [ThreadPool]: On "jaffle_shop": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:58:08.384194 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:08.384484 [debug] [ThreadPool]: Using databricks connection "jaffle_shop"
[0m16:58:08.384634 [debug] [ThreadPool]: On jaffle_shop: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

      select current_catalog()
  
[0m16:58:08.384767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:09.850834 [debug] [ThreadPool]: SQL status: OK in 1.47 seconds
[0m16:58:09.862761 [debug] [ThreadPool]: Using databricks connection "jaffle_shop"
[0m16:58:09.863106 [debug] [ThreadPool]: On jaffle_shop: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

    use catalog `default`
  
[0m16:58:10.476349 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

    use catalog `default`
  
[0m16:58:10.477015 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m16:58:10.477433 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:1592)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:58)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:55)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:55)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:96)
	at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentCatalog(CatalogManager.scala:135)
	at com.databricks.sql.DatabricksCatalogManager.setCurrentCatalog(DatabricksCatalogManager.scala:141)
	at org.apache.spark.sql.execution.command.SetCatalogCommand.run(SetCatalogCommand.scala:30)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:188)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:206)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:225)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.assertExecutedPlanPrepared(QueryExecution.scala:240)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:351)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m16:58:10.477818 [debug] [ThreadPool]: Databricks adapter: operation-id: b'\xe0\xe1C\xe2R\x97I)\xa9\x83\xd2e\x8b\x07\x86%'
[0m16:58:10.478402 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro use_catalog
[0m16:58:10.478754 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m16:58:10.483494 [debug] [ThreadPool]: Using databricks connection "jaffle_shop"
[0m16:58:10.483924 [debug] [ThreadPool]: On jaffle_shop: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

    use catalog `spark_catalog`
  
[0m16:58:10.963619 [debug] [ThreadPool]: SQL status: OK in 0.48 seconds
[0m16:58:10.965596 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about default.jaffle_shop: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m16:58:10.966009 [debug] [ThreadPool]: with database=default, schema=jaffle_shop, relations=[]
[0m16:58:10.966397 [debug] [ThreadPool]: On jaffle_shop: ROLLBACK
[0m16:58:10.966699 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:58:10.966975 [debug] [ThreadPool]: On jaffle_shop: Close
[0m16:58:11.424117 [debug] [ThreadPool]: Acquiring new databricks connection "stripe"
[0m16:58:11.425231 [debug] [ThreadPool]: On "stripe": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m16:58:11.430687 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:11.431133 [debug] [ThreadPool]: Using databricks connection "stripe"
[0m16:58:11.431442 [debug] [ThreadPool]: On stripe: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

      select current_catalog()
  
[0m16:58:11.431732 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:58:12.421178 [debug] [ThreadPool]: SQL status: OK in 0.99 seconds
[0m16:58:12.429985 [debug] [ThreadPool]: Using databricks connection "stripe"
[0m16:58:12.430401 [debug] [ThreadPool]: On stripe: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

    use catalog `default`
  
[0m16:58:13.024813 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

    use catalog `default`
  
[0m16:58:13.025679 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m16:58:13.026320 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:1592)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:58)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:55)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:55)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:96)
	at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentCatalog(CatalogManager.scala:135)
	at com.databricks.sql.DatabricksCatalogManager.setCurrentCatalog(DatabricksCatalogManager.scala:141)
	at org.apache.spark.sql.execution.command.SetCatalogCommand.run(SetCatalogCommand.scala:30)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:188)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:206)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:225)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.assertExecutedPlanPrepared(QueryExecution.scala:240)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:351)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m16:58:13.026692 [debug] [ThreadPool]: Databricks adapter: operation-id: b'\xb2\x12\xb1\n\x91,MR\xb8d O\x12\x13\x03\xc6'
[0m16:58:13.027271 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro use_catalog
[0m16:58:13.027612 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m16:58:13.033144 [debug] [ThreadPool]: Using databricks connection "stripe"
[0m16:58:13.033574 [debug] [ThreadPool]: On stripe: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

    use catalog `spark_catalog`
  
[0m16:58:13.513352 [debug] [ThreadPool]: SQL status: OK in 0.48 seconds
[0m16:58:13.515571 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about default.stripe: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m16:58:13.516004 [debug] [ThreadPool]: with database=default, schema=stripe, relations=[]
[0m16:58:13.516445 [debug] [ThreadPool]: On stripe: ROLLBACK
[0m16:58:13.516806 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m16:58:13.517140 [debug] [ThreadPool]: On stripe: Close
[0m16:58:13.986390 [info ] [MainThread]: Catalog written to /Users/tcsilva/workspace/pessoal/estudos/dbt/jaffle_shop/target/catalog.json
[0m16:58:13.987077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d4b1100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d811e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d811f40>]}
[0m16:58:13.987685 [debug] [MainThread]: Flushing usage events
[0m16:58:14.897251 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m16:58:14.897716 [debug] [MainThread]: Connection 'stripe' was properly closed.


============================== 2022-10-13 16:58:21.488980 | b2a80669-0339-418b-81be-f7babf895be5 ==============================
[0m16:58:21.489038 [info ] [MainThread]: Running with dbt=1.2.1
[0m16:58:21.490409 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m16:58:21.491040 [debug] [MainThread]: Tracking: tracking
[0m16:58:21.509872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d984a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9842e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d999610>]}
[0m16:58:21.513305 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m16:58:21.513818 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m16:58:21.514226 [info ] [MainThread]: 
[0m16:58:21.514467 [info ] [MainThread]: 
[0m16:58:21.514719 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-10-13 17:01:50.032811 | 37a95ced-e8a9-445b-9741-fdbd252d2936 ==============================
[0m17:01:50.032875 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:01:50.034073 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m17:01:50.034503 [debug] [MainThread]: Tracking: tracking
[0m17:01:50.053420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a7a7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a7a430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a7a640>]}
[0m17:01:50.106580 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:01:50.107103 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:01:50.107365 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:01:50.107593 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:01:50.120197 [debug] [MainThread]: 1603: static parser failed on staging/stripe/stg_payments.sql
[0m17:01:50.136309 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/stripe/stg_payments.sql
[0m17:01:50.138057 [debug] [MainThread]: 1603: static parser failed on staging/jaffle_shop/stg_orders.sql
[0m17:01:50.142980 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/jaffle_shop/stg_orders.sql
[0m17:01:50.144497 [debug] [MainThread]: 1603: static parser failed on staging/jaffle_shop/stg_customers.sql
[0m17:01:50.148597 [debug] [MainThread]: 1602: parser fallback to jinja rendering on staging/jaffle_shop/stg_customers.sql
[0m17:01:50.159802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '37a95ced-e8a9-445b-9741-fdbd252d2936', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c165e0>]}
[0m17:01:50.243284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '37a95ced-e8a9-445b-9741-fdbd252d2936', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118bdd7f0>]}
[0m17:01:50.244626 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:01:50.245642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37a95ced-e8a9-445b-9741-fdbd252d2936', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118bdd880>]}
[0m17:01:50.249118 [info ] [MainThread]: 
[0m17:01:50.250399 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:01:50.252436 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:01:50.269079 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:01:50.269493 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:01:50.269706 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:01:50.269907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:51.964257 [debug] [ThreadPool]: SQL status: OK in 1.69 seconds
[0m17:01:51.977280 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:01:51.977667 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:01:51.977961 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:01:52.781474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37a95ced-e8a9-445b-9741-fdbd252d2936', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118bf3a60>]}
[0m17:01:52.782891 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:01:52.783939 [info ] [MainThread]: 
[0m17:01:52.793282 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:01:52.794585 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:01:52.795258 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:01:52.795902 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:01:52.802381 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:01:52.803701 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.804175 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:01:52.804649 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.805599 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:01:52.805954 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:01:52.806415 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:01:52.806722 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:01:52.806973 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:01:52.809809 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:01:52.810436 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.810705 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:01:52.811049 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.811779 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:01:52.812098 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:01:52.812623 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:01:52.812842 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:01:52.813019 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:01:52.817810 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:01:52.818494 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.818735 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:01:52.818914 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.819385 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:01:52.819919 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:01:52.820302 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m17:01:52.820606 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m17:01:52.820909 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m17:01:52.824188 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m17:01:52.825023 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.825413 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m17:01:52.825721 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.826445 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:01:52.827257 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:01:52.827825 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m17:01:52.827989 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m17:01:52.828132 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m17:01:52.831224 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m17:01:52.831756 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.831972 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m17:01:52.832147 [debug] [Thread-1  ]: finished collecting timing info
[0m17:01:52.832628 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:01:52.833844 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:01:52.834167 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m17:01:52.843964 [info ] [MainThread]: Done.
[0m17:01:52.846497 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m17:01:52.846823 [info ] [MainThread]: Building catalog
[0m17:01:52.848861 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop_dbt"
[0m17:01:52.849446 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.dim_customers
[0m17:01:52.849889 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.fct_orders
[0m17:01:52.850213 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_customers
[0m17:01:52.850419 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_orders
[0m17:01:52.850650 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_payments
[0m17:01:52.853023 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop"
[0m17:01:52.853402 [debug] [ThreadPool]: On "jaffle_shop": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m17:01:52.859920 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:01:52.860236 [debug] [ThreadPool]: Using databricks connection "jaffle_shop"
[0m17:01:52.860391 [debug] [ThreadPool]: On jaffle_shop: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

      select current_catalog()
  
[0m17:01:52.860516 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:53.932554 [debug] [ThreadPool]: SQL status: OK in 1.07 seconds
[0m17:01:53.949530 [debug] [ThreadPool]: Using databricks connection "jaffle_shop"
[0m17:01:53.949908 [debug] [ThreadPool]: On jaffle_shop: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

    use catalog `default`
  
[0m17:01:54.560316 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

    use catalog `default`
  
[0m17:01:54.560931 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m17:01:54.561313 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:1592)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:58)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:55)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:55)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:96)
	at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentCatalog(CatalogManager.scala:135)
	at com.databricks.sql.DatabricksCatalogManager.setCurrentCatalog(DatabricksCatalogManager.scala:141)
	at org.apache.spark.sql.execution.command.SetCatalogCommand.run(SetCatalogCommand.scala:30)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:188)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:206)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:225)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.assertExecutedPlanPrepared(QueryExecution.scala:240)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:351)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:01:54.561686 [debug] [ThreadPool]: Databricks adapter: operation-id: b'?6\xcc\x85\xfb\x88M\xa6\xb2\xf5x\x9b\xd4\xd45\xa7'
[0m17:01:54.562262 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro use_catalog
[0m17:01:54.562598 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m17:01:54.567254 [debug] [ThreadPool]: Using databricks connection "jaffle_shop"
[0m17:01:54.567612 [debug] [ThreadPool]: On jaffle_shop: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "jaffle_shop"} */

    use catalog `spark_catalog`
  
[0m17:01:55.052928 [debug] [ThreadPool]: SQL status: OK in 0.49 seconds
[0m17:01:55.055231 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about default.jaffle_shop: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m17:01:55.055818 [debug] [ThreadPool]: with database=default, schema=jaffle_shop, relations=[]
[0m17:01:55.056294 [debug] [ThreadPool]: On jaffle_shop: ROLLBACK
[0m17:01:55.056647 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:01:55.056963 [debug] [ThreadPool]: On jaffle_shop: Close
[0m17:01:55.526685 [debug] [ThreadPool]: Acquiring new databricks connection "stripe"
[0m17:01:55.527514 [debug] [ThreadPool]: On "stripe": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m17:01:55.533330 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:01:55.533802 [debug] [ThreadPool]: Using databricks connection "stripe"
[0m17:01:55.534194 [debug] [ThreadPool]: On stripe: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

      select current_catalog()
  
[0m17:01:55.534534 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:01:56.528714 [debug] [ThreadPool]: SQL status: OK in 0.99 seconds
[0m17:01:56.538988 [debug] [ThreadPool]: Using databricks connection "stripe"
[0m17:01:56.539443 [debug] [ThreadPool]: On stripe: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

    use catalog `default`
  
[0m17:01:57.152313 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

    use catalog `default`
  
[0m17:01:57.152933 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m17:01:57.153317 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:1592)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:58)
	at org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:55)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:55)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:96)
	at org.apache.spark.sql.connector.catalog.CatalogManager.setCurrentCatalog(CatalogManager.scala:135)
	at com.databricks.sql.DatabricksCatalogManager.setCurrentCatalog(DatabricksCatalogManager.scala:141)
	at org.apache.spark.sql.execution.command.SetCatalogCommand.run(SetCatalogCommand.scala:30)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:78)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:89)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:239)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:186)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:141)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:336)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:160)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:167)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:575)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:551)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:156)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:156)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:141)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:186)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:191)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:188)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:206)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:225)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.assertExecutedPlanPrepared(QueryExecution.scala:240)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:351)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:01:57.153752 [debug] [ThreadPool]: Databricks adapter: operation-id: b'1\x92;.\xcf^I=\x87p\xcc\x1d9u\xad\xad'
[0m17:01:57.154320 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro use_catalog
[0m17:01:57.154646 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m17:01:57.158487 [debug] [ThreadPool]: Using databricks connection "stripe"
[0m17:01:57.158830 [debug] [ThreadPool]: On stripe: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "stripe"} */

    use catalog `spark_catalog`
  
[0m17:01:57.621297 [debug] [ThreadPool]: SQL status: OK in 0.46 seconds
[0m17:01:57.624155 [debug] [ThreadPool]: Databricks adapter: Error while retrieving information about default.stripe: Runtime Error
  Catalog 'default' plugin class not found: spark.sql.catalog.default is not defined
[0m17:01:57.624636 [debug] [ThreadPool]: with database=default, schema=stripe, relations=[]
[0m17:01:57.625097 [debug] [ThreadPool]: On stripe: ROLLBACK
[0m17:01:57.625550 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:01:57.625995 [debug] [ThreadPool]: On stripe: Close
[0m17:01:58.092424 [info ] [MainThread]: Catalog written to /Users/tcsilva/workspace/pessoal/estudos/dbt/jaffle_shop/target/catalog.json
[0m17:01:58.093131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a7a7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d6f610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d6f730>]}
[0m17:01:58.093599 [debug] [MainThread]: Flushing usage events
[0m17:01:58.849602 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m17:01:58.850203 [debug] [MainThread]: Connection 'stripe' was properly closed.


============================== 2022-10-13 17:02:04.621387 | 7845ac86-09e6-4305-b774-d83b442d0ed1 ==============================
[0m17:02:04.621470 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:02:04.622499 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m17:02:04.623232 [debug] [MainThread]: Tracking: tracking
[0m17:02:04.648452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123a47250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123a47b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123a47df0>]}
[0m17:02:04.652866 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m17:02:04.653359 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m17:02:04.653760 [info ] [MainThread]: 
[0m17:02:04.654183 [info ] [MainThread]: 
[0m17:02:04.654466 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-10-13 17:02:22.768994 | c262bc65-1c05-41bd-ba45-b03f92f4d0f7 ==============================
[0m17:02:22.769028 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:02:22.770097 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:02:22.770350 [debug] [MainThread]: Tracking: tracking
[0m17:02:22.793291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cef1b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cef2e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cef2e50>]}
[0m17:02:22.889745 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:02:22.890205 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:02:22.900716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d0bce80>]}
[0m17:02:22.909466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cffeb80>]}
[0m17:02:22.909813 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:02:22.910103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cffedf0>]}
[0m17:02:22.911728 [info ] [MainThread]: 
[0m17:02:22.912466 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:02:22.914071 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:02:22.925860 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:02:22.926144 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:02:22.926309 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:02:23.978515 [debug] [ThreadPool]: SQL status: OK in 1.05 seconds
[0m17:02:23.991510 [debug] [ThreadPool]: On list_schemas: Close
[0m17:02:24.440125 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:02:24.447639 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:02:24.447917 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:02:24.448109 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:02:24.448321 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:02:25.574742 [debug] [ThreadPool]: SQL status: OK in 1.13 seconds
[0m17:02:25.591886 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:02:25.592561 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:02:25.592964 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:02:26.049048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cef2e20>]}
[0m17:02:26.049910 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:02:26.050244 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:02:26.050910 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:02:26.051446 [info ] [MainThread]: 
[0m17:02:26.058995 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:02:26.059592 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:02:26.060363 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:02:26.060616 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:02:26.060855 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:02:26.064281 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:02:26.065014 [debug] [Thread-1  ]: finished collecting timing info
[0m17:02:26.065300 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:02:26.110319 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:02:26.110937 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:02:26.111160 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:02:26.111287 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from from ('default', 'jaffle_shop_customers')

[0m17:02:26.111409 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:02:27.116077 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from from ('default', 'jaffle_shop_customers')

[0m17:02:27.116596 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: could not resolve `from` to a table-valued function; line 11 pos 5
[0m17:02:27.116903 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: could not resolve `from` to a table-valued function; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: could not resolve `from` to a table-valued function; line 11 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:02:27.117195 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xa4\xd1\xbb\x8c>\xf2Hl\x98\xc4\x1e<\x9c)\xca\x88'
[0m17:02:27.117696 [debug] [Thread-1  ]: finished collecting timing info
[0m17:02:27.118057 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:02:27.118338 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:02:27.118600 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:02:27.567612 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  could not resolve `from` to a table-valued function; line 11 pos 5
[0m17:02:27.568714 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d09f4c0>]}
[0m17:02:27.569713 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.51s]
[0m17:02:27.570819 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:02:27.571338 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:02:27.571937 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:02:27.573719 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:02:27.574260 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:02:27.574677 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:02:27.580473 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:02:27.581391 [debug] [Thread-1  ]: finished collecting timing info
[0m17:02:27.581746 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:02:27.587708 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:02:27.588544 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:02:27.588997 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:02:27.589276 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from ('default', 'jaffle_shop_orders')

[0m17:02:27.589517 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:02:28.545239 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from ('default', 'jaffle_shop_orders')

[0m17:02:28.546146 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
no viable alternative at input '('default''(line 12, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from ('default', 'jaffle_shop_orders')
------^^^

[0m17:02:28.546683 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
no viable alternative at input '('default''(line 12, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from ('default', 'jaffle_shop_orders')
------^^^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
no viable alternative at input '('default''(line 12, pos 6)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from ('default', 'jaffle_shop_orders')
------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:298)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:159)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:88)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:106)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:97)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$3(SparkExecuteStatementOperation.scala:337)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:337)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:02:28.547050 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x04jc\xa5\xa6\xdfK]\x9a\xf2\xff\x9dA$\xdc\x1b'
[0m17:02:28.547588 [debug] [Thread-1  ]: finished collecting timing info
[0m17:02:28.547924 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:02:28.548196 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:02:28.548452 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:02:29.002375 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  
  no viable alternative at input '('default''(line 12, pos 6)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
  create or replace view jaffle_shop_dbt.stg_orders
    
    
    as
      select
      id as order_id,
      user_id as customer_id,
      order_date,
      status
  
  from ('default', 'jaffle_shop_orders')
  ------^^^
  
[0m17:02:29.003130 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1f9700>]}
[0m17:02:29.003822 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.43s]
[0m17:02:29.004601 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:02:29.005137 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:02:29.006309 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:02:29.007373 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:02:29.007739 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:02:29.008058 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:02:29.012636 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:02:29.013448 [debug] [Thread-1  ]: finished collecting timing info
[0m17:02:29.013836 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:02:29.019722 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:02:29.020592 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:02:29.020861 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:02:29.021076 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from from from ('default', 'stripe_payments')

[0m17:02:29.021292 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:02:29.985522 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from from from ('default', 'stripe_payments')

[0m17:02:29.986124 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: could not resolve `from` to a table-valued function; line 13 pos 10
[0m17:02:29.986554 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: could not resolve `from` to a table-valued function; line 13 pos 10
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: could not resolve `from` to a table-valued function; line 13 pos 10
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:02:29.986927 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\\\x97\xe4\xea\x8aE\x8a\x80\x98\x18\x046\xf2r\x82'
[0m17:02:29.987549 [debug] [Thread-1  ]: finished collecting timing info
[0m17:02:29.987972 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:02:29.988334 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:02:29.988752 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:02:30.437062 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  could not resolve `from` to a table-valued function; line 13 pos 10
[0m17:02:30.437856 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c262bc65-1c05-41bd-ba45-b03f92f4d0f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d22b1c0>]}
[0m17:02:30.438729 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.43s]
[0m17:02:30.439704 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:02:30.441705 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:02:30.442500 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:02:30.443787 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:02:30.445056 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:02:30.445633 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:02:30.446537 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:02:30.448289 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:02:30.448683 [debug] [MainThread]: On master: ROLLBACK
[0m17:02:30.448934 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:02:30.919989 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:02:30.920889 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:02:30.921456 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:02:30.922025 [debug] [MainThread]: On master: ROLLBACK
[0m17:02:30.922514 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:02:30.922840 [debug] [MainThread]: On master: Close
[0m17:02:31.380242 [info ] [MainThread]: 
[0m17:02:31.381549 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 8.47 seconds (8.47s).
[0m17:02:31.382533 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:02:31.383058 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:02:31.398457 [info ] [MainThread]: 
[0m17:02:31.399004 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:02:31.399533 [info ] [MainThread]: 
[0m17:02:31.399883 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:02:31.400147 [error] [MainThread]:   could not resolve `from` to a table-valued function; line 11 pos 5
[0m17:02:31.400412 [info ] [MainThread]: 
[0m17:02:31.400706 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:02:31.401366 [error] [MainThread]:   
[0m17:02:31.401710 [error] [MainThread]:   no viable alternative at input '('default''(line 12, pos 6)
[0m17:02:31.402086 [error] [MainThread]:   
[0m17:02:31.402369 [error] [MainThread]:   == SQL ==
[0m17:02:31.402611 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
[0m17:02:31.402852 [error] [MainThread]:   create or replace view jaffle_shop_dbt.stg_orders
[0m17:02:31.403097 [error] [MainThread]:     
[0m17:02:31.403483 [error] [MainThread]:     
[0m17:02:31.403852 [error] [MainThread]:     as
[0m17:02:31.404234 [error] [MainThread]:       select
[0m17:02:31.404591 [error] [MainThread]:       id as order_id,
[0m17:02:31.404937 [error] [MainThread]:       user_id as customer_id,
[0m17:02:31.405338 [error] [MainThread]:       order_date,
[0m17:02:31.405611 [error] [MainThread]:       status
[0m17:02:31.405834 [error] [MainThread]:   
[0m17:02:31.406053 [error] [MainThread]:   from ('default', 'jaffle_shop_orders')
[0m17:02:31.406269 [error] [MainThread]:   ------^^^
[0m17:02:31.406487 [error] [MainThread]:   
[0m17:02:31.406733 [info ] [MainThread]: 
[0m17:02:31.406976 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:02:31.407222 [error] [MainThread]:   could not resolve `from` to a table-valued function; line 13 pos 10
[0m17:02:31.407460 [info ] [MainThread]: 
[0m17:02:31.407706 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:02:31.408069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d149070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d149190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d223340>]}
[0m17:02:31.408309 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:03:32.170762 | bf824e35-0aa3-4484-ba62-4cb673f4a481 ==============================
[0m17:03:32.170815 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:03:32.171827 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m17:03:32.172070 [debug] [MainThread]: Tracking: tracking
[0m17:03:32.191862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141820d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114182430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114182640>]}
[0m17:03:32.247680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:03:32.248154 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:03:32.248418 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:03:32.248728 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql


============================== 2022-10-13 17:03:39.587254 | 0a127866-7948-4c38-a2f0-19db18963276 ==============================
[0m17:03:39.587307 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:03:39.588508 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:03:39.588951 [debug] [MainThread]: Tracking: tracking
[0m17:03:39.606330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bbe85e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bbe87f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bbe8ee0>]}
[0m17:03:39.660426 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:03:39.660979 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:03:39.661240 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:03:39.661467 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:03:39.675678 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:03:39.692119 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:03:39.695391 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:03:39.697745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bd8d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bda4e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bda4e20>]}
[0m17:03:39.698074 [debug] [MainThread]: Flushing usage events
[0m17:03:40.358302 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:03:40.368848 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:05:17.134314 | b85222e3-6ef6-4b01-a495-4a12451d189b ==============================
[0m17:05:17.134425 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:05:17.135637 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:05:17.136097 [debug] [MainThread]: Tracking: tracking
[0m17:05:17.155302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbcd5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbcd7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbcdee0>]}
[0m17:05:17.239680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:05:17.240373 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:05:17.240684 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:05:17.241007 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:05:17.255672 [debug] [MainThread]: 1603: static parser failed on staging/jaffle_shop/stg_customers.sql
[0m17:05:17.261291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dd654c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ddaaf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ddaa1c0>]}
[0m17:05:17.261698 [debug] [MainThread]: Flushing usage events
[0m17:05:19.266489 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  source() takes exactly two arguments (1 given)
[0m17:05:19.285156 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 349, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 473, in parse_project
    parser.parse_file(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/base.py", line 392, in parse_file
    self.parse_node(file_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/base.py", line 366, in parse_node
    self.render_update(node, config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/models.py", line 147, in render_update
    super().render_update(node, config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/base.py", line 341, in render_update
    context = self.render_with_context(node, config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/base.py", line 238, in render_with_context
    get_rendered(parsed_node.raw_sql, context, parsed_node, capture_macros=True)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/jinja.py", line 571, in get_rendered
    return render_template(template, ctx, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/jinja.py", line 526, in render_template
    return template.render(ctx)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/jinja2/environment.py", line 1090, in render
    self.environment.handle_exception()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/jinja2/environment.py", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/jinja2/_compat.py", line 28, in reraise
    raise value.with_traceback(tb)
  File "<template>", line 6, in top-level template code
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/jinja2/sandbox.py", line 462, in call
    return __context.call(__obj, *args, **kwargs)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/context/providers.py", line 264, in __call__
    raise_compiler_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  source() takes exactly two arguments (1 given)



============================== 2022-10-13 17:05:53.024408 | 4884486c-9609-4f49-8240-77aad873ae61 ==============================
[0m17:05:53.024522 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:05:53.026435 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:05:53.026698 [debug] [MainThread]: Tracking: tracking
[0m17:05:53.048860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1140695e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1140697f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114069ee0>]}
[0m17:05:53.132806 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:05:53.133320 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:05:53.133637 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:05:53.133899 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:05:53.148574 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:05:53.162574 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:05:53.165311 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:05:53.167252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11420d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114225e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114225e20>]}
[0m17:05:53.167520 [debug] [MainThread]: Flushing usage events
[0m17:05:53.823503 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named 'default.stripe_payments' which was not found
[0m17:05:53.831713 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named 'default.stripe_payments' which was not found



============================== 2022-10-13 17:06:36.827310 | c773911d-3f1e-4658-ac43-f50d6ea03170 ==============================
[0m17:06:36.827473 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:06:36.828408 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:06:36.828980 [debug] [MainThread]: Tracking: tracking
[0m17:06:36.847981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f605730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f605a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f6056a0>]}
[0m17:06:36.934833 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:06:36.935810 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:06:36.936528 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:06:36.937054 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:06:36.954403 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:06:36.970292 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:06:36.973427 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:06:36.975642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f799460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f7b1e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f7b1df0>]}
[0m17:06:36.976102 [debug] [MainThread]: Flushing usage events
[0m17:06:37.645042 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named 'default.stripe_payments' which was not found
[0m17:06:37.653741 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named 'default.stripe_payments' which was not found



============================== 2022-10-13 17:07:31.170544 | 786c278d-6242-4bf8-9228-4b4ab25c9e3a ==============================
[0m17:07:31.170580 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:07:31.171705 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:07:31.172023 [debug] [MainThread]: Tracking: tracking
[0m17:07:31.189705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221115e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221117f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122111ee0>]}
[0m17:07:31.244479 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 3 files changed.
[0m17:07:31.245045 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:07:31.245382 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:07:31.245637 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:07:31.260730 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:07:31.275130 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:07:31.277732 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:07:31.279640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222b74f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222d0eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222d0e80>]}
[0m17:07:31.279895 [debug] [MainThread]: Flushing usage events
[0m17:07:31.957477 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named 'default.stripe_payments' which was not found
[0m17:07:31.961309 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named 'default.stripe_payments' which was not found



============================== 2022-10-13 17:09:41.893544 | c0734dc5-656b-4a45-94d9-41bcdee03b4a ==============================
[0m17:09:41.893629 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:09:41.895151 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:09:41.895593 [debug] [MainThread]: Tracking: tracking
[0m17:09:41.916456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f4d460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f4dbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f4d9d0>]}
[0m17:09:42.002630 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:09:42.003397 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:09:42.003825 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:09:42.004096 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:09:42.004382 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:09:42.020160 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:09:42.033595 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:09:42.036644 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:09:42.041903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a129f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0ed8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0edcd0>]}
[0m17:09:42.042214 [debug] [MainThread]: Flushing usage events
[0m17:09:42.799616 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:09:42.809850 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:09:56.127707 | e52440fc-d955-4e3c-83de-4214d5d52cae ==============================
[0m17:09:56.127742 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:09:56.128800 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:09:56.129043 [debug] [MainThread]: Tracking: tracking
[0m17:09:56.148400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1219ec0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1219ec430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1219ec640>]}
[0m17:09:56.245260 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:09:56.245761 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:09:56.246070 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:09:56.246338 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:09:56.246774 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:09:56.262077 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:09:56.275880 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:09:56.278829 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:09:56.284370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121bc9ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121b80550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121b80940>]}
[0m17:09:56.284691 [debug] [MainThread]: Flushing usage events
[0m17:09:56.891236 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named '.stripe_payments' which was not found
[0m17:09:56.899135 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named '.stripe_payments' which was not found



============================== 2022-10-13 17:10:56.846730 | a3110495-ed92-468c-80e7-cd5d8dd5e6bc ==============================
[0m17:10:56.846859 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:10:56.848726 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:10:56.849092 [debug] [MainThread]: Tracking: tracking
[0m17:10:56.867411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114be67c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114be6430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114be6640>]}
[0m17:10:56.959791 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:10:56.960297 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:10:56.960600 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:10:56.960873 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:10:56.961111 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:10:56.974728 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:10:56.988350 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:10:56.991466 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:10:56.998403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114dc3ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d78550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d78940>]}
[0m17:10:56.998824 [debug] [MainThread]: Flushing usage events
[0m17:10:57.683479 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found
[0m17:10:57.694084 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found



============================== 2022-10-13 17:11:31.018557 | 1798d057-67d7-4ea3-a56c-4285b63aac59 ==============================
[0m17:11:31.018642 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:11:31.019670 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:11:31.019901 [debug] [MainThread]: Tracking: tracking
[0m17:11:31.038190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1232385e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1232387f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123238ee0>]}
[0m17:11:31.133360 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:11:31.133964 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:11:31.134296 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:11:31.134537 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:11:31.134767 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:11:31.148293 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:11:31.163385 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:11:31.166773 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:11:31.173848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12342bf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1233df5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1233df9a0>]}
[0m17:11:31.174211 [debug] [MainThread]: Flushing usage events
[0m17:11:32.101448 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:11:32.108452 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:11:52.670453 | ecd210e2-9981-4a15-ad6d-b8dfaba4286e ==============================
[0m17:11:52.670484 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:11:52.671398 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:11:52.671597 [debug] [MainThread]: Tracking: tracking
[0m17:11:52.688436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fa987c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fa98430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fa98640>]}
[0m17:11:52.743911 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:11:52.744480 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:11:52.744848 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:11:52.745096 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:11:52.745327 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:11:52.765327 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:11:52.779062 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:11:52.781917 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:11:52.787400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc77ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc2c550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc2c940>]}
[0m17:11:52.787728 [debug] [MainThread]: Flushing usage events
[0m17:11:53.427948 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found
[0m17:11:53.431220 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found



============================== 2022-10-13 17:12:17.726466 | 45bc3751-c99d-41bd-a6a8-6b86afd732cd ==============================
[0m17:12:17.726499 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:12:17.728127 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:12:17.728348 [debug] [MainThread]: Tracking: tracking
[0m17:12:17.751960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f35460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f35bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f359d0>]}
[0m17:12:17.806198 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 4 files changed.
[0m17:12:17.807064 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:12:17.807486 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:12:17.807823 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:12:17.808135 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:12:17.822758 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:12:17.837002 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:12:17.840511 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:12:17.846932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d20bf70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1ce8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1cecd0>]}
[0m17:12:17.847288 [debug] [MainThread]: Flushing usage events
[0m17:12:18.449975 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:12:18.454071 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:12:34.337920 | 23403e1a-0417-44f5-a55f-d44bf1c15fb4 ==============================
[0m17:12:34.337956 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:12:34.338988 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:12:34.339426 [debug] [MainThread]: Tracking: tracking
[0m17:12:34.361159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114745730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114745a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147456a0>]}
[0m17:12:34.426813 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:12:34.427189 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:12:34.427352 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:12:34.427496 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:12:34.427947 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:12:34.428231 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:12:34.428631 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:12:34.448866 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:12:34.467788 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:12:34.471130 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:12:34.473635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148db460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148f5e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148f5df0>]}
[0m17:12:34.474171 [debug] [MainThread]: Flushing usage events
[0m17:12:35.135246 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found
[0m17:12:35.138766 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found



============================== 2022-10-13 17:13:27.775597 | 408a9970-182c-4283-91cc-4d747e67e39a ==============================
[0m17:13:27.775657 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:13:27.777142 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:13:27.777530 [debug] [MainThread]: Tracking: tracking
[0m17:13:27.803014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f7230d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f723430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f723640>]}
[0m17:13:27.891998 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:13:27.892366 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:13:27.892549 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:13:27.892683 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:13:27.893091 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:13:27.893340 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:13:27.893576 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:13:27.910191 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:13:27.924356 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:13:27.926949 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:13:27.928891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f8b5430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f8cfdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f8cfdc0>]}
[0m17:13:27.929194 [debug] [MainThread]: Flushing usage events
[0m17:13:28.620318 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:13:28.629546 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:14:32.201209 | e5c5e006-aa9a-4051-b7d5-4f85365ab34b ==============================
[0m17:14:32.201387 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:14:32.203359 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:14:32.203811 [debug] [MainThread]: Tracking: tracking
[0m17:14:32.225185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117cb15e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117cb17f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117cb1ee0>]}
[0m17:14:32.310813 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:14:32.311165 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:14:32.311595 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:14:32.311940 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:14:32.312413 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:14:32.312771 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:14:32.313056 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:14:32.327547 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:14:32.341090 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:14:32.343655 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:14:32.345564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e57490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e71e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e71e20>]}
[0m17:14:32.345870 [debug] [MainThread]: Flushing usage events
[0m17:14:33.135576 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named '.stripe_payments' which was not found
[0m17:14:33.145890 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named '.stripe_payments' which was not found



============================== 2022-10-13 17:15:36.234480 | 6a5d08aa-1713-4795-8be0-c17f5d181743 ==============================
[0m17:15:36.234525 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:15:36.235923 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:15:36.236419 [debug] [MainThread]: Tracking: tracking
[0m17:15:36.256319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abd9460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abd9bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abd99d0>]}
[0m17:15:36.343212 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:15:36.343825 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:15:36.344029 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:15:36.344201 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:15:36.344549 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:15:36.344819 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:15:36.345054 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:15:36.360997 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:15:36.375946 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:15:36.378784 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:15:36.381017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad7f4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad9ae80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad9ae50>]}
[0m17:15:36.381334 [debug] [MainThread]: Flushing usage events
[0m17:15:37.060567 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:15:37.069146 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:18:04.613472 | c598acad-e44d-486e-9f1f-231f1ec480cd ==============================
[0m17:18:04.613517 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:18:04.615174 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:18:04.615464 [debug] [MainThread]: Tracking: tracking
[0m17:18:04.634467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12354d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12354d7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12354dee0>]}
[0m17:18:04.721861 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:18:04.722226 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:18:04.722402 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:18:04.722549 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:18:04.722860 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:18:04.723089 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:18:04.723305 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:18:04.736555 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:18:04.751376 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:18:04.755094 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:18:04.757310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236f2490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12370de50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12370de20>]}
[0m17:18:04.757722 [debug] [MainThread]: Flushing usage events
[0m17:18:05.635121 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named '.stripe_payments' which was not found
[0m17:18:05.643837 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Model 'model.jaffle_shop.stg_payments' (models/staging/stripe/stg_payments.sql) depends on a source named '.stripe_payments' which was not found



============================== 2022-10-13 17:20:07.515301 | e1fc0b03-460f-461b-9529-e88957bdf765 ==============================
[0m17:20:07.515313 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:20:07.515671 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m17:20:07.515832 [debug] [MainThread]: Tracking: tracking
[0m17:20:07.530376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10591be80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10591b9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10591b790>]}
[0m17:20:08.049481 [debug] [MainThread]: Executing "git --help"
[0m17:20:08.059924 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m17:20:08.060615 [debug] [MainThread]: STDERR: "b''"
[0m17:20:08.066788 [debug] [MainThread]: Acquiring new databricks connection "debug"
[0m17:20:08.069247 [debug] [MainThread]: Using databricks connection "debug"
[0m17:20:08.069865 [debug] [MainThread]: On debug: select 1 as id
[0m17:20:08.070096 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:20:10.072219 [debug] [MainThread]: SQL status: OK in 2.0 seconds
[0m17:20:10.075359 [debug] [MainThread]: On debug: Close
[0m17:20:10.758225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f0c040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f0c940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f0c220>]}
[0m17:20:10.759717 [debug] [MainThread]: Flushing usage events
[0m17:20:11.534585 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-10-13 17:20:18.507460 | 0a7a005c-cf6c-4161-9bcf-dcfd76659b4d ==============================
[0m17:20:18.507493 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:20:18.508701 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:20:18.508932 [debug] [MainThread]: Tracking: tracking
[0m17:20:18.524228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a01b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a02e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a02e50>]}
[0m17:20:18.577336 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:20:18.577684 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:20:18.577832 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:20:18.577979 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:20:18.578431 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:20:18.578681 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:20:18.578902 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:20:18.592955 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:20:18.608748 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:20:18.612095 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:20:18.614176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ba6400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bc1dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bc1d90>]}
[0m17:20:18.614461 [debug] [MainThread]: Flushing usage events
[0m17:20:19.326889 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found
[0m17:20:19.330276 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Model 'model.jaffle_shop.stg_customers' (models/staging/jaffle_shop/stg_customers.sql) depends on a source named 'default.jaffle_shop_customers' which was not found



============================== 2022-10-13 17:20:44.263834 | 82fca9d9-7416-4997-a4d5-c318b7abef89 ==============================
[0m17:20:44.263903 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:20:44.264850 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:20:44.265074 [debug] [MainThread]: Tracking: tracking
[0m17:20:44.281261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c295e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c297f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c29ee0>]}
[0m17:20:44.336944 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 0 files added, 3 files changed.
[0m17:20:44.337406 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:20:44.337622 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:20:44.337764 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:20:44.338171 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:20:44.338427 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:20:44.338657 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:20:44.354243 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:20:44.369761 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:20:44.372460 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:20:44.385865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e750d0>]}
[0m17:20:44.396290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119da0820>]}
[0m17:20:44.396750 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m17:20:44.397049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119da07f0>]}
[0m17:20:44.398605 [info ] [MainThread]: 
[0m17:20:44.399300 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:20:44.400751 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:20:44.411495 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:20:44.411752 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:20:44.411956 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:20:45.456965 [debug] [ThreadPool]: SQL status: OK in 1.04 seconds
[0m17:20:45.470108 [debug] [ThreadPool]: On list_schemas: Close
[0m17:20:46.056540 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:20:46.068332 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:46.068723 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:20:46.068984 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:20:46.069243 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:20:47.216968 [debug] [ThreadPool]: SQL status: OK in 1.15 seconds
[0m17:20:47.223737 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:20:47.224171 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:20:47.224483 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:20:47.676234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dcfe20>]}
[0m17:20:47.677570 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:47.678179 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:20:47.679340 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:20:47.680198 [info ] [MainThread]: 
[0m17:20:47.689689 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:20:47.690401 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:20:47.691353 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:20:47.691700 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:20:47.692338 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:20:47.695209 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:20:47.696075 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:47.696402 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:20:47.730678 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:20:47.732077 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:47.732354 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:20:47.732557 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop_customers

[0m17:20:47.732731 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:20:49.466295 [debug] [Thread-1  ]: SQL status: OK in 1.73 seconds
[0m17:20:49.515158 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:49.515502 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:20:49.515764 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:20:49.516048 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:20:49.967748 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f2bd90>]}
[0m17:20:49.968248 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.28s]
[0m17:20:49.968724 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:20:49.969126 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:20:49.969738 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:20:49.970325 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:20:49.970523 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:20:49.970688 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:20:49.972344 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:20:49.972956 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:49.973190 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:20:49.977762 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:20:49.978362 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:49.978546 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:20:49.978699 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop_orders

[0m17:20:49.978834 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:20:51.516727 [debug] [Thread-1  ]: SQL status: OK in 1.54 seconds
[0m17:20:51.522033 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:51.522660 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:20:51.523068 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:20:51.523422 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:20:51.975585 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c295e0>]}
[0m17:20:51.976985 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 2.01s]
[0m17:20:51.977951 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:20:51.978595 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:20:51.979751 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:20:51.980943 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:20:51.981352 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:20:51.981727 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:20:51.984924 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:20:51.986245 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:51.986683 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:20:51.993381 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:20:51.994274 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:51.994598 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:20:51.994852 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe_payments

[0m17:20:51.995076 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:20:53.651270 [debug] [Thread-1  ]: SQL status: OK in 1.66 seconds
[0m17:20:53.656161 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:53.656625 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:20:53.656959 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:20:53.657193 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:20:54.108115 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e5cdf0>]}
[0m17:20:54.109167 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 2.13s]
[0m17:20:54.110240 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:20:54.111812 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:20:54.112495 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m17:20:54.113440 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m17:20:54.113766 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m17:20:54.114068 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m17:20:54.118616 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m17:20:54.119392 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:54.119710 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m17:20:54.169968 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m17:20:54.170762 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:54.171020 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m17:20:54.171205 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m17:20:54.171343 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:20:58.172703 [debug] [Thread-1  ]: SQL status: OK in 4.0 seconds
[0m17:20:58.194818 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:58.195266 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m17:20:58.195501 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:20:58.195704 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m17:20:58.652323 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f837f0>]}
[0m17:20:58.653433 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.54s]
[0m17:20:58.654502 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:20:58.656910 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:20:58.657512 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m17:20:58.658575 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m17:20:58.658941 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m17:20:58.659247 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m17:20:58.670408 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m17:20:58.671196 [debug] [Thread-1  ]: finished collecting timing info
[0m17:20:58.671475 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m17:20:58.677736 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m17:20:58.678363 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:20:58.678568 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m17:20:58.678740 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m17:20:58.678911 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:21:09.977546 [debug] [Thread-1  ]: SQL status: OK in 11.3 seconds
[0m17:21:10.433448 [debug] [Thread-1  ]: finished collecting timing info
[0m17:21:10.433762 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m17:21:10.433920 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:21:10.434052 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m17:21:10.883760 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '82fca9d9-7416-4997-a4d5-c318b7abef89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f4d1c0>]}
[0m17:21:10.895284 [info ] [Thread-1  ]: 5 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 12.23s]
[0m17:21:10.896504 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:21:10.898447 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:21:10.898833 [debug] [MainThread]: On master: ROLLBACK
[0m17:21:10.899109 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:21:11.367063 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:21:11.367411 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:21:11.367592 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:21:11.367754 [debug] [MainThread]: On master: ROLLBACK
[0m17:21:11.367892 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:21:11.368040 [debug] [MainThread]: On master: Close
[0m17:21:11.813439 [info ] [MainThread]: 
[0m17:21:11.814467 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 27.41 seconds (27.41s).
[0m17:21:11.814777 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:21:11.814912 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m17:21:11.823854 [info ] [MainThread]: 
[0m17:21:11.824300 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:21:11.824633 [info ] [MainThread]: 
[0m17:21:11.825057 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m17:21:11.825442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f83be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c47d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ef5bb0>]}
[0m17:21:11.825724 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:24:58.427737 | 5ca64ffb-9db9-41d1-834a-c109fa9c75e4 ==============================
[0m17:24:58.427787 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:24:58.429579 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:24:58.429970 [debug] [MainThread]: Tracking: tracking
[0m17:24:58.455783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119fc5280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119fc5a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119fc56a0>]}
[0m17:24:58.551420 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 3 files changed.
[0m17:24:58.552004 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:24:58.552312 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:24:58.552612 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:24:58.552866 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:24:58.553100 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:24:58.567777 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:24:58.581142 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:24:58.583930 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:24:58.603934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a160640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a1bfdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a1acd00>]}
[0m17:24:58.604339 [debug] [MainThread]: Flushing usage events
[0m17:24:59.662508 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found
[0m17:24:59.670486 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'default.jaffle_shop_orders' which was not found



============================== 2022-10-13 17:26:07.265023 | 90547cf8-0779-4f20-81cd-ace92be3d583 ==============================
[0m17:26:07.265060 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:26:07.266620 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:26:07.267008 [debug] [MainThread]: Tracking: tracking
[0m17:26:07.285302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b45db80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b4621f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b4621c0>]}
[0m17:26:07.388938 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 3 files changed.
[0m17:26:07.389395 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:26:07.389779 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:26:07.390289 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:26:07.390587 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:26:07.390822 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:26:07.408655 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:26:07.423799 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:26:07.426685 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:26:07.448323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b6085e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b667d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b667940>]}
[0m17:26:07.448694 [debug] [MainThread]: Flushing usage events
[0m17:26:08.108760 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'stripe.jaffle_shop_orders' which was not found
[0m17:26:08.115389 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'stripe.jaffle_shop_orders' which was not found



============================== 2022-10-13 17:27:04.481662 | 67532d51-8ed6-41cc-931b-c8a96674ac8d ==============================
[0m17:27:04.481749 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:27:04.483369 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:27:04.483594 [debug] [MainThread]: Tracking: tracking
[0m17:27:04.501556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c04730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c04a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c046a0>]}
[0m17:27:04.554071 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 2 files added, 3 files changed.
[0m17:27:04.554807 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:27:04.555272 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:27:04.555588 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:27:04.555894 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:27:04.556155 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:27:04.570408 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:27:04.582955 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:27:04.585554 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:27:04.619170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111da50d0>]}
[0m17:27:04.626275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d670a0>]}
[0m17:27:04.626681 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:27:04.626987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d670d0>]}
[0m17:27:04.628507 [info ] [MainThread]: 
[0m17:27:04.629048 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:27:04.630042 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:27:04.639954 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:27:04.640219 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:27:04.640378 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:27:06.264312 [debug] [ThreadPool]: SQL status: OK in 1.62 seconds
[0m17:27:06.277198 [debug] [ThreadPool]: On list_schemas: Close
[0m17:27:06.911297 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:27:06.920545 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:27:06.920877 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:27:06.921096 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:27:06.921291 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:27:08.367160 [debug] [ThreadPool]: SQL status: OK in 1.45 seconds
[0m17:27:08.373360 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:27:08.373819 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:27:08.374137 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:27:08.832777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111d7ae50>]}
[0m17:27:08.833859 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:27:08.834552 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:27:08.835446 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:27:08.836131 [info ] [MainThread]: 
[0m17:27:08.846962 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:27:08.847590 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:27:08.848461 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:27:08.848773 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:27:08.849072 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:27:08.855009 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:27:08.856193 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:08.856565 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:27:08.890530 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:27:08.891203 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:27:08.891372 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:27:08.891501 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.jaffle_shop_customers

[0m17:27:08.891624 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:27:10.069077 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.jaffle_shop_customers

[0m17:27:10.069704 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:27:10.070084 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:27:10.070438 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'j3W\x91\x0b\xe9I\xbe\x83\xa3\xb7\xd2LhA3'
[0m17:27:10.071187 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:10.071896 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:27:10.072297 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:27:10.072695 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:27:10.524000 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Catalog namespace is not supported.
[0m17:27:10.525194 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f371f0>]}
[0m17:27:10.526026 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.68s]
[0m17:27:10.526999 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:27:10.527486 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:27:10.528925 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:27:10.530076 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:27:10.530479 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:27:10.531128 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:27:10.536259 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:27:10.537246 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:10.537562 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:27:10.543697 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:27:10.544504 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:27:10.544789 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:27:10.545026 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.jaffle_shop_orders

[0m17:27:10.545418 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:27:11.931930 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.jaffle_shop_orders

[0m17:27:11.932548 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:27:11.932918 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:27:11.933267 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xc3\x93\xd1\xb5\xf1cF\x9b\x81\xae\xd2\x8cZ\x10F\xef'
[0m17:27:11.933860 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:11.934262 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:27:11.934603 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:27:11.934925 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:27:12.400863 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Catalog namespace is not supported.
[0m17:27:12.402223 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f5f400>]}
[0m17:27:12.403150 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.87s]
[0m17:27:12.404285 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:27:12.404805 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:27:12.405954 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:27:12.407201 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:27:12.407692 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:27:12.408136 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:27:12.412857 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:27:12.413921 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:12.414265 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:27:12.419926 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:27:12.420597 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:27:12.420802 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:27:12.420973 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.stripe_payments

[0m17:27:12.421139 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:27:13.485875 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.stripe_payments

[0m17:27:13.486777 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:27:13.487334 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:27:13.487684 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'n\xd4#"\xfe\xf3C\xac\xa1\'\xd6\xc5\xc3\xe7i\x07'
[0m17:27:13.488266 [debug] [Thread-1  ]: finished collecting timing info
[0m17:27:13.488660 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:27:13.488981 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:27:13.489291 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:27:13.941488 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:27:13.942343 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '67532d51-8ed6-41cc-931b-c8a96674ac8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f8e220>]}
[0m17:27:13.943627 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.54s]
[0m17:27:13.945038 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:27:13.947110 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:27:13.947949 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:27:13.949028 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:27:13.950381 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:27:13.951118 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:27:13.951993 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:27:13.954689 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:27:13.955079 [debug] [MainThread]: On master: ROLLBACK
[0m17:27:13.955383 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:27:14.437918 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:27:14.438797 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:27:14.439337 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:27:14.439874 [debug] [MainThread]: On master: ROLLBACK
[0m17:27:14.440377 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:27:14.440705 [debug] [MainThread]: On master: Close
[0m17:27:14.902148 [info ] [MainThread]: 
[0m17:27:14.903588 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 10.27 seconds (10.27s).
[0m17:27:14.904649 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:27:14.905220 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:27:14.922462 [info ] [MainThread]: 
[0m17:27:14.923133 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:27:14.923535 [info ] [MainThread]: 
[0m17:27:14.923846 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:27:14.924181 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:27:14.924476 [info ] [MainThread]: 
[0m17:27:14.925099 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:27:14.925390 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:27:14.925682 [info ] [MainThread]: 
[0m17:27:14.925958 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:27:14.926225 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:27:14.926505 [info ] [MainThread]: 
[0m17:27:14.926790 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:27:14.927190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111c12fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f6e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111f6e7f0>]}
[0m17:27:14.927499 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:28:15.454703 | 47803d66-98a1-4560-9a17-df90550b71ae ==============================
[0m17:28:15.454736 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:28:15.455936 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:28:15.456401 [debug] [MainThread]: Tracking: tracking
[0m17:28:15.473589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b2715e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b2717f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b271ee0>]}
[0m17:28:15.525166 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m17:28:15.525870 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:28:15.526082 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:28:15.526374 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:28:15.526565 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:28:15.526700 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:28:15.526875 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:28:15.527046 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:28:15.527212 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:28:15.540063 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:28:15.555569 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:28:15.559073 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:28:15.562561 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:28:15.565695 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:28:15.594326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b42c0d0>]}
[0m17:28:15.602612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b271730>]}
[0m17:28:15.603034 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:28:15.603343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b28fca0>]}
[0m17:28:15.604950 [info ] [MainThread]: 
[0m17:28:15.605549 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:28:15.606591 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:28:15.617950 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:28:15.618426 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:28:15.618664 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:28:16.639456 [debug] [ThreadPool]: SQL status: OK in 1.02 seconds
[0m17:28:16.652199 [debug] [ThreadPool]: On list_schemas: Close
[0m17:28:17.128064 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:28:17.142591 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:28:17.142972 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:28:17.143232 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:28:17.143471 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:28:18.613657 [debug] [ThreadPool]: SQL status: OK in 1.47 seconds
[0m17:28:18.619334 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:28:18.619717 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:28:18.619971 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:28:19.441937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b401ca0>]}
[0m17:28:19.443087 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:28:19.443747 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:28:19.444648 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:28:19.445363 [info ] [MainThread]: 
[0m17:28:19.453275 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:28:19.453919 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:28:19.454964 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:28:19.455343 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:28:19.455721 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:28:19.460269 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:28:19.460992 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:19.461294 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:28:19.493585 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:28:19.494194 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:28:19.494398 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:28:19.494546 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.customers

[0m17:28:19.494686 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:28:21.480254 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.customers

[0m17:28:21.481147 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:28:21.481807 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:28:21.482320 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x14\x0cLN\xb4\x01I\x86\x8c\xdb\xd44\x1d\xf5.\xed'
[0m17:28:21.482989 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:21.483404 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:28:21.483884 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:28:21.484282 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:28:22.403804 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Catalog namespace is not supported.
[0m17:28:22.404672 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5924c0>]}
[0m17:28:22.405472 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 2.95s]
[0m17:28:22.406599 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:28:22.407467 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:28:22.408458 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:28:22.409444 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:28:22.409919 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:28:22.410212 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:28:22.415530 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:28:22.416215 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:22.416494 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:28:22.421385 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:28:22.422086 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:28:22.422330 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:28:22.422511 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.orders

[0m17:28:22.422680 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:28:23.553107 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.orders

[0m17:28:23.553746 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:28:23.554155 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:28:23.554517 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xfeOr\x9a\xf1\x9aGX\x902\x93\xd5\x8c\x1f\xa3\xc7'
[0m17:28:23.555217 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:23.555639 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:28:23.555976 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:28:23.556299 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:28:24.008612 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Catalog namespace is not supported.
[0m17:28:24.009501 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5f23a0>]}
[0m17:28:24.010410 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.60s]
[0m17:28:24.011876 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:28:24.012875 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:28:24.014104 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:28:24.015355 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:28:24.015809 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:28:24.016127 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:28:24.020834 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:28:24.021629 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:24.022017 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:28:24.027494 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:28:24.028314 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:28:24.028556 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:28:24.028754 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.payments

[0m17:28:24.028947 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:28:25.108402 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.payments

[0m17:28:25.109242 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:28:25.109727 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:28:25.110118 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'}\xc7Yg\xe9\xacK\x92\x96\x99\xa6\xac9D\xdf\xe6'
[0m17:28:25.110761 [debug] [Thread-1  ]: finished collecting timing info
[0m17:28:25.111192 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:28:25.111549 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:28:25.111886 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:28:25.580647 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:28:25.581854 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '47803d66-98a1-4560-9a17-df90550b71ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b61e280>]}
[0m17:28:25.582801 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.57s]
[0m17:28:25.583710 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:28:25.585420 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:28:25.586013 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:28:25.587177 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:28:25.588632 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:28:25.589010 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:28:25.589687 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:28:25.591878 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:28:25.592426 [debug] [MainThread]: On master: ROLLBACK
[0m17:28:25.592817 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:28:26.089679 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:28:26.090566 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:28:26.091136 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:28:26.091757 [debug] [MainThread]: On master: ROLLBACK
[0m17:28:26.092646 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:28:26.093138 [debug] [MainThread]: On master: Close
[0m17:28:26.587680 [info ] [MainThread]: 
[0m17:28:26.588994 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 10.98 seconds (10.98s).
[0m17:28:26.589879 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:28:26.590362 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:28:26.613832 [info ] [MainThread]: 
[0m17:28:26.614436 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:28:26.614948 [info ] [MainThread]: 
[0m17:28:26.615271 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:28:26.615804 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:28:26.616127 [info ] [MainThread]: 
[0m17:28:26.616411 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:28:26.616681 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:28:26.616950 [info ] [MainThread]: 
[0m17:28:26.617232 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:28:26.617511 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:28:26.617788 [info ] [MainThread]: 
[0m17:28:26.618074 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:28:26.618465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b487ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b4876d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b6138b0>]}
[0m17:28:26.618795 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:30:22.472915 | 58cfc0d3-28a4-484a-8808-5aa36e06d835 ==============================
[0m17:30:22.472963 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:30:22.474489 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:30:22.474747 [debug] [MainThread]: Tracking: tracking
[0m17:30:22.492542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11578a520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11578a670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11578a8b0>]}
[0m17:30:22.546675 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:30:22.547024 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:30:22.554422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115941ee0>]}
[0m17:30:22.562593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115882be0>]}
[0m17:30:22.563023 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:30:22.563363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115882e50>]}
[0m17:30:22.565164 [info ] [MainThread]: 
[0m17:30:22.565893 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:30:22.567015 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:30:22.576195 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:30:22.576535 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:30:22.576761 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:30:23.844759 [debug] [ThreadPool]: SQL status: OK in 1.27 seconds
[0m17:30:23.857362 [debug] [ThreadPool]: On list_schemas: Close
[0m17:30:24.398793 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:30:24.410561 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:24.410983 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:30:24.411357 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:30:24.411555 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:30:25.696869 [debug] [ThreadPool]: SQL status: OK in 1.29 seconds
[0m17:30:25.703431 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:30:25.703874 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:30:25.704327 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:30:26.158078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11578ab50>]}
[0m17:30:26.158961 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:26.159392 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:30:26.160233 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:30:26.161091 [info ] [MainThread]: 
[0m17:30:26.167282 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:30:26.167907 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:30:26.168779 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:30:26.169145 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:30:26.169467 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:30:26.174239 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:30:26.175964 [debug] [Thread-1  ]: finished collecting timing info
[0m17:30:26.176465 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:30:26.216657 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:30:26.217305 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:26.217532 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:30:26.217663 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.customers

[0m17:30:26.217788 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:30:27.303476 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.customers

[0m17:30:27.304070 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:30:27.304506 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:30:27.305027 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x0b\xee\x01\x9ar\x98E\xfb\x9bt\t#\x1d\x8e\x91\xb0'
[0m17:30:27.305967 [debug] [Thread-1  ]: finished collecting timing info
[0m17:30:27.306497 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:30:27.306799 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:30:27.307079 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:30:27.781836 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Catalog namespace is not supported.
[0m17:30:27.783185 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115924130>]}
[0m17:30:27.784030 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.61s]
[0m17:30:27.785296 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:30:27.786085 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:30:27.787614 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:30:27.789134 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:30:27.790016 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:30:27.790596 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:30:27.796644 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:30:27.797458 [debug] [Thread-1  ]: finished collecting timing info
[0m17:30:27.797798 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:30:27.804423 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:30:27.805502 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:27.805914 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:30:27.806171 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.orders

[0m17:30:27.806490 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:30:28.991094 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.orders

[0m17:30:28.991675 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:30:28.992052 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:30:28.992413 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"\xe8?\xff\x15'5C6\x96J\x8aY\xd7\x04\xf5\x9a"
[0m17:30:28.993124 [debug] [Thread-1  ]: finished collecting timing info
[0m17:30:28.993547 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:30:28.993893 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:30:28.994219 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:30:29.448846 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Catalog namespace is not supported.
[0m17:30:29.449943 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115a80700>]}
[0m17:30:29.451042 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.66s]
[0m17:30:29.452765 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:30:29.453836 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:30:29.454943 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:30:29.456319 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:30:29.456711 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:30:29.457029 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:30:29.462752 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:30:29.463541 [debug] [Thread-1  ]: finished collecting timing info
[0m17:30:29.463846 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:30:29.469344 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:30:29.470084 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:29.470464 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:30:29.470691 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.payments

[0m17:30:29.470888 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:30:30.543715 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.payments

[0m17:30:30.544037 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:30:30.544239 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:30:30.544380 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xf8\x9ao\x06\xad[AN\x88\x9c\x12\xc0\xae\xce\t\xcb'
[0m17:30:30.544938 [debug] [Thread-1  ]: finished collecting timing info
[0m17:30:30.545222 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:30:30.545510 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:30:30.545684 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:30:30.999604 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:30:31.000584 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '58cfc0d3-28a4-484a-8808-5aa36e06d835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ab51c0>]}
[0m17:30:31.001560 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.54s]
[0m17:30:31.002531 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:30:31.004863 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:30:31.005404 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:30:31.006445 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:30:31.007843 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:30:31.008335 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:30:31.009177 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:30:31.011576 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:30:31.011950 [debug] [MainThread]: On master: ROLLBACK
[0m17:30:31.012193 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:30:31.482926 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:30:31.483819 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:31.484369 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:30:31.484979 [debug] [MainThread]: On master: ROLLBACK
[0m17:30:31.485352 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:30:31.485724 [debug] [MainThread]: On master: Close
[0m17:30:31.947131 [info ] [MainThread]: 
[0m17:30:31.947974 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 9.38 seconds (9.38s).
[0m17:30:31.948908 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:30:31.949357 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:30:31.962560 [info ] [MainThread]: 
[0m17:30:31.962949 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:30:31.963424 [info ] [MainThread]: 
[0m17:30:31.963710 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:30:31.963959 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:30:31.964206 [info ] [MainThread]: 
[0m17:30:31.964443 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:30:31.964709 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:30:31.965164 [info ] [MainThread]: 
[0m17:30:31.965669 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:30:31.966177 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:30:31.966700 [info ] [MainThread]: 
[0m17:30:31.967284 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:30:31.967731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1159ce070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1159ce190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115882e20>]}
[0m17:30:31.968207 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:32:24.931960 | 484242cd-4ece-4560-a2e8-6af74fce6cb3 ==============================
[0m17:32:24.931996 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:32:24.933018 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m17:32:24.933253 [debug] [MainThread]: Tracking: tracking
[0m17:32:24.950761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186a6730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186a6a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186a66a0>]}
[0m17:32:25.000586 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:32:25.000890 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:32:25.008957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '484242cd-4ece-4560-a2e8-6af74fce6cb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11885ffa0>]}
[0m17:32:25.016614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '484242cd-4ece-4560-a2e8-6af74fce6cb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187a0bb0>]}
[0m17:32:25.017012 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:32:25.017256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '484242cd-4ece-4560-a2e8-6af74fce6cb3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187a0f10>]}
[0m17:32:25.018034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187a0ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187a0d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187a09a0>]}
[0m17:32:25.018300 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:33:44.927350 | 49f145cf-facc-4a42-b09e-8a410ea5dd71 ==============================
[0m17:33:44.927390 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:33:44.928236 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:33:44.928599 [debug] [MainThread]: Tracking: tracking
[0m17:33:44.948454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b915e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b917f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b91ee0>]}
[0m17:33:45.014311 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:33:45.014650 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:33:45.023154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115d5cfd0>]}
[0m17:33:45.033451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ca9cd0>]}
[0m17:33:45.034096 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:33:45.034736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ca9f40>]}
[0m17:33:45.037328 [info ] [MainThread]: 
[0m17:33:45.038347 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:33:45.039972 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:33:45.058598 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:33:45.059089 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:33:45.059303 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:33:46.713272 [debug] [ThreadPool]: SQL status: OK in 1.65 seconds
[0m17:33:46.719124 [debug] [ThreadPool]: On list_schemas: Close
[0m17:33:47.275489 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:33:47.281156 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:47.281438 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:33:47.281601 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:33:47.281736 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:33:48.656585 [debug] [ThreadPool]: SQL status: OK in 1.37 seconds
[0m17:33:48.660324 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:33:48.660735 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:33:48.661066 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:33:49.108151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ba1490>]}
[0m17:33:49.108663 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:49.108894 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:33:49.109331 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:33:49.109676 [info ] [MainThread]: 
[0m17:33:49.112988 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:33:49.113361 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:33:49.113925 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:33:49.114137 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:33:49.114309 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:33:49.117225 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:33:49.118188 [debug] [Thread-1  ]: finished collecting timing info
[0m17:33:49.118649 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:33:49.155644 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:33:49.156238 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:49.156405 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:33:49.156539 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.customers

[0m17:33:49.156669 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:33:50.249343 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.customers

[0m17:33:50.249658 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:33:50.249816 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:33:50.249954 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'H\xdc\x99\xd0\xcdrHw\x90\xba\xd8C@\xf7\x03\xa1'
[0m17:33:50.250210 [debug] [Thread-1  ]: finished collecting timing info
[0m17:33:50.250369 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:33:50.250500 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:33:50.250626 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:33:50.742716 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Catalog namespace is not supported.
[0m17:33:50.743290 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115d72be0>]}
[0m17:33:50.743811 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.63s]
[0m17:33:50.744268 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:33:50.744507 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:33:50.744948 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:33:50.745457 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:33:50.745620 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:33:50.745761 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:33:50.748617 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:33:50.749660 [debug] [Thread-1  ]: finished collecting timing info
[0m17:33:50.749886 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:33:50.753180 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:33:50.753671 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:50.753837 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:33:50.753969 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.orders

[0m17:33:50.754093 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:33:51.833543 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.orders

[0m17:33:51.833827 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:33:51.833988 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:33:51.834118 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'2t;\t\xc5\tN\xcd\x92\x11\xed\x1b&\x1a\x1a\x11'
[0m17:33:51.834368 [debug] [Thread-1  ]: finished collecting timing info
[0m17:33:51.834528 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:33:51.834658 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:33:51.834791 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:33:52.288685 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Catalog namespace is not supported.
[0m17:33:52.289070 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115e9b7f0>]}
[0m17:33:52.289475 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.54s]
[0m17:33:52.289951 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:33:52.290199 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:33:52.290678 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:33:52.291268 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:33:52.291480 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:33:52.291652 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:33:52.294281 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:33:52.295480 [debug] [Thread-1  ]: finished collecting timing info
[0m17:33:52.295707 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:33:52.299092 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:33:52.299563 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:52.299717 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:33:52.299845 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.payments

[0m17:33:52.299980 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:33:53.359717 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.payments

[0m17:33:53.360026 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:33:53.360264 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:33:53.360447 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\\\xed\xeft\xd3qF\x0c\x883OHe<\xc0b'
[0m17:33:53.360706 [debug] [Thread-1  ]: finished collecting timing info
[0m17:33:53.360869 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:33:53.361000 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:33:53.361122 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:33:53.808452 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:33:53.808857 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49f145cf-facc-4a42-b09e-8a410ea5dd71', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ed02e0>]}
[0m17:33:53.809322 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.52s]
[0m17:33:53.810108 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:33:53.810997 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:33:53.811580 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:33:53.812137 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:33:53.812663 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:33:53.813062 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:33:53.813515 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:33:53.814603 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:33:53.814880 [debug] [MainThread]: On master: ROLLBACK
[0m17:33:53.815032 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:33:54.281501 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:54.281843 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:54.282055 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:33:54.282224 [debug] [MainThread]: On master: ROLLBACK
[0m17:33:54.282405 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:54.282579 [debug] [MainThread]: On master: Close
[0m17:33:54.731275 [info ] [MainThread]: 
[0m17:33:54.731824 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 9.69 seconds (9.69s).
[0m17:33:54.732137 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:33:54.732362 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:33:54.740484 [info ] [MainThread]: 
[0m17:33:54.741038 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:33:54.741457 [info ] [MainThread]: 
[0m17:33:54.741694 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:33:54.741903 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:33:54.742108 [info ] [MainThread]: 
[0m17:33:54.742309 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:33:54.742515 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:33:54.742719 [info ] [MainThread]: 
[0m17:33:54.742916 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:33:54.743109 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:33:54.743321 [info ] [MainThread]: 
[0m17:33:54.743524 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:33:54.743827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115de9070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115de9190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ca9f10>]}
[0m17:33:54.744069 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:34:25.061800 | 96a8b6f7-7a44-443f-bc4d-abb12aac12c6 ==============================
[0m17:34:25.061836 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:34:25.063044 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:34:25.063504 [debug] [MainThread]: Tracking: tracking
[0m17:34:25.081000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2880d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d288430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d288640>]}
[0m17:34:25.136438 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:34:25.137336 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.payments
[0m17:34:25.137610 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:34:25.137961 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.orders
[0m17:34:25.138165 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.customers
[0m17:34:25.138307 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:34:25.154499 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:34:25.169226 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:34:25.172043 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:34:25.174493 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:34:25.177252 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:34:25.208384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d42e0d0>]}
[0m17:34:25.216101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d389b80>]}
[0m17:34:25.216536 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:34:25.216868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d389ee0>]}
[0m17:34:25.218268 [info ] [MainThread]: 
[0m17:34:25.218915 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:34:25.219920 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:34:25.228718 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:34:25.228988 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:34:25.229137 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:34:26.456926 [debug] [ThreadPool]: SQL status: OK in 1.23 seconds
[0m17:34:26.467560 [debug] [ThreadPool]: On list_schemas: Close
[0m17:34:27.166460 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:34:27.180812 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:27.181215 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:34:27.181491 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:34:27.181774 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:34:28.552411 [debug] [ThreadPool]: SQL status: OK in 1.37 seconds
[0m17:34:28.559268 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:34:28.559710 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:34:28.560028 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:34:29.407612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d283370>]}
[0m17:34:29.408674 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:29.409018 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:34:29.409627 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:34:29.410121 [info ] [MainThread]: 
[0m17:34:29.415973 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:34:29.416528 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:34:29.417265 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:34:29.417517 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:34:29.417747 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:34:29.421375 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:34:29.421998 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:29.422254 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:34:29.456792 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:34:29.457448 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:29.457653 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:34:29.457803 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

[0m17:34:29.457963 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:34:31.152478 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

[0m17:34:31.153092 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:34:31.153525 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.customers; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.customers; line 11 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:34:31.153896 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'[\xf6c\x06\x08\xbfC\xf6\xb7\x82rl"\x81\xd5\xd3'
[0m17:34:31.154549 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:31.154995 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:34:31.155357 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:34:31.155695 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:34:32.072392 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:34:32.073283 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d595ac0>]}
[0m17:34:32.074147 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 2.66s]
[0m17:34:32.075136 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:34:32.075687 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:34:32.076511 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:34:32.078159 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:34:32.078516 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:34:32.078822 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:34:32.084130 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:34:32.084950 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:32.085279 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:34:32.091428 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:34:32.092835 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:32.093171 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:34:32.093597 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.orders

[0m17:34:32.093851 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:34:33.149729 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.orders

[0m17:34:33.150329 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:34:33.150741 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.orders; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.orders; line 12 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:34:33.151119 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xdc\x00u!\x00\x84BH\x99\xe9\xa5)w~i{'
[0m17:34:33.151724 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:33.152128 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:34:33.152486 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:34:33.153172 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:34:33.602144 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:34:33.603017 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d5f61f0>]}
[0m17:34:33.604094 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.52s]
[0m17:34:33.605259 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:34:33.605934 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:34:33.607096 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:34:33.608406 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:34:33.608822 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:34:33.609137 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:34:33.613812 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:34:33.614882 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:33.615277 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:34:33.620671 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:34:33.621374 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:33.621609 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:34:33.621808 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.payments

[0m17:34:33.622004 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:34:34.583447 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.payments

[0m17:34:34.584367 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: stripe.payments; line 13 pos 5
[0m17:34:34.585040 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: stripe.payments; line 13 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: stripe.payments; line 13 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:34:34.585597 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"'\xcb%'Z}H{\xa0\xd5\xf3\xa4\x8a\x9b\xc0\xcf"
[0m17:34:34.586238 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:34.586669 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:34:34.587031 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:34:34.587371 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:34:35.041155 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Table or view not found: stripe.payments; line 13 pos 5
[0m17:34:35.041978 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96a8b6f7-7a44-443f-bc4d-abb12aac12c6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6232b0>]}
[0m17:34:35.043012 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.43s]
[0m17:34:35.044390 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:34:35.046910 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:34:35.047382 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:34:35.048346 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:34:35.049660 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:34:35.050371 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:34:35.051324 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:34:35.053523 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:34:35.053928 [debug] [MainThread]: On master: ROLLBACK
[0m17:34:35.054207 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:34:35.530499 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:34:35.531406 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:35.531959 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:34:35.532532 [debug] [MainThread]: On master: ROLLBACK
[0m17:34:35.533016 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:34:35.533362 [debug] [MainThread]: On master: Close
[0m17:34:35.983198 [info ] [MainThread]: 
[0m17:34:35.984434 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 10.76 seconds (10.76s).
[0m17:34:35.985328 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:34:35.986157 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:34:36.005865 [info ] [MainThread]: 
[0m17:34:36.006397 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:34:36.007065 [info ] [MainThread]: 
[0m17:34:36.007604 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:34:36.008179 [error] [MainThread]:   Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:34:36.008586 [info ] [MainThread]: 
[0m17:34:36.008897 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:34:36.009188 [error] [MainThread]:   Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:34:36.009462 [info ] [MainThread]: 
[0m17:34:36.009967 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:34:36.010412 [error] [MainThread]:   Table or view not found: stripe.payments; line 13 pos 5
[0m17:34:36.010719 [info ] [MainThread]: 
[0m17:34:36.011021 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:34:36.011419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d489610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d489790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d61ae80>]}
[0m17:34:36.011754 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:34:53.585464 | 751697a6-5de8-4b75-ba9f-10f3433770e7 ==============================
[0m17:34:53.585539 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:34:53.586914 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:34:53.587181 [debug] [MainThread]: Tracking: tracking
[0m17:34:53.604819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b630d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b63430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b63640>]}
[0m17:34:53.659481 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:34:53.660083 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:34:53.676159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117d490d0>]}
[0m17:34:53.683840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c5eb80>]}
[0m17:34:53.684272 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:34:53.684616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c5eee0>]}
[0m17:34:53.686159 [info ] [MainThread]: 
[0m17:34:53.686727 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:34:53.687856 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:34:53.700152 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:34:53.700412 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:34:53.700590 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:34:54.716938 [debug] [ThreadPool]: SQL status: OK in 1.02 seconds
[0m17:34:54.728083 [debug] [ThreadPool]: On list_schemas: Close
[0m17:34:55.183741 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:34:55.199129 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:55.199624 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:34:55.200033 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:34:55.200302 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:34:56.447878 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m17:34:56.455039 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:34:56.455501 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:34:56.455810 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:34:56.903500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b5df10>]}
[0m17:34:56.904967 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:56.905631 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:34:56.906512 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:34:56.907108 [info ] [MainThread]: 
[0m17:34:56.913076 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:34:56.913682 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:34:56.914515 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:34:56.914806 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:34:56.915085 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:34:56.919214 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:34:56.919971 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:56.920258 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:34:56.956248 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:34:56.956974 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:56.957287 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:34:56.957468 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

[0m17:34:56.957620 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:34:57.906280 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

[0m17:34:57.906981 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:34:57.907475 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.customers; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.customers; line 11 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:34:57.907775 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xde\xf3\xd8z>\x9aE\xb6\xbb=Z\x94\xaf\na\x9d'
[0m17:34:57.908436 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:57.908872 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:34:57.909179 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:34:57.909404 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:34:58.398022 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:34:58.399133 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e87220>]}
[0m17:34:58.400501 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.48s]
[0m17:34:58.401748 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:34:58.402609 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:34:58.404178 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:34:58.405588 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:34:58.406053 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:34:58.406339 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:34:58.410679 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:34:58.411380 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:58.411652 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:34:58.420975 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:34:58.421681 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:58.421886 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:34:58.422045 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.orders

[0m17:34:58.422198 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:34:59.415630 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.orders

[0m17:34:59.416260 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:34:59.416959 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.orders; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.orders; line 12 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:34:59.417307 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'8Z<\x0e\x91\x06H.\x91\xb7N\x00\x82\xf0\x15 '
[0m17:34:59.417815 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:59.418150 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:34:59.418430 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:34:59.418691 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:34:59.870385 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:34:59.871521 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e8c7c0>]}
[0m17:34:59.872385 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.47s]
[0m17:34:59.873276 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:34:59.873764 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:34:59.874805 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:34:59.876045 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:34:59.876446 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:34:59.876751 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:34:59.881279 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:34:59.883487 [debug] [Thread-1  ]: finished collecting timing info
[0m17:34:59.883892 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:34:59.889274 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:34:59.890011 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:59.890261 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:34:59.890467 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.payments

[0m17:34:59.890662 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:00.833733 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.payments

[0m17:35:00.834619 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: stripe.payments; line 13 pos 5
[0m17:35:00.835144 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: stripe.payments; line 13 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: stripe.payments; line 13 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:35:00.835549 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x01\xc1bx\xe8\xc5A\xec\x89\x15\x15$zu{\xf9'
[0m17:35:00.836476 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:00.836948 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:35:00.837313 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:00.837642 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:35:01.289334 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Table or view not found: stripe.payments; line 13 pos 5
[0m17:35:01.290607 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '751697a6-5de8-4b75-ba9f-10f3433770e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117ea6c10>]}
[0m17:35:01.291580 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.42s]
[0m17:35:01.292804 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:35:01.295104 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:35:01.296223 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:35:01.297376 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:35:01.298941 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:35:01.299504 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:35:01.300591 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:35:01.303004 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:35:01.303450 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:01.303719 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:35:01.767099 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:01.767925 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:01.768310 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:01.768683 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:01.769018 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:01.769337 [debug] [MainThread]: On master: Close
[0m17:35:02.219777 [info ] [MainThread]: 
[0m17:35:02.220322 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 8.53 seconds (8.53s).
[0m17:35:02.220769 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:35:02.221002 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:35:02.229929 [info ] [MainThread]: 
[0m17:35:02.230300 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:35:02.230819 [info ] [MainThread]: 
[0m17:35:02.231198 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:35:02.231537 [error] [MainThread]:   Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:35:02.231909 [info ] [MainThread]: 
[0m17:35:02.232294 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:35:02.232578 [error] [MainThread]:   Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:35:02.232884 [info ] [MainThread]: 
[0m17:35:02.233303 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:35:02.233805 [error] [MainThread]:   Table or view not found: stripe.payments; line 13 pos 5
[0m17:35:02.234272 [info ] [MainThread]: 
[0m17:35:02.234789 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:35:02.235390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c5eaf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c5eb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117ea6460>]}
[0m17:35:02.235930 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:35:21.684799 | 2bd9ebb1-7626-467f-92ff-6257b978c7f4 ==============================
[0m17:35:21.684838 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:35:21.686623 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:35:21.687113 [debug] [MainThread]: Tracking: tracking
[0m17:35:21.709117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118469100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11846e340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11846e4c0>]}
[0m17:35:21.764892 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:35:21.765657 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.orders
[0m17:35:21.765881 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.customers
[0m17:35:21.766032 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:35:21.766307 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.payments
[0m17:35:21.766446 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:35:21.779657 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:35:21.795407 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:35:21.810416 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:35:21.814381 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:35:21.817380 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:35:21.853868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186270d0>]}
[0m17:35:21.863956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11846e1c0>]}
[0m17:35:21.864557 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:35:21.864891 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11846e220>]}
[0m17:35:21.866528 [info ] [MainThread]: 
[0m17:35:21.867110 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:35:21.868374 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:35:21.880036 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:35:21.880329 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:35:21.880488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:35:22.933943 [debug] [ThreadPool]: SQL status: OK in 1.05 seconds
[0m17:35:22.946865 [debug] [ThreadPool]: On list_schemas: Close
[0m17:35:23.421484 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:35:23.433115 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:23.433452 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:35:23.433668 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:35:23.433845 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:35:24.557015 [debug] [ThreadPool]: SQL status: OK in 1.12 seconds
[0m17:35:24.562864 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:35:24.563307 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:35:24.563605 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:35:25.054334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185f9ee0>]}
[0m17:35:25.055399 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:25.055860 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:25.056741 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:35:25.057477 [info ] [MainThread]: 
[0m17:35:25.063856 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:35:25.064584 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:35:25.065829 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:35:25.066205 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:35:25.066546 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:35:25.070970 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:35:25.071729 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:25.072045 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:35:25.110795 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:35:25.111367 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:25.111598 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:35:25.111727 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.customers

[0m17:35:25.111850 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:26.210216 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.customers

[0m17:35:26.210723 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: default.customers; line 11 pos 5
[0m17:35:26.210980 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: default.customers; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: default.customers; line 11 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:35:26.211209 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xbc\xe7\x1f-\xd5:G\x99\xb3\x9ePd\xaag\xcc '
[0m17:35:26.211644 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:26.211914 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:35:26.212137 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:26.212352 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:35:26.690498 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Table or view not found: default.customers; line 11 pos 5
[0m17:35:26.691392 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118789f70>]}
[0m17:35:26.692554 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.63s]
[0m17:35:26.693853 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:35:26.695205 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:35:26.696240 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:35:26.697785 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:35:26.698158 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:35:26.698425 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:35:26.702760 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:35:26.703658 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:26.703972 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:35:26.709979 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:35:26.710710 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:26.710942 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:35:26.711130 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.orders

[0m17:35:26.711303 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:27.686301 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.orders

[0m17:35:27.687280 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: default.orders; line 12 pos 5
[0m17:35:27.688058 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: default.orders; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: default.orders; line 12 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:35:27.688695 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'P;S\x9a\x962K\x94\xb8B\xc0\x00=%\xc5\xbd'
[0m17:35:27.689985 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:27.690564 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:35:27.691101 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:27.691542 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:35:28.141728 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Table or view not found: default.orders; line 12 pos 5
[0m17:35:28.142627 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187eb190>]}
[0m17:35:28.143577 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.45s]
[0m17:35:28.144588 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:35:28.145732 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:35:28.147054 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:35:28.148566 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:35:28.149036 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:35:28.149397 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:35:28.155970 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:35:28.156805 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:28.157201 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:35:28.163148 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:35:28.163923 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:28.164186 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:35:28.164410 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.payments

[0m17:35:28.164624 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:29.107674 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.payments

[0m17:35:29.108136 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: default.payments; line 13 pos 5
[0m17:35:29.108452 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: default.payments; line 13 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: default.payments; line 13 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:35:29.108701 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"$\xa6\xe5\xca\x97VJ\x87\x9f\xfbt\x7f\xcc'G*"
[0m17:35:29.108956 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:29.109166 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:35:29.109312 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:29.109459 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:35:29.552373 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Table or view not found: default.payments; line 13 pos 5
[0m17:35:29.553087 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bd9ebb1-7626-467f-92ff-6257b978c7f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118817250>]}
[0m17:35:29.553652 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.41s]
[0m17:35:29.554255 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:35:29.555442 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:35:29.555800 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:35:29.556592 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:35:29.557666 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:35:29.558133 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:35:29.558885 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:35:29.560134 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:35:29.560411 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:29.560587 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:35:30.022124 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:30.023033 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:30.023568 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:30.023946 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:30.024284 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:30.024604 [debug] [MainThread]: On master: Close
[0m17:35:30.466510 [info ] [MainThread]: 
[0m17:35:30.467108 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 8.60 seconds (8.60s).
[0m17:35:30.467803 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:35:30.468268 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:35:30.485010 [info ] [MainThread]: 
[0m17:35:30.485591 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:35:30.485996 [info ] [MainThread]: 
[0m17:35:30.486369 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:35:30.486734 [error] [MainThread]:   Table or view not found: default.customers; line 11 pos 5
[0m17:35:30.487081 [info ] [MainThread]: 
[0m17:35:30.487513 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:35:30.487809 [error] [MainThread]:   Table or view not found: default.orders; line 12 pos 5
[0m17:35:30.488342 [info ] [MainThread]: 
[0m17:35:30.488646 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:35:30.489002 [error] [MainThread]:   Table or view not found: default.payments; line 13 pos 5
[0m17:35:30.489233 [info ] [MainThread]: 
[0m17:35:30.489502 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:35:30.489810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11869e400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11869e3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118809cd0>]}
[0m17:35:30.490107 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:35:39.613431 | 9151d247-ae9d-4c34-b18d-fd1b3c265f85 ==============================
[0m17:35:39.613465 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:35:39.615564 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:35:39.616399 [debug] [MainThread]: Tracking: tracking
[0m17:35:39.637675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7a1280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7a1a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7a16a0>]}
[0m17:35:39.717688 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:35:39.718427 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.payments
[0m17:35:39.718632 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:35:39.718922 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.orders
[0m17:35:39.719113 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.customers
[0m17:35:39.719247 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:35:39.739433 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:35:39.765274 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:35:39.769497 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:35:39.772660 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:35:39.776422 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:35:39.821869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e94d0d0>]}
[0m17:35:39.830263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e8a5ca0>]}
[0m17:35:39.830640 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:35:39.830928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e8a5f10>]}
[0m17:35:39.832363 [info ] [MainThread]: 
[0m17:35:39.832855 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:35:39.834011 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:35:39.850452 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:35:39.850881 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:35:39.851251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:35:40.865115 [debug] [ThreadPool]: SQL status: OK in 1.01 seconds
[0m17:35:40.875873 [debug] [ThreadPool]: On list_schemas: Close
[0m17:35:41.327210 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:35:41.344156 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:41.344533 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:35:41.344906 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:35:41.345131 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:35:42.570726 [debug] [ThreadPool]: SQL status: OK in 1.23 seconds
[0m17:35:42.575765 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:35:42.576107 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:35:42.576342 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:35:43.052127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e921ee0>]}
[0m17:35:43.053210 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:43.053751 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:43.054772 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:35:43.055770 [info ] [MainThread]: 
[0m17:35:43.062444 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:35:43.063122 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:35:43.064091 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:35:43.064384 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:35:43.064661 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:35:43.069078 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:35:43.069949 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:43.070227 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:35:43.107280 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:35:43.108108 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:43.108378 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:35:43.108514 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from stripe.default.customers

[0m17:35:43.108770 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:44.189598 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from stripe.default.customers

[0m17:35:44.190426 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:35:44.190997 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:35:44.191453 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xac\xb3mU4\x00K\x0f\x9e\xc8\x9b\xfe9\x0b6\xfc'
[0m17:35:44.192091 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:44.192499 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:35:44.192848 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:44.193184 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:35:44.644029 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Catalog namespace is not supported.
[0m17:35:44.644932 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eaa6be0>]}
[0m17:35:44.645905 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.58s]
[0m17:35:44.647018 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:35:44.647807 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:35:44.648947 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:35:44.650210 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:35:44.650580 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:35:44.650901 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:35:44.656293 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:35:44.657184 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:44.657556 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:35:44.663765 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:35:44.664537 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:44.664792 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:35:44.665005 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from stripe.default.orders

[0m17:35:44.665210 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:45.732966 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from stripe.default.orders

[0m17:35:45.733302 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:35:45.733518 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:35:45.733682 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xad\x8f\xe3\xd2B\xfeF\xea\xbd\xd9\xb0\xb6}5\x00\xe3'
[0m17:35:45.734005 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:45.734197 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:35:45.734335 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:45.734463 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:35:46.179540 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Catalog namespace is not supported.
[0m17:35:46.180368 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb10220>]}
[0m17:35:46.181160 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.53s]
[0m17:35:46.182066 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:35:46.182594 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:35:46.183625 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:35:46.185440 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:35:46.185916 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:35:46.186252 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:35:46.192271 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:35:46.193273 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:46.193718 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:35:46.199670 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:35:46.200947 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:46.201365 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:35:46.201605 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.default.payments

[0m17:35:46.201806 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:35:47.617608 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.default.payments

[0m17:35:47.618504 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:35:47.619077 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:35:47.619621 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xf1\x9f\xf6[\xc8\xbaK\xef\xa9\x82\xe3v\xc7\t\xf6\xc8'
[0m17:35:47.620542 [debug] [Thread-1  ]: finished collecting timing info
[0m17:35:47.620943 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:35:47.621287 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:35:47.621612 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:35:48.185794 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:35:48.187023 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9151d247-ae9d-4c34-b18d-fd1b3c265f85', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb40130>]}
[0m17:35:48.187914 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 2.00s]
[0m17:35:48.189600 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:35:48.192030 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:35:48.192641 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:35:48.193735 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:35:48.194707 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:35:48.195803 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:35:48.196820 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:35:48.199261 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:35:48.199727 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:48.200097 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:35:48.712566 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:48.713200 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:48.713582 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:48.713953 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:48.714291 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:48.714622 [debug] [MainThread]: On master: Close
[0m17:35:49.166891 [info ] [MainThread]: 
[0m17:35:49.167772 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 9.33 seconds (9.33s).
[0m17:35:49.168426 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:35:49.168766 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:35:49.184613 [info ] [MainThread]: 
[0m17:35:49.185104 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:35:49.185535 [info ] [MainThread]: 
[0m17:35:49.185898 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:35:49.186280 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:35:49.186964 [info ] [MainThread]: 
[0m17:35:49.187413 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:35:49.187763 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:35:49.188109 [info ] [MainThread]: 
[0m17:35:49.188566 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:35:49.188886 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:35:49.189426 [info ] [MainThread]: 
[0m17:35:49.190259 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:35:49.191078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7915b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e939880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb0fc70>]}
[0m17:35:49.191705 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:36:07.752364 | 8ff5d339-1f45-4424-b873-5cd7249680ee ==============================
[0m17:36:07.752445 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:36:07.754246 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:36:07.754711 [debug] [MainThread]: Tracking: tracking
[0m17:36:07.771781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207b5b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207ba1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207ba1c0>]}
[0m17:36:07.831384 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:36:07.832129 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.orders
[0m17:36:07.832383 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.customers
[0m17:36:07.832541 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:36:07.852851 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:36:07.870929 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:36:07.874989 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:36:07.878730 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:36:07.913351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1209c2f40>]}
[0m17:36:07.921143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208c8b50>]}
[0m17:36:07.921502 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:36:07.921806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208c8eb0>]}
[0m17:36:07.923227 [info ] [MainThread]: 
[0m17:36:07.923777 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:36:07.925113 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:36:07.934706 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:36:07.934971 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:36:07.935150 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:36:08.917984 [debug] [ThreadPool]: SQL status: OK in 0.98 seconds
[0m17:36:08.924640 [debug] [ThreadPool]: On list_schemas: Close
[0m17:36:09.484475 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:36:09.492542 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:36:09.492826 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:36:09.493153 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:36:09.493412 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:36:10.594505 [debug] [ThreadPool]: SQL status: OK in 1.1 seconds
[0m17:36:10.598752 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:36:10.599041 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:36:10.599261 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:36:11.056807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208c8b20>]}
[0m17:36:11.057321 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:36:11.057772 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:36:11.058569 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:36:11.059121 [info ] [MainThread]: 
[0m17:36:11.067323 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:36:11.067713 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:36:11.068254 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:36:11.068447 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:36:11.068665 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:36:11.071339 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:36:11.072325 [debug] [Thread-1  ]: finished collecting timing info
[0m17:36:11.072589 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:36:11.106728 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:36:11.107455 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:36:11.107657 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:36:11.107792 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

[0m17:36:11.107920 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:36:12.036738 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.customers

[0m17:36:12.037096 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:36:12.037363 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.customers; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.customers; line 11 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:36:12.037574 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xab{\x9fk\x96wA\x96\xa8\xce\xe5Q\xcb\xcc\xf6/'
[0m17:36:12.037852 [debug] [Thread-1  ]: finished collecting timing info
[0m17:36:12.038041 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:36:12.038168 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:36:12.038347 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:36:12.493713 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:36:12.495118 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120afd220>]}
[0m17:36:12.496137 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.43s]
[0m17:36:12.497221 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:36:12.497966 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:36:12.499205 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:36:12.500692 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:36:12.501201 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:36:12.501610 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:36:12.507504 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:36:12.508480 [debug] [Thread-1  ]: finished collecting timing info
[0m17:36:12.508969 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:36:12.514874 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:36:12.515858 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:36:12.516212 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:36:12.516613 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.orders

[0m17:36:12.516883 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:36:13.480239 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.orders

[0m17:36:13.480827 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:36:13.481213 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.orders; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.orders; line 12 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:36:13.481554 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'B\xee\x94\xaf\xd9\x99O\x10\xbb$u\xf6q9\x1dD'
[0m17:36:13.482137 [debug] [Thread-1  ]: finished collecting timing info
[0m17:36:13.482534 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:36:13.482863 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:36:13.483180 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:36:13.946197 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:36:13.947334 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b24340>]}
[0m17:36:13.948206 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.45s]
[0m17:36:13.949636 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:36:13.950487 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:36:13.951569 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:36:13.953318 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:36:13.953784 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:36:13.954288 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:36:13.960172 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:36:13.960915 [debug] [Thread-1  ]: finished collecting timing info
[0m17:36:13.961237 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:36:13.966807 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:36:13.967504 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:36:13.967735 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:36:13.967933 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.default.payments

[0m17:36:13.968134 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:36:15.038753 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.default.payments

[0m17:36:15.039575 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:36:15.040106 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:36:15.040602 [debug] [Thread-1  ]: Databricks adapter: operation-id: b"2\x83\x92'\tVLb\xb08\xca\xc9\xf5F\xd0\xef"
[0m17:36:15.041256 [debug] [Thread-1  ]: finished collecting timing info
[0m17:36:15.041693 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:36:15.042032 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:36:15.042352 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:36:15.494266 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:36:15.495261 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ff5d339-1f45-4424-b873-5cd7249680ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b55100>]}
[0m17:36:15.496352 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.54s]
[0m17:36:15.497338 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:36:15.499042 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:36:15.500120 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:36:15.501033 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:36:15.502554 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:36:15.503212 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:36:15.504126 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:36:15.506510 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:36:15.506952 [debug] [MainThread]: On master: ROLLBACK
[0m17:36:15.507264 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:36:15.972922 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:36:15.973709 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:36:15.974118 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:36:15.974511 [debug] [MainThread]: On master: ROLLBACK
[0m17:36:15.974859 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:36:15.975178 [debug] [MainThread]: On master: Close
[0m17:36:16.456908 [info ] [MainThread]: 
[0m17:36:16.457820 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 8.53 seconds (8.53s).
[0m17:36:16.458928 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:36:16.459389 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:36:16.474138 [info ] [MainThread]: 
[0m17:36:16.474603 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:36:16.475010 [info ] [MainThread]: 
[0m17:36:16.475350 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:36:16.475701 [error] [MainThread]:   Table or view not found: jaffle_shop.customers; line 11 pos 5
[0m17:36:16.476231 [info ] [MainThread]: 
[0m17:36:16.476591 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:36:16.476923 [error] [MainThread]:   Table or view not found: jaffle_shop.orders; line 12 pos 5
[0m17:36:16.477234 [info ] [MainThread]: 
[0m17:36:16.477539 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:36:16.477858 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:36:16.478331 [info ] [MainThread]: 
[0m17:36:16.478764 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:36:16.479413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b33130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b33160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b33640>]}
[0m17:36:16.479753 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:36:43.088866 | fdd4519e-0845-44ea-8cae-d060336f4653 ==============================
[0m17:36:43.089012 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:36:43.090197 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:36:43.090569 [debug] [MainThread]: Tracking: tracking
[0m17:36:43.111261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224b15e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224b17f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224b1ee0>]}
[0m17:36:43.164601 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:36:43.165426 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.orders
[0m17:36:43.165728 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.customers
[0m17:36:43.165879 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:36:43.166151 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.payments
[0m17:36:43.166290 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:36:43.179738 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:36:43.196069 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:36:43.202065 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:36:43.207203 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:36:43.211345 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:36:43.234097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12265a340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1226c9c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1226c9bb0>]}
[0m17:36:43.234505 [debug] [MainThread]: Flushing usage events
[0m17:36:43.829752 [error] [MainThread]: Encountered an error:
Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'jaffle_shop.orders' which was not found
[0m17:36:43.838864 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 390, in load
    self.process_sources(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 921, in process_sources
    _process_sources_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1302, in _process_sources_for_node
    invalid_source_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 955, in invalid_source_fail_unless_test
    source_target_not_found(node, target_name, target_table_name, disabled=disabled)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 656, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Model 'model.jaffle_shop.stg_orders' (models/staging/jaffle_shop/stg_orders.sql) depends on a source named 'jaffle_shop.orders' which was not found



============================== 2022-10-13 17:37:48.172307 | 848fdb29-9f06-4589-a8ba-72c189ca9831 ==============================
[0m17:37:48.172413 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:37:48.174550 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:37:48.175196 [debug] [MainThread]: Tracking: tracking
[0m17:37:48.195382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c55460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c55bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c559d0>]}
[0m17:37:48.263732 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 5 files changed.
[0m17:37:48.264749 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.orders
[0m17:37:48.265076 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.customers
[0m17:37:48.265270 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:37:48.265593 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.payments
[0m17:37:48.265805 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:37:48.265994 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/stripe/stg_payments.sql
[0m17:37:48.266184 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_customers.sql
[0m17:37:48.266368 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/staging/jaffle_shop/stg_orders.sql
[0m17:37:48.283438 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:37:48.296810 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:37:48.300613 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:37:48.305235 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:37:48.309548 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:37:48.343425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e3a0d0>]}
[0m17:37:48.373881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121d76fa0>]}
[0m17:37:48.374353 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:37:48.374744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c5b1c0>]}
[0m17:37:48.376431 [info ] [MainThread]: 
[0m17:37:48.377061 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:37:48.378168 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:37:48.387669 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:37:48.388059 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:37:48.388256 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:37:53.677007 [debug] [ThreadPool]: SQL status: OK in 5.29 seconds
[0m17:37:53.688102 [debug] [ThreadPool]: On list_schemas: Close
[0m17:37:54.263555 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:37:54.280290 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:37:54.280610 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:37:54.280821 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:37:54.280998 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:37:55.495230 [debug] [ThreadPool]: SQL status: OK in 1.21 seconds
[0m17:37:55.504288 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:37:55.505022 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:37:55.505628 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:37:55.960727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c68610>]}
[0m17:37:55.961656 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:37:55.962090 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:37:55.963037 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:37:55.963680 [info ] [MainThread]: 
[0m17:37:55.973114 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:37:55.973900 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:37:55.974773 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:37:55.975061 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:37:55.975310 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:37:55.979242 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:37:55.980056 [debug] [Thread-1  ]: finished collecting timing info
[0m17:37:55.980440 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:37:56.016107 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:37:56.016740 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:37:56.016977 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:37:56.017135 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.jaffle_shop_customers

[0m17:37:56.017264 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:37:56.949591 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from jaffle_shop.jaffle_shop_customers

[0m17:37:56.950484 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.jaffle_shop_customers; line 11 pos 5
[0m17:37:56.951007 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.jaffle_shop_customers; line 11 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.jaffle_shop_customers; line 11 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:37:56.951367 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x11m\xda\x1f\xc8\\J@\xac\xc8\xdc(\x14\xa4\xe7L'
[0m17:37:56.952010 [debug] [Thread-1  ]: finished collecting timing info
[0m17:37:56.952429 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:37:56.952787 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:37:56.953119 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:37:57.405006 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Table or view not found: jaffle_shop.jaffle_shop_customers; line 11 pos 5
[0m17:37:57.406362 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121f7a220>]}
[0m17:37:57.407613 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.43s]
[0m17:37:57.408483 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:37:57.408899 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:37:57.409710 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:37:57.410850 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:37:57.411320 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:37:57.411646 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:37:57.418207 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:37:57.419159 [debug] [Thread-1  ]: finished collecting timing info
[0m17:37:57.419745 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:37:57.425830 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:37:57.426544 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:37:57.426777 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:37:57.426957 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.jaffle_shop_orders

[0m17:37:57.427127 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:37:58.459523 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from jaffle_shop.jaffle_shop_orders

[0m17:37:58.460227 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: jaffle_shop.jaffle_shop_orders; line 12 pos 5
[0m17:37:58.460789 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.jaffle_shop_orders; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: jaffle_shop.jaffle_shop_orders; line 12 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m17:37:58.461121 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x17c_t\x86\x19Cm\xad\xdb\xb1\xe3\xf5K\x00\x0c'
[0m17:37:58.461625 [debug] [Thread-1  ]: finished collecting timing info
[0m17:37:58.461960 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:37:58.462232 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:37:58.462488 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:37:58.923622 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Table or view not found: jaffle_shop.jaffle_shop_orders; line 12 pos 5
[0m17:37:58.924286 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121fd83d0>]}
[0m17:37:58.924897 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.51s]
[0m17:37:58.925532 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:37:58.925950 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:37:58.926765 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:37:58.927634 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:37:58.927973 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:37:58.928228 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:37:58.931987 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:37:58.932670 [debug] [Thread-1  ]: finished collecting timing info
[0m17:37:58.933085 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:37:58.938107 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:37:58.938802 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:37:58.939038 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:37:58.939221 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.default.stripe_payments

[0m17:37:58.939392 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:38:00.000222 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from stripe.default.stripe_payments

[0m17:38:00.001140 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:38:00.001603 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:38:00.001990 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x8c\x86\x15}\xfb\xcfH\x04\x9e\x98\xcd>[\xb0\xf08'
[0m17:38:00.002813 [debug] [Thread-1  ]: finished collecting timing info
[0m17:38:00.003614 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:38:00.004112 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:38:00.004489 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:38:00.468972 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:38:00.470166 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '848fdb29-9f06-4589-a8ba-72c189ca9831', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122003460>]}
[0m17:38:00.471117 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.54s]
[0m17:38:00.472088 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:38:00.474271 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:38:00.475015 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:38:00.475830 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:38:00.476794 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:38:00.477263 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:38:00.478037 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:38:00.480007 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:38:00.480380 [debug] [MainThread]: On master: ROLLBACK
[0m17:38:00.480627 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:38:00.942620 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:38:00.943536 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:38:00.944092 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:38:00.944651 [debug] [MainThread]: On master: ROLLBACK
[0m17:38:00.945138 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:38:00.945472 [debug] [MainThread]: On master: Close
[0m17:38:01.395399 [info ] [MainThread]: 
[0m17:38:01.396598 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 13.02 seconds (13.02s).
[0m17:38:01.397541 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:38:01.397904 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:38:01.420062 [info ] [MainThread]: 
[0m17:38:01.420994 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:38:01.421693 [info ] [MainThread]: 
[0m17:38:01.422035 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:38:01.422349 [error] [MainThread]:   Table or view not found: jaffle_shop.jaffle_shop_customers; line 11 pos 5
[0m17:38:01.422829 [info ] [MainThread]: 
[0m17:38:01.423165 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:38:01.423848 [error] [MainThread]:   Table or view not found: jaffle_shop.jaffle_shop_orders; line 12 pos 5
[0m17:38:01.424236 [info ] [MainThread]: 
[0m17:38:01.424542 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:38:01.424938 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:38:01.425237 [info ] [MainThread]: 
[0m17:38:01.425523 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:38:01.425921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e3abe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121e5ca30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ff9e80>]}
[0m17:38:01.426232 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:45:24.338786 | 873561c9-39a3-4d09-8bf3-19586e5534bc ==============================
[0m17:45:24.338835 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:45:24.340668 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:45:24.340996 [debug] [MainThread]: Tracking: tracking
[0m17:45:24.367904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a915e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a917f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a91ee0>]}
[0m17:45:24.468765 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:45:24.470082 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:45:24.470418 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:45:24.470649 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:45:24.471017 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:45:24.471230 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:45:24.487250 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:45:24.524641 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:45:24.528800 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:45:24.532675 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:45:24.538243 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:45:24.590615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c500d0>]}
[0m17:45:24.601871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a910d0>]}
[0m17:45:24.602878 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:45:24.603548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119aa8ca0>]}
[0m17:45:24.606956 [info ] [MainThread]: 
[0m17:45:24.608359 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:45:24.610611 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:45:24.633989 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:45:24.635146 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:45:24.635725 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:45:26.254414 [debug] [ThreadPool]: SQL status: OK in 1.62 seconds
[0m17:45:26.265984 [debug] [ThreadPool]: On list_schemas: Close
[0m17:45:26.795308 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:45:26.814399 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:26.814867 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:45:26.815139 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:45:26.815348 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:45:27.965933 [debug] [ThreadPool]: SQL status: OK in 1.15 seconds
[0m17:45:27.972045 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:45:27.972352 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:45:27.972543 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:45:28.422670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c21a30>]}
[0m17:45:28.423323 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:28.423548 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:45:28.423979 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:45:28.424311 [info ] [MainThread]: 
[0m17:45:28.431901 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:45:28.432514 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:45:28.433125 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:45:28.433312 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:45:28.433480 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:45:28.436113 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:45:28.436745 [debug] [Thread-1  ]: finished collecting timing info
[0m17:45:28.436922 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:45:28.478911 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:45:28.479803 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:28.480081 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:45:28.480264 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.jaffle_shop_customers

[0m17:45:28.480454 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:45:29.639624 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop.jaffle_shop_customers

[0m17:45:29.639947 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:45:29.640110 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:45:29.640243 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\x85\xd3\xe2\xbaQmL4\xa9\xcb\xaa|\x1cA\xbd/'
[0m17:45:29.640501 [debug] [Thread-1  ]: finished collecting timing info
[0m17:45:29.640719 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:45:29.640841 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:45:29.640954 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:45:30.106066 [debug] [Thread-1  ]: Runtime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)
  Catalog namespace is not supported.
[0m17:45:30.106718 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119de2730>]}
[0m17:45:30.107515 [error] [Thread-1  ]: 1 of 5 ERROR creating view model jaffle_shop_dbt.stg_customers ................. [[31mERROR[0m in 1.67s]
[0m17:45:30.108280 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:45:30.108867 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:45:30.110173 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:45:30.111855 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:45:30.112326 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:45:30.112621 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:45:30.117838 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:45:30.118468 [debug] [Thread-1  ]: finished collecting timing info
[0m17:45:30.118716 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:45:30.123498 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:45:30.124186 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:30.124370 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:45:30.124520 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.jaffle_shop_orders

[0m17:45:30.124662 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:45:31.194464 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop.jaffle_shop_orders

[0m17:45:31.195142 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:45:31.195547 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:45:31.195827 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xf9*\xb3\xfdB\x00BN\x96\x17p\x16i\xc3\x8a\xd6'
[0m17:45:31.196353 [debug] [Thread-1  ]: finished collecting timing info
[0m17:45:31.196910 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:45:31.197268 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:45:31.197543 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:45:31.665051 [debug] [Thread-1  ]: Runtime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)
  Catalog namespace is not supported.
[0m17:45:31.665886 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e15280>]}
[0m17:45:31.666632 [error] [Thread-1  ]: 2 of 5 ERROR creating view model jaffle_shop_dbt.stg_orders .................... [[31mERROR[0m in 1.55s]
[0m17:45:31.667494 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:45:31.668080 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:45:31.669024 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:45:31.670158 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:45:31.670640 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:45:31.671059 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:45:31.676346 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:45:31.677982 [debug] [Thread-1  ]: finished collecting timing info
[0m17:45:31.678487 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:45:31.684150 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:45:31.684935 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:31.685209 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:45:31.685389 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.stripe_payments

[0m17:45:31.685566 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:45:32.741386 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe.stripe_payments

[0m17:45:32.741925 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog namespace is not supported.
[0m17:45:32.742231 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Catalog namespace is not supported.
	at com.databricks.sql.managedcatalog.ManagedCatalogErrors$.catalogNamespaceNotSupportException(ManagedCatalogErrors.scala:40)
	at com.databricks.sql.managedcatalog.ExceptionManagedCatalog.getTablesByName(ExceptionManagedCatalog.scala:119)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.fastGetTablesByName(ManagedCatalogSessionCatalog.scala:839)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.fetchFromCatalog(DeltaCatalog.scala:345)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$loadTables$1(DeltaCatalog.scala:293)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.loadTables(DeltaCatalog.scala:291)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.$anonfun$submit$1(Analyzer.scala:1692)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$record(Analyzer.scala:1751)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anon$3.submit(Analyzer.scala:1666)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1276)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:216)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:301)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:294)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:126)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:184)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:274)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:331)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:273)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:128)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:265)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:129)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:126)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:118)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$2(SparkExecuteStatementOperation.scala:340)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:968)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$compileQuery$1(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.compileQuery(SparkExecuteStatementOperation.scala:334)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:390)
	... 16 more

[0m17:45:32.742471 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'l\x04U&\xe3\x1aF\xdd\xaf\x90=\xc46\xbfP\x8c'
[0m17:45:32.742878 [debug] [Thread-1  ]: finished collecting timing info
[0m17:45:32.743163 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:45:32.743384 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:45:32.743605 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:45:33.186530 [debug] [Thread-1  ]: Runtime Error in model stg_payments (models/staging/stripe/stg_payments.sql)
  Catalog namespace is not supported.
[0m17:45:33.187158 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '873561c9-39a3-4d09-8bf3-19586e5534bc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e41310>]}
[0m17:45:33.187781 [error] [Thread-1  ]: 3 of 5 ERROR creating view model jaffle_shop_dbt.stg_payments .................. [[31mERROR[0m in 1.52s]
[0m17:45:33.188483 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:45:33.189922 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:45:33.190403 [info ] [Thread-1  ]: 4 of 5 SKIP relation jaffle_shop_dbt.fct_orders ................................ [[33mSKIP[0m]
[0m17:45:33.191118 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:45:33.192075 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:45:33.192485 [info ] [Thread-1  ]: 5 of 5 SKIP relation jaffle_shop_dbt.dim_customers ............................. [[33mSKIP[0m]
[0m17:45:33.193075 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:45:33.195237 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:45:33.195928 [debug] [MainThread]: On master: ROLLBACK
[0m17:45:33.196405 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:45:33.673699 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:33.674104 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:45:33.674347 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:45:33.674590 [debug] [MainThread]: On master: ROLLBACK
[0m17:45:33.674806 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:45:33.675018 [debug] [MainThread]: On master: Close
[0m17:45:34.144415 [info ] [MainThread]: 
[0m17:45:34.145433 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 9.54 seconds (9.54s).
[0m17:45:34.146903 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:45:34.147879 [debug] [MainThread]: Connection 'model.jaffle_shop.stg_payments' was properly closed.
[0m17:45:34.170427 [info ] [MainThread]: 
[0m17:45:34.170979 [info ] [MainThread]: [31mCompleted with 3 errors and 0 warnings:[0m
[0m17:45:34.171468 [info ] [MainThread]: 
[0m17:45:34.171795 [error] [MainThread]: [33mRuntime Error in model stg_customers (models/staging/jaffle_shop/stg_customers.sql)[0m
[0m17:45:34.172076 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:45:34.172344 [info ] [MainThread]: 
[0m17:45:34.172612 [error] [MainThread]: [33mRuntime Error in model stg_orders (models/staging/jaffle_shop/stg_orders.sql)[0m
[0m17:45:34.172900 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:45:34.173182 [info ] [MainThread]: 
[0m17:45:34.173428 [error] [MainThread]: [33mRuntime Error in model stg_payments (models/staging/stripe/stg_payments.sql)[0m
[0m17:45:34.173665 [error] [MainThread]:   Catalog namespace is not supported.
[0m17:45:34.173906 [info ] [MainThread]: 
[0m17:45:34.174151 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=3 SKIP=2 TOTAL=5
[0m17:45:34.174501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c3a9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c3aa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e37a00>]}
[0m17:45:34.174773 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:46:32.367941 | 97e55c35-5117-43c1-b48f-7ffe58c7288c ==============================
[0m17:46:32.367987 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:46:32.369127 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m17:46:32.369632 [debug] [MainThread]: Tracking: tracking
[0m17:46:32.387403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cd9460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cd9bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cd99d0>]}
[0m17:46:32.439211 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m17:46:32.439957 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m17:46:32.440182 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m17:46:32.440336 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m17:46:32.440604 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.stripe.stripe_payments
[0m17:46:32.440744 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/stripe/src_stripe.yml
[0m17:46:32.453350 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:46:32.467589 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:46:32.471088 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:46:32.473935 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:46:32.476722 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:46:32.505565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e920d0>]}
[0m17:46:32.513116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116cde2e0>]}
[0m17:46:32.513531 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:46:32.513850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e6f040>]}
[0m17:46:32.515250 [info ] [MainThread]: 
[0m17:46:32.515770 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:46:32.516859 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m17:46:32.527600 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m17:46:32.527869 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:46:32.528024 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:46:33.630491 [debug] [ThreadPool]: SQL status: OK in 1.1 seconds
[0m17:46:33.641730 [debug] [ThreadPool]: On list_schemas: Close
[0m17:46:34.123864 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:46:34.137510 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:34.137972 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:46:34.138275 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:46:34.138468 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:46:35.367103 [debug] [ThreadPool]: SQL status: OK in 1.23 seconds
[0m17:46:35.374898 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:46:35.375438 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:46:35.375699 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:46:35.828850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e92730>]}
[0m17:46:35.829744 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:35.830147 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:46:35.830931 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:46:35.831471 [info ] [MainThread]: 
[0m17:46:35.837660 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:46:35.838278 [info ] [Thread-1  ]: 1 of 5 START view model jaffle_shop_dbt.stg_customers .......................... [RUN]
[0m17:46:35.839143 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:46:35.839490 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:46:35.839775 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:46:35.843870 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:46:35.844640 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:35.845537 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:46:35.885378 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_customers"
[0m17:46:35.885998 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:35.886211 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_customers"
[0m17:46:35.886343 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_customers"} */
create or replace view jaffle_shop_dbt.stg_customers
  
  
  as
    select
    id as customer_id,
    first_name,
    last_name

from default.jaffle_shop_customers

[0m17:46:35.886468 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:46:37.514591 [debug] [Thread-1  ]: SQL status: OK in 1.63 seconds
[0m17:46:37.535080 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:37.535497 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: ROLLBACK
[0m17:46:37.535709 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:46:37.535832 [debug] [Thread-1  ]: On model.jaffle_shop.stg_customers: Close
[0m17:46:37.995550 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1170280d0>]}
[0m17:46:37.996066 [info ] [Thread-1  ]: 1 of 5 OK created view model jaffle_shop_dbt.stg_customers ..................... [[32mOK[0m in 2.16s]
[0m17:46:37.996529 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:46:37.996798 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:46:37.997310 [info ] [Thread-1  ]: 2 of 5 START view model jaffle_shop_dbt.stg_orders ............................. [RUN]
[0m17:46:37.997876 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:46:37.998124 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:46:37.998274 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:46:38.002459 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:46:38.003035 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:38.003246 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:46:38.007039 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_orders"
[0m17:46:38.007643 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:38.007831 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_orders"
[0m17:46:38.007965 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_orders"} */
create or replace view jaffle_shop_dbt.stg_orders
  
  
  as
    select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from default.jaffle_shop_orders

[0m17:46:38.008101 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:46:39.458800 [debug] [Thread-1  ]: SQL status: OK in 1.45 seconds
[0m17:46:39.462420 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:39.462943 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: ROLLBACK
[0m17:46:39.463285 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:46:39.463657 [debug] [Thread-1  ]: On model.jaffle_shop.stg_orders: Close
[0m17:46:39.909985 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e985e0>]}
[0m17:46:39.910782 [info ] [Thread-1  ]: 2 of 5 OK created view model jaffle_shop_dbt.stg_orders ........................ [[32mOK[0m in 1.91s]
[0m17:46:39.911371 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:46:39.911880 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:46:39.912741 [info ] [Thread-1  ]: 3 of 5 START view model jaffle_shop_dbt.stg_payments ........................... [RUN]
[0m17:46:39.913758 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:46:39.914212 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:46:39.914551 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:46:39.919939 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:46:39.920652 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:39.920951 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:46:39.924348 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.stg_payments"
[0m17:46:39.925074 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:39.925322 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.stg_payments"
[0m17:46:39.925482 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.stg_payments"} */
create or replace view jaffle_shop_dbt.stg_payments
  
  
  as
    select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    amount,
    status

from default.stripe_payments

[0m17:46:39.925663 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:46:41.351348 [debug] [Thread-1  ]: SQL status: OK in 1.43 seconds
[0m17:46:41.356319 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:41.356842 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: ROLLBACK
[0m17:46:41.357188 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:46:41.357586 [debug] [Thread-1  ]: On model.jaffle_shop.stg_payments: Close
[0m17:46:41.809469 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e98400>]}
[0m17:46:41.810629 [info ] [Thread-1  ]: 3 of 5 OK created view model jaffle_shop_dbt.stg_payments ...................... [[32mOK[0m in 1.90s]
[0m17:46:41.812222 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:46:41.814331 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:46:41.815295 [info ] [Thread-1  ]: 4 of 5 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m17:46:41.816923 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m17:46:41.817384 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m17:46:41.817798 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m17:46:41.828832 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m17:46:41.829563 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:41.830030 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m17:46:41.881184 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m17:46:41.881866 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:41.882125 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m17:46:41.882315 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m17:46:41.882459 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:46:45.843301 [debug] [Thread-1  ]: SQL status: OK in 3.96 seconds
[0m17:46:45.854775 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:45.855073 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m17:46:45.855248 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:46:45.855382 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m17:46:46.300293 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1170306a0>]}
[0m17:46:46.335946 [info ] [Thread-1  ]: 4 of 5 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.48s]
[0m17:46:46.337860 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:46:46.339851 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:46:46.341096 [info ] [Thread-1  ]: 5 of 5 START table model jaffle_shop_dbt.dim_customers ......................... [RUN]
[0m17:46:46.342297 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m17:46:46.342774 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m17:46:46.343165 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m17:46:46.349945 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m17:46:46.351013 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:46.351335 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m17:46:46.362281 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.dim_customers"
[0m17:46:46.363477 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:46.363967 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.dim_customers"
[0m17:46:46.364380 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.dim_customers"} */

      create or replace table jaffle_shop_dbt.dim_customers
    
    
    using delta
    
    
    
    
    
    
    as
      with 

customers as (

    select * from jaffle_shop_dbt.stg_customers

),

orders as (
    select * from jaffle_shop_dbt.fct_orders
),

customer_orders as (
    select
        customer_id,
        count(order_id) as number_of_orders,
        sum(amount) as lifetime_value
    from orders
    group by 1
),
final as (
    select
        customers.customer_id,
        customers.first_name,
        customers.last_name
    from customers
    left join customer_orders using (customer_id)
)
select * from final
[0m17:46:46.364646 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:46:50.060185 [debug] [Thread-1  ]: SQL status: OK in 3.7 seconds
[0m17:46:50.064280 [debug] [Thread-1  ]: finished collecting timing info
[0m17:46:50.064582 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: ROLLBACK
[0m17:46:50.064772 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m17:46:50.064931 [debug] [Thread-1  ]: On model.jaffle_shop.dim_customers: Close
[0m17:46:50.510913 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97e55c35-5117-43c1-b48f-7ffe58c7288c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11704efa0>]}
[0m17:46:50.511787 [info ] [Thread-1  ]: 5 of 5 OK created table model jaffle_shop_dbt.dim_customers .................... [[32mOK[0m in 4.17s]
[0m17:46:50.512508 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:46:50.514523 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:46:50.514963 [debug] [MainThread]: On master: ROLLBACK
[0m17:46:50.515338 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:46:51.002589 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:46:51.003035 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:46:51.003412 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:46:51.003830 [debug] [MainThread]: On master: ROLLBACK
[0m17:46:51.004194 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:46:51.004462 [debug] [MainThread]: On master: Close
[0m17:46:51.480786 [info ] [MainThread]: 
[0m17:46:51.481520 [info ] [MainThread]: Finished running 3 view models, 2 table models in 0 hours 0 minutes and 18.97 seconds (18.97s).
[0m17:46:51.482031 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:46:51.482230 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m17:46:51.492841 [info ] [MainThread]: 
[0m17:46:51.493212 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:46:51.493517 [info ] [MainThread]: 
[0m17:46:51.493764 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m17:46:51.494092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e62d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e8f430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e8fdf0>]}
[0m17:46:51.494371 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 17:47:54.213586 | 4e865882-8c74-48ed-9303-f10f97d6bdd9 ==============================
[0m17:47:54.213627 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:47:54.215270 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m17:47:54.216238 [debug] [MainThread]: Tracking: tracking
[0m17:47:54.234273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f9a10d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f9a1430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f9a1640>]}
[0m17:47:54.287432 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:47:54.287696 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:47:54.294683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e865882-8c74-48ed-9303-f10f97d6bdd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fb5afa0>]}
[0m17:47:54.302148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e865882-8c74-48ed-9303-f10f97d6bdd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11faa5ca0>]}
[0m17:47:54.302487 [info ] [MainThread]: Found 5 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:47:54.302778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e865882-8c74-48ed-9303-f10f97d6bdd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11faa5f10>]}
[0m17:47:54.304341 [info ] [MainThread]: 
[0m17:47:54.304920 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m17:47:54.306295 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m17:47:54.318776 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:47:54.319201 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m17:47:54.319533 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m17:47:54.319707 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:47:55.755938 [debug] [ThreadPool]: SQL status: OK in 1.44 seconds
[0m17:47:55.768463 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m17:47:55.768824 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:47:55.769064 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m17:47:56.369863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e865882-8c74-48ed-9303-f10f97d6bdd9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fb1df40>]}
[0m17:47:56.371100 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:47:56.371812 [info ] [MainThread]: 
[0m17:47:56.379168 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_customers
[0m17:47:56.379920 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_customers"
[0m17:47:56.380248 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_customers
[0m17:47:56.380523 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_customers
[0m17:47:56.384781 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_customers"
[0m17:47:56.385518 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.385838 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_customers
[0m17:47:56.386094 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.386700 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_customers
[0m17:47:56.386979 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_orders
[0m17:47:56.387390 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_orders"
[0m17:47:56.387610 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_orders
[0m17:47:56.387814 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_orders
[0m17:47:56.391231 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_orders"
[0m17:47:56.392104 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.392465 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_orders
[0m17:47:56.392703 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.393347 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_orders
[0m17:47:56.393767 [debug] [Thread-1  ]: Began running node model.jaffle_shop.stg_payments
[0m17:47:56.394304 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.stg_payments"
[0m17:47:56.394548 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.stg_payments
[0m17:47:56.394742 [debug] [Thread-1  ]: Compiling model.jaffle_shop.stg_payments
[0m17:47:56.399998 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.stg_payments"
[0m17:47:56.400596 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.400809 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.stg_payments
[0m17:47:56.400976 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.401421 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.stg_payments
[0m17:47:56.402074 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m17:47:56.402439 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m17:47:56.402605 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m17:47:56.402753 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m17:47:56.405657 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m17:47:56.406225 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.406526 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m17:47:56.406693 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.407145 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m17:47:56.407719 [debug] [Thread-1  ]: Began running node model.jaffle_shop.dim_customers
[0m17:47:56.408026 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.dim_customers"
[0m17:47:56.408167 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.dim_customers
[0m17:47:56.408296 [debug] [Thread-1  ]: Compiling model.jaffle_shop.dim_customers
[0m17:47:56.411098 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.dim_customers"
[0m17:47:56.412153 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.412407 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.dim_customers
[0m17:47:56.412798 [debug] [Thread-1  ]: finished collecting timing info
[0m17:47:56.413335 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.dim_customers
[0m17:47:56.414326 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:47:56.414492 [debug] [MainThread]: Connection 'model.jaffle_shop.dim_customers' was properly closed.
[0m17:47:56.421407 [info ] [MainThread]: Done.
[0m17:47:56.427232 [debug] [MainThread]: Acquiring new databricks connection "generate_catalog"
[0m17:47:56.427737 [info ] [MainThread]: Building catalog
[0m17:47:56.429605 [debug] [ThreadPool]: Acquiring new databricks connection "default"
[0m17:47:56.429956 [debug] [ThreadPool]: On "default": cache miss for schema "{self.database}.{self.schema}", this is inefficient
[0m17:47:56.431817 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:47:56.432005 [debug] [ThreadPool]: Using databricks connection "default"
[0m17:47:56.432170 [debug] [ThreadPool]: On default: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "default"} */
show table extended in default like '*'
  
[0m17:47:56.432334 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:47:58.492594 [debug] [ThreadPool]: SQL status: OK in 2.06 seconds
[0m17:47:58.499085 [debug] [ThreadPool]: with database=None, schema=default, relations=[_ReferenceKey(database=None, schema='default', identifier='diamonds'), _ReferenceKey(database=None, schema='default', identifier='jaffle_shop_customers'), _ReferenceKey(database=None, schema='default', identifier='jaffle_shop_orders'), _ReferenceKey(database=None, schema='default', identifier='language_state_oh'), _ReferenceKey(database=None, schema='default', identifier='stripe_payments'), _ReferenceKey(database=None, schema='default', identifier='zzz_game_opponents'), _ReferenceKey(database=None, schema='default', identifier='zzz_game_scores'), _ReferenceKey(database=None, schema='default', identifier='zzz_games'), _ReferenceKey(database=None, schema='default', identifier='zzz_teams')]
[0m17:47:58.499578 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.diamonds
[0m17:47:58.500152 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.jaffle_shop_customers
[0m17:47:58.500577 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.jaffle_shop_orders
[0m17:47:58.500979 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.language_state_oh
[0m17:47:58.501382 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.stripe_payments
[0m17:47:58.501779 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.zzz_game_opponents
[0m17:47:58.502141 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.zzz_game_scores
[0m17:47:58.502497 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.zzz_games
[0m17:47:58.502866 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation default.zzz_teams
[0m17:47:58.508434 [debug] [ThreadPool]: On default: ROLLBACK
[0m17:47:58.508818 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m17:47:58.509081 [debug] [ThreadPool]: On default: Close
[0m17:47:59.018097 [debug] [ThreadPool]: Acquiring new databricks connection "jaffle_shop_dbt"
[0m17:47:59.018902 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.dim_customers
[0m17:47:59.019565 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.fct_orders
[0m17:47:59.020144 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_customers
[0m17:47:59.020767 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_orders
[0m17:47:59.021301 [debug] [ThreadPool]: Spark adapter: Getting table schema for relation jaffle_shop_dbt.stg_payments
[0m17:47:59.046364 [info ] [MainThread]: Catalog written to /Users/tcsilva/workspace/pessoal/estudos/dbt/jaffle_shop/target/catalog.json
[0m17:47:59.047011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f9a10d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc9f910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc9f6a0>]}
[0m17:47:59.047561 [debug] [MainThread]: Flushing usage events
[0m17:47:59.696955 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m17:47:59.697539 [debug] [MainThread]: Connection 'jaffle_shop_dbt' was properly closed.


============================== 2022-10-13 17:48:33.460614 | 5b993a59-be75-427f-a788-ecac1c65d45e ==============================
[0m17:48:33.460677 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:48:33.461875 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m17:48:33.462365 [debug] [MainThread]: Tracking: tracking
[0m17:48:33.479763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da91130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da91b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da915e0>]}
[0m17:48:33.484016 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m17:48:33.484501 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m17:48:33.484783 [info ] [MainThread]: 
[0m17:48:33.485115 [info ] [MainThread]: 
[0m17:48:33.485336 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-10-13 19:55:58.777479 | 9c8c535e-cdb2-44ec-aca6-304b71c718d7 ==============================
[0m19:55:58.777531 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:55:58.779321 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:55:58.779657 [debug] [MainThread]: Tracking: tracking
[0m19:55:58.818899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c02e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c02b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c02430>]}
[0m19:55:58.856176 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:55:58.856597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9c8c535e-cdb2-44ec-aca6-304b71c718d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bdcca0>]}
[0m19:55:58.909447 [debug] [MainThread]: Parsing macros/statement.sql
[0m19:55:58.914349 [debug] [MainThread]: Parsing macros/catalog.sql
[0m19:55:58.917903 [debug] [MainThread]: Parsing macros/adapters.sql
[0m19:55:58.948217 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m19:55:58.960360 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m19:55:58.961174 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m19:55:58.967055 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m19:55:58.994686 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m19:55:59.007044 [debug] [MainThread]: Parsing macros/adapters.sql
[0m19:55:59.071296 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m19:55:59.074571 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m19:55:59.088909 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m19:55:59.090571 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m19:55:59.096387 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m19:55:59.126026 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m19:55:59.133518 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m19:55:59.144717 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m19:55:59.156026 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m19:55:59.156951 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m19:55:59.158405 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m19:55:59.165897 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m19:55:59.168833 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m19:55:59.184192 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m19:55:59.185742 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m19:55:59.186782 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m19:55:59.189622 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m19:55:59.197055 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m19:55:59.202414 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m19:55:59.204612 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m19:55:59.222915 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m19:55:59.242261 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m19:55:59.259470 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m19:55:59.265466 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m19:55:59.267706 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m19:55:59.269543 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m19:55:59.274698 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m19:55:59.299069 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m19:55:59.301857 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m19:55:59.312334 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m19:55:59.329132 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m19:55:59.339173 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m19:55:59.343163 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m19:55:59.353530 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m19:55:59.355155 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m19:55:59.359037 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m19:55:59.362289 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m19:55:59.370899 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m19:55:59.400875 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m19:55:59.403868 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m19:55:59.406960 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m19:55:59.408717 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m19:55:59.409687 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m19:55:59.410614 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m19:55:59.411471 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m19:55:59.413485 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m19:55:59.418845 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m19:55:59.427837 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m19:55:59.428819 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m19:55:59.430988 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m19:55:59.433280 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m19:55:59.434633 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m19:55:59.436362 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m19:55:59.438403 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m19:55:59.441175 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m19:55:59.443110 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m19:55:59.446998 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m19:55:59.449875 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m19:55:59.451717 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m19:55:59.453239 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m19:55:59.454979 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m19:55:59.456226 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m19:55:59.457429 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m19:55:59.458545 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m19:55:59.465927 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m19:55:59.467225 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m19:55:59.469230 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m19:55:59.471271 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m19:55:59.472535 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m19:55:59.475074 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m19:55:59.477812 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m19:55:59.501715 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m19:55:59.504911 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m19:55:59.521557 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m19:55:59.526846 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m19:55:59.539794 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m19:55:59.556045 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m19:56:00.053917 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m19:56:00.070776 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m19:56:00.075676 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m19:56:00.078881 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m19:56:00.085617 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m19:56:00.091981 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m19:56:00.165637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9c8c535e-cdb2-44ec-aca6-304b71c718d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112db85b0>]}
[0m19:56:00.175418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9c8c535e-cdb2-44ec-aca6-304b71c718d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112bf1940>]}
[0m19:56:00.175848 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:56:00.176116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9c8c535e-cdb2-44ec-aca6-304b71c718d7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3bb50>]}
[0m19:56:00.176855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112c3bbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112dcb730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112dcb910>]}
[0m19:56:00.177108 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 19:57:00.074492 | 52af82c7-a20f-44dd-ab34-76bdd3589365 ==============================
[0m19:57:00.074548 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:57:00.075486 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:57:00.075756 [debug] [MainThread]: Tracking: tracking
[0m19:57:00.092257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e242b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e242430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e242850>]}
[0m19:57:00.121335 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:57:00.121718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '52af82c7-a20f-44dd-ab34-76bdd3589365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e21c730>]}
[0m19:57:00.125662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e38ddf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e38d6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e38d7f0>]}
[0m19:57:00.126011 [debug] [MainThread]: Flushing usage events
[0m19:57:00.947988 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m19:57:00.965065 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 19:57:06.222605 | e9152df2-1177-4b2e-ab4e-52ec0895130a ==============================
[0m19:57:06.222693 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:57:06.223726 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:57:06.223948 [debug] [MainThread]: Tracking: tracking
[0m19:57:06.242225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123836b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123836df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123836430>]}
[0m19:57:06.271737 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:57:06.272123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e9152df2-1177-4b2e-ab4e-52ec0895130a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123810730>]}
[0m19:57:06.276115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123981df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1239816a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1239817f0>]}
[0m19:57:06.276407 [debug] [MainThread]: Flushing usage events
[0m19:57:06.873342 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m19:57:06.875769 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 19:57:18.697599 | de7e22f4-9386-494b-b1da-ec9565ab1eac ==============================
[0m19:57:18.697669 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:57:18.699287 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:57:18.699553 [debug] [MainThread]: Tracking: tracking
[0m19:57:18.715840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cf3af10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cf3a5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cf3a340>]}
[0m19:57:18.747349 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:57:18.748014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'de7e22f4-9386-494b-b1da-ec9565ab1eac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cf14c70>]}
[0m19:57:18.752638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d085df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d0856a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d0857f0>]}
[0m19:57:18.752949 [debug] [MainThread]: Flushing usage events
[0m19:57:19.746250 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m19:57:19.750779 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  10 |       freshness:
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 19:58:16.109953 | a984a268-0e9e-4566-a2de-e6b3f7d4c200 ==============================
[0m19:58:16.110004 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:58:16.111090 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:58:16.111319 [debug] [MainThread]: Tracking: tracking
[0m19:58:16.127962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118559250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185595b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118559850>]}
[0m19:58:16.160812 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:58:16.161212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a984a268-0e9e-4566-a2de-e6b3f7d4c200', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118533fd0>]}
[0m19:58:16.165214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186a4d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186a4700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186a4820>]}
[0m19:58:16.165489 [debug] [MainThread]: Flushing usage events
[0m19:58:17.173514 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m19:58:17.178367 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  10 |       freshness:
  11 |         warn_after: {count: 12, period: hour}
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 19:58:25.173601 | 3e62d765-768c-4bef-b172-6015f24aed1c ==============================
[0m19:58:25.173667 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:58:25.174734 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:58:25.175037 [debug] [MainThread]: Tracking: tracking
[0m19:58:25.195064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c9ef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c9e5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c9e340>]}
[0m19:58:25.224819 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:58:25.225383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3e62d765-768c-4bef-b172-6015f24aed1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c787c0>]}
[0m19:58:25.231112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114de9df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114de96a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114de97f0>]}
[0m19:58:25.231473 [debug] [MainThread]: Flushing usage events
[0m19:58:25.819559 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m19:58:25.823709 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  10 |       freshness:
  11 |         warn_after: {count: 12, period: hour}
  12 |         error_after: {count: 24, period: hour}
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 19:58:29.726829 | 1b2d5dae-0f0e-469f-8c00-2dbc81a6b8d3 ==============================
[0m19:58:29.726890 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:58:29.728240 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:58:29.728585 [debug] [MainThread]: Tracking: tracking
[0m19:58:29.747243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12237ee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12237e5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12237e370>]}
[0m19:58:29.775594 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m19:58:29.776026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1b2d5dae-0f0e-469f-8c00-2dbc81a6b8d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122358bb0>]}
[0m19:58:29.781370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224c9d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224c9700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224c9820>]}
[0m19:58:29.781705 [debug] [MainThread]: Flushing usage events
[0m19:58:30.378758 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m19:58:30.383486 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  10 |       freshness:
  11 |         warn_after: {count: 12, period: hour}
  12 |         error_after: {count: 24, period: hour}
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 20:00:49.531674 | 84809b50-c20e-4fd2-89cb-9d5e4cc06a61 ==============================
[0m20:00:49.531784 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:00:49.533524 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:00:49.533975 [debug] [MainThread]: Tracking: tracking
[0m20:00:49.557604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115756eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115756c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115756160>]}
[0m20:00:49.597666 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m20:00:49.598250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '84809b50-c20e-4fd2-89cb-9d5e4cc06a61', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11578d100>]}
[0m20:00:49.607498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158a7d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158a7700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158a7850>]}
[0m20:00:49.607891 [debug] [MainThread]: Flushing usage events
[0m20:00:55.528427 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7
[0m20:00:55.543069 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 9
  ------------------------------
  6  |     tables:
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |       load_at_field: ORDER_DATE
  10 |       freshness:
  11 |         warn_after: {count: 12, period: hour}
  12 |         error_after: {count: 24, period: hour}
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 9, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 9
    ------------------------------
    6  |     tables:
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |       load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 9, column 7



============================== 2022-10-13 20:01:29.407733 | 0a82b1a2-b952-45e5-95d2-fb7ffc36a585 ==============================
[0m20:01:29.407792 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:01:29.409038 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:01:29.409400 [debug] [MainThread]: Tracking: tracking
[0m20:01:29.428798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119deee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dee160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dee430>]}
[0m20:01:29.464846 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m20:01:29.465633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0a82b1a2-b952-45e5-95d2-fb7ffc36a585', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e1fa90>]}
[0m20:01:29.473914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f39dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f396a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f39850>]}
[0m20:01:29.474243 [debug] [MainThread]: Flushing usage events
[0m20:01:30.128616 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 10
    ------------------------------
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |         load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 10, column 7
[0m20:01:30.139099 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 7, column 7
did not find expected '-' indicator
  in "<unicode string>", line 10, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 10
  ------------------------------
  7  |       - name: jaffle_shop_orders
  8  |       - name: jaffle_shop_customers
  9  |         load_at_field: ORDER_DATE
  10 |       freshness:
  11 |         warn_after: {count: 12, period: hour}
  12 |         error_after: {count: 24, period: hour}
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 7, column 7
  did not find expected '-' indicator
    in "<unicode string>", line 10, column 7

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/src_jaffle_shop.yml - Runtime Error
    Syntax error near line 10
    ------------------------------
    7  |       - name: jaffle_shop_orders
    8  |       - name: jaffle_shop_customers
    9  |         load_at_field: ORDER_DATE
    10 |       freshness:
    11 |         warn_after: {count: 12, period: hour}
    12 |         error_after: {count: 24, period: hour}
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 7, column 7
    did not find expected '-' indicator
      in "<unicode string>", line 10, column 7



============================== 2022-10-13 20:01:36.297796 | b768a7bf-7c57-4f1b-9d53-ecad19cb0b58 ==============================
[0m20:01:36.297908 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:01:36.299161 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:01:36.299597 [debug] [MainThread]: Tracking: tracking
[0m20:01:36.320277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c579eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c579310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c579160>]}
[0m20:01:36.362261 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m20:01:36.362738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b768a7bf-7c57-4f1b-9d53-ecad19cb0b58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5bb670>]}
[0m20:01:36.411547 [debug] [MainThread]: Parsing macros/statement.sql
[0m20:01:36.415330 [debug] [MainThread]: Parsing macros/catalog.sql
[0m20:01:36.417854 [debug] [MainThread]: Parsing macros/adapters.sql
[0m20:01:36.445351 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m20:01:36.454661 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m20:01:36.455431 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m20:01:36.458905 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m20:01:36.480405 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m20:01:36.488748 [debug] [MainThread]: Parsing macros/adapters.sql
[0m20:01:36.544682 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m20:01:36.548834 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m20:01:36.559834 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m20:01:36.560573 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m20:01:36.563965 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m20:01:36.593104 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m20:01:36.598929 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m20:01:36.607081 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m20:01:36.613390 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m20:01:36.614207 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m20:01:36.615689 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m20:01:36.621293 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m20:01:36.623644 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m20:01:36.637891 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m20:01:36.638751 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m20:01:36.639380 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m20:01:36.641382 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m20:01:36.645823 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m20:01:36.648474 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m20:01:36.650216 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m20:01:36.667080 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m20:01:36.681828 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m20:01:36.694818 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m20:01:36.699885 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m20:01:36.701759 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m20:01:36.703503 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m20:01:36.707676 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m20:01:36.724693 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m20:01:36.726443 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m20:01:36.737915 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m20:01:36.754312 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m20:01:36.759730 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m20:01:36.762517 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m20:01:36.767614 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m20:01:36.768984 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m20:01:36.772587 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m20:01:36.775469 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m20:01:36.784003 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m20:01:36.803909 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m20:01:36.805453 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m20:01:36.807836 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m20:01:36.809402 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m20:01:36.810290 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m20:01:36.811106 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m20:01:36.811773 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m20:01:36.813113 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m20:01:36.817178 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m20:01:36.827418 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m20:01:36.828570 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m20:01:36.830234 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m20:01:36.831685 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m20:01:36.833275 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m20:01:36.834930 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m20:01:36.836222 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m20:01:36.837727 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m20:01:36.839194 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m20:01:36.842326 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m20:01:36.844307 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m20:01:36.846073 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m20:01:36.847628 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m20:01:36.849165 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m20:01:36.850152 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m20:01:36.851357 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m20:01:36.852414 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m20:01:36.858329 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m20:01:36.859345 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m20:01:36.861166 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m20:01:36.863182 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m20:01:36.864345 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m20:01:36.866617 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m20:01:36.869313 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m20:01:36.886089 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m20:01:36.890159 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m20:01:36.904931 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m20:01:36.909210 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m20:01:36.915893 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m20:01:36.927468 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m20:01:37.308789 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:01:37.322040 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:01:37.326147 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:01:37.330181 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m20:01:37.334345 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:01:37.338968 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:01:37.404541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b768a7bf-7c57-4f1b-9d53-ecad19cb0b58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c71b760>]}
[0m20:01:37.414054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b768a7bf-7c57-4f1b-9d53-ecad19cb0b58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c579820>]}
[0m20:01:37.414477 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:01:37.414782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b768a7bf-7c57-4f1b-9d53-ecad19cb0b58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c5b1e80>]}
[0m20:01:37.416340 [info ] [MainThread]: 
[0m20:01:37.416743 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m20:01:37.423655 [info ] [MainThread]: Done.
[0m20:01:37.424296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c75a8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c75a520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c75a820>]}
[0m20:01:37.424586 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:01:37.953699 | e4b5aa42-104b-4361-8af4-09b285a43c89 ==============================
[0m20:01:37.953866 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:01:37.954849 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:01:37.955109 [debug] [MainThread]: Tracking: tracking
[0m20:01:37.970867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11495e1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11495e5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11495e850>]}
[0m20:01:38.037939 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:01:38.038518 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:01:38.050496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4b5aa42-104b-4361-8af4-09b285a43c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114af20d0>]}
[0m20:01:38.062372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4b5aa42-104b-4361-8af4-09b285a43c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a333a0>]}
[0m20:01:38.062822 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:01:38.063128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4b5aa42-104b-4361-8af4-09b285a43c89', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a33430>]}
[0m20:01:38.064149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a33400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a33430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114a33190>]}
[0m20:01:38.064488 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:02:33.919584 | 5fb0e3c8-a398-4111-a72e-4f6cffbab97b ==============================
[0m20:02:33.919659 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:02:33.920608 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:02:33.920964 [debug] [MainThread]: Tracking: tracking
[0m20:02:33.943507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbba550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbbaa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cbba520>]}
[0m20:02:34.004047 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:02:34.004831 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:02:34.005079 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:02:34.005233 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:02:34.018270 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:02:34.037161 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:02:34.043552 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:02:34.047279 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:02:34.050653 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:02:34.081327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5fb0e3c8-a398-4111-a72e-4f6cffbab97b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116df10d0>]}
[0m20:02:34.090244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5fb0e3c8-a398-4111-a72e-4f6cffbab97b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc9e5e0>]}
[0m20:02:34.090678 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:02:34.091004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5fb0e3c8-a398-4111-a72e-4f6cffbab97b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cc9e670>]}
[0m20:02:34.092464 [info ] [MainThread]: 
[0m20:02:34.092837 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m20:02:34.100351 [info ] [MainThread]: Done.
[0m20:02:34.100910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cb97af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd4e6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10cd4e700>]}
[0m20:02:34.101188 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:02:35.249831 | 4a7d7ef2-4c99-4add-b502-bd9aee196563 ==============================
[0m20:02:35.249884 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:02:35.251033 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:02:35.251319 [debug] [MainThread]: Tracking: tracking
[0m20:02:35.268049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e56d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e56580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e568e0>]}
[0m20:02:35.326618 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:02:35.326895 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:02:35.334079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a7d7ef2-4c99-4add-b502-bd9aee196563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117fea0d0>]}
[0m20:02:35.342838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a7d7ef2-4c99-4add-b502-bd9aee196563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f37460>]}
[0m20:02:35.343270 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:02:35.343552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a7d7ef2-4c99-4add-b502-bd9aee196563', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f374f0>]}
[0m20:02:35.344314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f374c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f37040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117f37190>]}
[0m20:02:35.344536 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:02:50.119159 | de6ca96d-8c88-4a17-83b7-282975a9cfcf ==============================
[0m20:02:50.119235 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:02:50.120590 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:02:50.121106 [debug] [MainThread]: Tracking: tracking
[0m20:02:50.140502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c0d5eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c0d5c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c0d5160>]}
[0m20:02:50.199083 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:02:50.199869 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:02:50.200127 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:02:50.200288 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:02:50.212976 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:02:50.226520 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:02:50.230905 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:02:50.234697 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:02:50.238331 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:02:50.269204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de6ca96d-8c88-4a17-83b7-282975a9cfcf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c3150d0>]}
[0m20:02:50.277843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de6ca96d-8c88-4a17-83b7-282975a9cfcf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c10d970>]}
[0m20:02:50.278228 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:02:50.278628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de6ca96d-8c88-4a17-83b7-282975a9cfcf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c10d9d0>]}
[0m20:02:50.280060 [info ] [MainThread]: 
[0m20:02:50.280477 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m20:02:50.288007 [info ] [MainThread]: Done.
[0m20:02:50.288495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1ba610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1ba670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1ba430>]}
[0m20:02:50.288770 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:02:53.071317 | be8efcbf-f1cb-42ec-87a9-7d093031333d ==============================
[0m20:02:53.071368 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:02:53.072366 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:02:53.072602 [debug] [MainThread]: Tracking: tracking
[0m20:02:53.091899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211bee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211be5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211be370>]}
[0m20:02:53.146867 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:02:53.147220 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:02:53.154599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be8efcbf-f1cb-42ec-87a9-7d093031333d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213530d0>]}
[0m20:02:53.163525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be8efcbf-f1cb-42ec-87a9-7d093031333d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121288400>]}
[0m20:02:53.163947 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:02:53.164214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be8efcbf-f1cb-42ec-87a9-7d093031333d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121288490>]}
[0m20:02:53.164955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121288460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121288490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1212880a0>]}
[0m20:02:53.165180 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:03:02.652615 | ee57187c-199a-4b51-a011-1055b462e16a ==============================
[0m20:03:02.652727 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:03:02.653734 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:03:02.654292 [debug] [MainThread]: Tracking: tracking
[0m20:03:02.670697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d72eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d72d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d72160>]}
[0m20:03:02.729667 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:03:02.730621 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:03:02.730975 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:03:02.731270 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:03:02.747032 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:03:02.761502 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:03:02.764489 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:03:02.767326 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:03:02.769924 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:03:02.803130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee57187c-199a-4b51-a011-1055b462e16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119fb50d0>]}
[0m20:03:02.811202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee57187c-199a-4b51-a011-1055b462e16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dad970>]}
[0m20:03:02.811645 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:03:02.811996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee57187c-199a-4b51-a011-1055b462e16a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dad9d0>]}
[0m20:03:02.813288 [info ] [MainThread]: 
[0m20:03:02.813646 [warn ] [MainThread]: [[33mWARNING[0m]: Nothing to do. Try checking your model configs and model specification args
[0m20:03:02.820039 [info ] [MainThread]: Done.
[0m20:03:02.820564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e56610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e56670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e56430>]}
[0m20:03:02.820838 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:03:04.587916 | aa49ce9d-4bba-4c55-8854-cfaeacd57f38 ==============================
[0m20:03:04.588018 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:03:04.589275 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:03:04.589871 [debug] [MainThread]: Tracking: tracking
[0m20:03:04.605629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ba8b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ba8df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ba8430>]}
[0m20:03:04.655963 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:03:04.656232 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:03:04.662797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aa49ce9d-4bba-4c55-8854-cfaeacd57f38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122d3e0d0>]}
[0m20:03:04.670123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aa49ce9d-4bba-4c55-8854-cfaeacd57f38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c82430>]}
[0m20:03:04.670469 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:03:04.670723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa49ce9d-4bba-4c55-8854-cfaeacd57f38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c824c0>]}
[0m20:03:04.671462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c82490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c824c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c82100>]}
[0m20:03:04.671684 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:17:48.471089 | d365de87-8f51-4d2f-a335-a5119aeaa85e ==============================
[0m20:17:48.471226 [info ] [MainThread]: Running with dbt=1.2.1


============================== 2022-10-13 20:17:48.471427 | 45f03a4f-24ca-42d8-8c61-60ed9663f958 ==============================
[0m20:17:48.471500 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:17:48.472788 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:17:48.472698 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:17:48.473115 [debug] [MainThread]: Tracking: tracking
[0m20:17:48.473147 [debug] [MainThread]: Tracking: tracking
[0m20:17:48.500018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc5ab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc5a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fc5a340>]}
[0m20:17:48.500633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6ce7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6cea30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6ce520>]}
[0m20:17:48.602443 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:17:48.602477 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:17:48.603282 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:17:48.603355 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:17:48.603575 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:17:48.603630 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:17:48.603834 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:17:48.603834 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:17:48.618503 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:17:48.618562 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:17:48.633888 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:17:48.634698 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:17:48.638291 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:17:48.638317 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:17:48.642524 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:17:48.642622 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:17:48.646357 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:17:48.646363 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:17:48.678635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd365de87-8f51-4d2f-a335-a5119aeaa85e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9050d0>]}
[0m20:17:48.678679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '45f03a4f-24ca-42d8-8c61-60ed9663f958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fe86f10>]}
[0m20:17:48.687252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd365de87-8f51-4d2f-a335-a5119aeaa85e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7b25e0>]}
[0m20:17:48.687480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45f03a4f-24ca-42d8-8c61-60ed9663f958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3b430>]}
[0m20:17:48.687679 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:17:48.687897 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:17:48.688036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd365de87-8f51-4d2f-a335-a5119aeaa85e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7b2670>]}
[0m20:17:48.688208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45f03a4f-24ca-42d8-8c61-60ed9663f958', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3b4c0>]}
[0m20:17:48.688969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3b490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3b4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd3b100>]}
[0m20:17:48.689231 [debug] [MainThread]: Flushing usage events
[0m20:17:48.689435 [info ] [MainThread]: 
[0m20:17:48.690043 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:17:48.691223 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:17:48.704393 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:17:48.704725 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:17:48.704955 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:17:48.705278 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:17:50.665274 [debug] [ThreadPool]: SQL status: OK in 1.96 seconds
[0m20:17:50.687536 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:17:50.687929 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:17:50.688199 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:17:51.402768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd365de87-8f51-4d2f-a335-a5119aeaa85e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a6ce970>]}
[0m20:17:51.404533 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:17:51.405991 [info ] [MainThread]: 
[0m20:17:51.417298 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:17:51.417679 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_customers .................... [RUN]
[0m20:17:51.418378 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_customers"
[0m20:17:51.418636 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:17:51.418884 [debug] [Thread-1  ]: finished collecting timing info
[0m20:17:51.419102 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:17:51.419604 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_customers"
[0m20:17:51.419835 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:17:51.420035 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:17:51.431707 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_customers"
[0m20:17:51.431970 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_customers: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_customers"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_customers
    
  
[0m20:17:51.432209 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:17:52.797825 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_customers"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_customers
    
  
[0m20:17:52.798458 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Column 'ORDER_DATE' does not exist. Did you mean one of the following? [spark_catalog.default.jaffle_shop_customers.ID, spark_catalog.default.jaffle_shop_customers.LAST_NAME, spark_catalog.default.jaffle_shop_customers.FIRST_NAME]; line 3 pos 10
[0m20:17:52.798783 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [MISSING_COLUMN] org.apache.spark.sql.AnalysisException: Column 'ORDER_DATE' does not exist. Did you mean one of the following? [spark_catalog.default.jaffle_shop_customers.ID, spark_catalog.default.jaffle_shop_customers.LAST_NAME, spark_catalog.default.jaffle_shop_customers.FIRST_NAME]; line 3 pos 10
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Column 'ORDER_DATE' does not exist. Did you mean one of the following? [spark_catalog.default.jaffle_shop_customers.ID, spark_catalog.default.jaffle_shop_customers.LAST_NAME, spark_catalog.default.jaffle_shop_customers.FIRST_NAME]; line 3 pos 10
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m20:17:52.799238 [debug] [Thread-1  ]: Databricks adapter: operation-id: b';0\xbb<>\tE\xc0\xbc\xafe\xf2`.\xca\x00'
[0m20:17:52.799794 [debug] [Thread-1  ]: Databricks adapter: Error while running:
macro collect_freshness
[0m20:17:52.800101 [debug] [Thread-1  ]: Databricks adapter: Runtime Error
  Column 'ORDER_DATE' does not exist. Did you mean one of the following? [spark_catalog.default.jaffle_shop_customers.ID, spark_catalog.default.jaffle_shop_customers.LAST_NAME, spark_catalog.default.jaffle_shop_customers.FIRST_NAME]; line 3 pos 10
[0m20:17:52.800658 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_customers: ROLLBACK
[0m20:17:52.800953 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:17:52.801221 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_customers: Close
[0m20:17:53.353798 [debug] [Thread-1  ]: finished collecting timing info
[0m20:17:53.354833 [debug] [Thread-1  ]: Runtime Error in source jaffle_shop_customers (models/staging/jaffle_shop/src_jaffle_shop.yml)
  Runtime Error
    Column 'ORDER_DATE' does not exist. Did you mean one of the following? [spark_catalog.default.jaffle_shop_customers.ID, spark_catalog.default.jaffle_shop_customers.LAST_NAME, spark_catalog.default.jaffle_shop_customers.FIRST_NAME]; line 3 pos 10
[0m20:17:53.355787 [error] [Thread-1  ]: 1 of 1 ERROR freshness of jaffle_shop.jaffle_shop_customers .................... [[31mERROR[0m in 1.94s]
[0m20:17:53.357209 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:17:53.359645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:17:53.360083 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_customers' was properly closed.
[0m20:17:53.372760 [info ] [MainThread]: 
[0m20:17:53.373753 [error] [MainThread]: [33mRuntime Error in source jaffle_shop_customers (models/staging/jaffle_shop/src_jaffle_shop.yml)[0m
[0m20:17:53.374184 [error] [MainThread]:   Runtime Error
[0m20:17:53.374665 [error] [MainThread]:     Column 'ORDER_DATE' does not exist. Did you mean one of the following? [spark_catalog.default.jaffle_shop_customers.ID, spark_catalog.default.jaffle_shop_customers.LAST_NAME, spark_catalog.default.jaffle_shop_customers.FIRST_NAME]; line 3 pos 10
[0m20:17:53.375022 [info ] [MainThread]: Done.
[0m20:17:53.375484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a98df10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a98dcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a98dc40>]}
[0m20:17:53.375873 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:19:37.310264 | be42698c-02ea-4766-8ce1-eef158f3e922 ==============================
[0m20:19:37.310309 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:19:37.311635 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:19:37.312191 [debug] [MainThread]: Tracking: tracking
[0m20:19:37.328922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f3bfe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f3bfe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f3bfa30>]}
[0m20:19:37.387860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:19:37.388967 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:19:37.389268 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:19:37.389461 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:19:37.405363 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:19:37.420637 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:19:37.424021 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:19:37.428058 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:19:37.431829 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:19:37.474317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be42698c-02ea-4766-8ce1-eef158f3e922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f5eaeb0>]}
[0m20:19:37.482666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be42698c-02ea-4766-8ce1-eef158f3e922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f4943d0>]}
[0m20:19:37.483134 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:19:37.483454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be42698c-02ea-4766-8ce1-eef158f3e922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f494460>]}
[0m20:19:37.484207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f494430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f494460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f4941c0>]}
[0m20:19:37.484486 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:19:37.594517 | 1d96bd02-bd02-420b-a85d-0fd3ed61e7f9 ==============================
[0m20:19:37.594630 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:19:37.595991 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:19:37.596421 [debug] [MainThread]: Tracking: tracking
[0m20:19:37.616532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213322b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121332370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121332160>]}
[0m20:19:37.670439 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:19:37.670730 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:19:37.677702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d96bd02-bd02-420b-a85d-0fd3ed61e7f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214cf0d0>]}
[0m20:19:37.685310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d96bd02-bd02-420b-a85d-0fd3ed61e7f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12141a6a0>]}
[0m20:19:37.685711 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:19:37.686018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d96bd02-bd02-420b-a85d-0fd3ed61e7f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12141a730>]}
[0m20:19:37.687480 [info ] [MainThread]: 
[0m20:19:37.688082 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:19:37.689032 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:19:37.701639 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:19:37.701976 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:19:37.702203 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:19:37.702352 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:19:39.197255 [debug] [ThreadPool]: SQL status: OK in 1.49 seconds
[0m20:19:39.209407 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:19:39.209829 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:19:39.210167 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:19:39.763209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d96bd02-bd02-420b-a85d-0fd3ed61e7f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12149ceb0>]}
[0m20:19:39.764473 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:19:39.765250 [info ] [MainThread]: 
[0m20:19:39.771051 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:19:39.771496 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:19:39.772336 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:19:39.772633 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:19:39.772920 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:39.773317 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:19:39.773996 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:19:39.774281 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:19:39.774512 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:19:39.791169 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:19:39.791525 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:19:39.791808 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:19:41.082245 [debug] [Thread-1  ]: SQL status: OK in 1.29 seconds
[0m20:19:41.086434 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:19:41.086671 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:19:41.086817 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:19:41.563546 [debug] [Thread-1  ]: finished collecting timing info
[0m20:19:41.564179 [error] [Thread-1  ]: 1 of 1 ERROR STALE freshness of jaffle_shop.jaffle_shop_orders ................. [[31mERROR STALE[0m in 1.79s]
[0m20:19:41.564846 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:19:41.566068 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:19:41.566325 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:19:41.574743 [info ] [MainThread]: 
[0m20:19:41.575197 [info ] [MainThread]: Done.
[0m20:19:41.575764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121581790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1215813d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121581760>]}
[0m20:19:41.576121 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:21:54.051947 | eeca3c2c-4176-429f-b126-bcdda6879242 ==============================
[0m20:21:54.052014 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:21:54.053759 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:21:54.054305 [debug] [MainThread]: Tracking: tracking
[0m20:21:54.071531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1152dceb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1152dc7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1152dc160>]}
[0m20:21:54.131719 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:21:54.132614 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:21:54.133097 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:21:54.133421 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:21:54.149726 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:21:54.165811 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:21:54.168431 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:21:54.171443 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:21:54.174356 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:21:54.194232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11546e7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11546e5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11546e970>]}
[0m20:21:54.194570 [debug] [MainThread]: Flushing usage events
[0m20:21:55.597022 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid sources config given in models/staging/jaffle_shop/src_jaffle_shop.yml @ sources: {'name': 'jaffle_shop', 'database': None, 'schema': 'default', 'tables': [{'name': 'jaffle_shop_customers'}, {'name': 'jaffle_shop_orders', 'loaded_at_field': 'ORDER_DATE', 'freshness': {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}}}]} - at path ['tables'][1]['freshness']: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas
[0m20:21:55.606543 [error] [MainThread]: jsonschema.exceptions.ValidationError: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas

Failed validating 'oneOf' in schema['properties']['tables']['items']['properties']['freshness']:
    {'default': {'error_after': {'count': None, 'period': None},
                 'filter': None,
                 'warn_after': {'count': None, 'period': None}},
     'oneOf': [{'$ref': '#/definitions/FreshnessThreshold'},
               {'type': 'null'}]}

On instance['tables'][1]['freshness']:
    {'error_after': {'count': 2, 'period': 'year'},
     'warn_after': {'count': 1, 'period': 'year'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 674, in _target_from_dict
    cls.validate(data)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/hologram/__init__.py", line 989, in validate
    raise ValidationError.create_from(error) from error
hologram.ValidationError: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas

Failed validating 'oneOf' in schema['properties']['tables']['items']['properties']['freshness']:
    {'default': {'error_after': {'count': None, 'period': None},
                 'filter': None,
                 'warn_after': {'count': None, 'period': None}},
     'oneOf': [{'$ref': '#/definitions/FreshnessThreshold'},
               {'type': 'null'}]}

On instance['tables'][1]['freshness']:
    {'error_after': {'count': 2, 'period': 'year'},
     'warn_after': {'count': 1, 'period': 'year'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 523, in parse_file
    parser.parse()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 703, in parse
    source = self._target_from_dict(UnparsedSourceDefinition, data)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 678, in _target_from_dict
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid sources config given in models/staging/jaffle_shop/src_jaffle_shop.yml @ sources: {'name': 'jaffle_shop', 'database': None, 'schema': 'default', 'tables': [{'name': 'jaffle_shop_customers'}, {'name': 'jaffle_shop_orders', 'loaded_at_field': 'ORDER_DATE', 'freshness': {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}}}]} - at path ['tables'][1]['freshness']: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas



============================== 2022-10-13 20:21:56.319406 | cf141503-852c-4dcd-8e25-25efbd47dc39 ==============================
[0m20:21:56.319496 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:21:56.320411 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:21:56.320655 [debug] [MainThread]: Tracking: tracking
[0m20:21:56.337945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b1fee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b1f850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b1fac0>]}
[0m20:21:56.394908 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:21:56.396038 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:21:56.396654 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:21:56.396904 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:21:56.414333 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:21:56.428524 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:21:56.432671 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:21:56.436625 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:21:56.443604 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:21:56.468420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123caea60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123cae820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123cae850>]}
[0m20:21:56.468783 [debug] [MainThread]: Flushing usage events
[0m20:21:57.134020 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid sources config given in models/staging/jaffle_shop/src_jaffle_shop.yml @ sources: {'name': 'jaffle_shop', 'database': None, 'schema': 'default', 'tables': [{'name': 'jaffle_shop_customers'}, {'name': 'jaffle_shop_orders', 'loaded_at_field': 'ORDER_DATE', 'freshness': {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}}}]} - at path ['tables'][1]['freshness']: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas
[0m20:21:57.140627 [error] [MainThread]: jsonschema.exceptions.ValidationError: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas

Failed validating 'oneOf' in schema['properties']['tables']['items']['properties']['freshness']:
    {'default': {'error_after': {'count': None, 'period': None},
                 'filter': None,
                 'warn_after': {'count': None, 'period': None}},
     'oneOf': [{'$ref': '#/definitions/FreshnessThreshold'},
               {'type': 'null'}]}

On instance['tables'][1]['freshness']:
    {'error_after': {'count': 2, 'period': 'year'},
     'warn_after': {'count': 1, 'period': 'year'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 674, in _target_from_dict
    cls.validate(data)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/hologram/__init__.py", line 989, in validate
    raise ValidationError.create_from(error) from error
hologram.ValidationError: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas

Failed validating 'oneOf' in schema['properties']['tables']['items']['properties']['freshness']:
    {'default': {'error_after': {'count': None, 'period': None},
                 'filter': None,
                 'warn_after': {'count': None, 'period': None}},
     'oneOf': [{'$ref': '#/definitions/FreshnessThreshold'},
               {'type': 'null'}]}

On instance['tables'][1]['freshness']:
    {'error_after': {'count': 2, 'period': 'year'},
     'warn_after': {'count': 1, 'period': 'year'}}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 523, in parse_file
    parser.parse()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 703, in parse
    source = self._target_from_dict(UnparsedSourceDefinition, data)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 678, in _target_from_dict
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid sources config given in models/staging/jaffle_shop/src_jaffle_shop.yml @ sources: {'name': 'jaffle_shop', 'database': None, 'schema': 'default', 'tables': [{'name': 'jaffle_shop_customers'}, {'name': 'jaffle_shop_orders', 'loaded_at_field': 'ORDER_DATE', 'freshness': {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}}}]} - at path ['tables'][1]['freshness']: {'warn_after': {'count': 1, 'period': 'year'}, 'error_after': {'count': 2, 'period': 'year'}} is not valid under any of the given schemas



============================== 2022-10-13 20:22:17.088315 | 3cf410db-9a0f-4736-ae8a-31fe4674cff9 ==============================
[0m20:22:17.088378 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:22:17.089490 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:22:17.090071 [debug] [MainThread]: Tracking: tracking
[0m20:22:17.109832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f426a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f42a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f42520>]}
[0m20:22:17.162217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:22:17.162912 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:22:17.163252 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:17.163433 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:22:17.176319 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:22:17.190426 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:22:17.193918 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:22:17.197166 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:22:17.201212 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:22:17.230564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cf410db-9a0f-4736-ae8a-31fe4674cff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141790d0>]}
[0m20:22:17.238948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cf410db-9a0f-4736-ae8a-31fe4674cff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f75190>]}
[0m20:22:17.239349 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:22:17.239667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cf410db-9a0f-4736-ae8a-31fe4674cff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f75a60>]}
[0m20:22:17.241700 [info ] [MainThread]: 
[0m20:22:17.242614 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:22:17.244286 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:22:17.256003 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:22:17.256288 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:22:17.256524 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:22:17.256687 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:22:18.918207 [debug] [ThreadPool]: SQL status: OK in 1.66 seconds
[0m20:22:18.925105 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:22:18.925383 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:22:18.925540 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close


============================== 2022-10-13 20:22:19.107749 | 2acc3a27-bbaf-4ac2-ac49-63d9174e03ee ==============================
[0m20:22:19.107819 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:22:19.109391 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:22:19.109861 [debug] [MainThread]: Tracking: tracking
[0m20:22:19.125665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1e0b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1e0df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1e0430>]}
[0m20:22:19.177354 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:22:19.177620 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:22:19.184327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2acc3a27-bbaf-4ac2-ac49-63d9174e03ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d3750d0>]}
[0m20:22:19.192834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2acc3a27-bbaf-4ac2-ac49-63d9174e03ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2c1430>]}
[0m20:22:19.193249 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:22:19.193511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2acc3a27-bbaf-4ac2-ac49-63d9174e03ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2c14c0>]}
[0m20:22:19.194317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2c1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2c14c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d2c1100>]}
[0m20:22:19.194666 [debug] [MainThread]: Flushing usage events
[0m20:22:19.379058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cf410db-9a0f-4736-ae8a-31fe4674cff9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113f10dc0>]}
[0m20:22:19.380308 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:22:19.381012 [info ] [MainThread]: 
[0m20:22:19.389162 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:19.389660 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:22:19.391020 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:22:19.391464 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:19.392074 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:19.392385 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:19.393286 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:22:19.393678 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:22:19.393936 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:22:19.409168 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:22:19.409446 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:22:19.409652 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:22:20.826945 [debug] [Thread-1  ]: SQL status: OK in 1.42 seconds
[0m20:22:20.833321 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:22:20.833798 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:22:20.834136 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:22:21.288824 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:21.290089 [error] [Thread-1  ]: 1 of 1 ERROR STALE freshness of jaffle_shop.jaffle_shop_orders ................. [[31mERROR STALE[0m in 1.90s]
[0m20:22:21.291312 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:21.294341 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:22:21.295150 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:22:21.310804 [info ] [MainThread]: 
[0m20:22:21.311430 [info ] [MainThread]: Done.
[0m20:22:21.312157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141cdbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141cdb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1141cdb80>]}
[0m20:22:21.312543 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:22:34.309651 | 47887e59-9207-4b42-afe6-8d6571c3af0a ==============================
[0m20:22:34.309725 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:22:34.310928 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:22:34.311363 [debug] [MainThread]: Tracking: tracking
[0m20:22:34.328024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e5eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e5c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e5160>]}
[0m20:22:34.386150 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:22:34.386984 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:22:34.387413 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:34.387598 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:22:34.403283 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:22:34.418139 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:22:34.421104 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:22:34.424524 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:22:34.428116 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:22:34.466183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47887e59-9207-4b42-afe6-8d6571c3af0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1188250d0>]}
[0m20:22:34.474033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47887e59-9207-4b42-afe6-8d6571c3af0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185ddc10>]}
[0m20:22:34.474442 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:22:34.474776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47887e59-9207-4b42-afe6-8d6571c3af0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185c8430>]}
[0m20:22:34.476334 [info ] [MainThread]: 
[0m20:22:34.476974 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:22:34.478119 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:22:34.489610 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:22:34.489982 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:22:34.490202 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:22:34.490358 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:22:35.976265 [debug] [ThreadPool]: SQL status: OK in 1.49 seconds
[0m20:22:35.983342 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:22:35.983627 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:22:35.983813 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:22:36.555130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47887e59-9207-4b42-afe6-8d6571c3af0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186ce6d0>]}
[0m20:22:36.555862 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:22:36.556350 [info ] [MainThread]: 
[0m20:22:36.560665 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:36.561015 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:22:36.561672 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:22:36.561870 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:36.562056 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:36.562198 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:36.562555 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:22:36.562720 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:22:36.562855 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:22:36.572088 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:22:36.572333 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:22:36.572496 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:22:36.945407 | 2f41f4b3-7bfb-4242-a647-70b6be6d1b79 ==============================
[0m20:22:36.945509 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:22:36.946874 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:22:36.947364 [debug] [MainThread]: Tracking: tracking
[0m20:22:36.964199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f58fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f58580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f58340>]}
[0m20:22:37.014844 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:22:37.015127 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:22:37.021995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f41f4b3-7bfb-4242-a647-70b6be6d1b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1190ec0d0>]}
[0m20:22:37.029567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f41f4b3-7bfb-4242-a647-70b6be6d1b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11902d400>]}
[0m20:22:37.029923 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:22:37.030208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f41f4b3-7bfb-4242-a647-70b6be6d1b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11902d490>]}
[0m20:22:37.030944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11902d460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11902d490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11902d0a0>]}
[0m20:22:37.031162 [debug] [MainThread]: Flushing usage events
[0m20:22:37.730215 [debug] [Thread-1  ]: SQL status: OK in 1.16 seconds
[0m20:22:37.737058 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:22:37.737600 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:22:37.738372 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:22:38.193336 [debug] [Thread-1  ]: finished collecting timing info
[0m20:22:38.194220 [error] [Thread-1  ]: 1 of 1 ERROR STALE freshness of jaffle_shop.jaffle_shop_orders ................. [[31mERROR STALE[0m in 1.63s]
[0m20:22:38.195032 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:22:38.196865 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:22:38.197196 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:22:38.210032 [info ] [MainThread]: 
[0m20:22:38.210398 [info ] [MainThread]: Done.
[0m20:22:38.210739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118876c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118876bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118876be0>]}
[0m20:22:38.211002 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:23:42.693094 | a5f0d8ca-b3da-44a2-9b14-c1ce49c61b66 ==============================
[0m20:23:42.693148 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:23:42.694136 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:23:42.694468 [debug] [MainThread]: Tracking: tracking
[0m20:23:42.712932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118205fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182052b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118205ca0>]}
[0m20:23:42.764193 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:23:42.764847 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:23:42.765177 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:23:42.765354 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:23:42.778595 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:23:42.793975 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:23:42.797567 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:23:42.801494 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:23:42.804881 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:23:42.836512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a5f0d8ca-b3da-44a2-9b14-c1ce49c61b66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118445e80>]}
[0m20:23:42.845249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a5f0d8ca-b3da-44a2-9b14-c1ce49c61b66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec3a0>]}
[0m20:23:42.845684 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:23:42.845951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a5f0d8ca-b3da-44a2-9b14-c1ce49c61b66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec430>]}
[0m20:23:42.846640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1182ec190>]}
[0m20:23:42.846869 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:23:46.240020 | 8685ec3e-44cf-4209-9bfc-108acb7bf989 ==============================
[0m20:23:46.240127 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:23:46.241785 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:23:46.242377 [debug] [MainThread]: Tracking: tracking
[0m20:23:46.261916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11631f040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11631fca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11631fac0>]}
[0m20:23:46.315992 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:23:46.316258 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:23:46.322994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8685ec3e-44cf-4209-9bfc-108acb7bf989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1164b20d0>]}
[0m20:23:46.330921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8685ec3e-44cf-4209-9bfc-108acb7bf989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116400430>]}
[0m20:23:46.331472 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:23:46.331839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8685ec3e-44cf-4209-9bfc-108acb7bf989', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1164004c0>]}
[0m20:23:46.332737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116400490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1164004c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116400100>]}
[0m20:23:46.332998 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:23:47.662388 | 07be4d28-2334-4652-ab7f-7a4113102f5e ==============================
[0m20:23:47.662498 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:23:47.663577 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:23:47.663881 [debug] [MainThread]: Tracking: tracking
[0m20:23:47.679990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11caa2eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11caa2310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11caa2160>]}
[0m20:23:47.738407 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:23:47.738750 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:23:47.746779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07be4d28-2334-4652-ab7f-7a4113102f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cc3f0d0>]}
[0m20:23:47.755367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07be4d28-2334-4652-ab7f-7a4113102f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cb8c490>]}
[0m20:23:47.755746 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:23:47.756076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07be4d28-2334-4652-ab7f-7a4113102f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ca991f0>]}
[0m20:23:47.757498 [info ] [MainThread]: 
[0m20:23:47.758047 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:23:47.759079 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:23:47.770053 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:23:47.770458 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:23:47.770711 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:23:47.770851 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:49.519283 [debug] [ThreadPool]: SQL status: OK in 1.75 seconds
[0m20:23:49.528949 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:23:49.529305 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:23:49.529552 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:23:50.105349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07be4d28-2334-4652-ab7f-7a4113102f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11cc0b850>]}
[0m20:23:50.106571 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:23:50.107197 [info ] [MainThread]: 
[0m20:23:50.114060 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:23:50.114524 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:23:50.115351 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:23:50.115773 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:23:50.116107 [debug] [Thread-1  ]: finished collecting timing info
[0m20:23:50.116377 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:23:50.117004 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:23:50.117285 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:23:50.117516 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:23:50.132223 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:23:50.132643 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:23:50.132929 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:23:51.343791 [debug] [Thread-1  ]: SQL status: OK in 1.21 seconds
[0m20:23:51.349994 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:23:51.350506 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:23:51.350902 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:23:51.797084 [debug] [Thread-1  ]: finished collecting timing info
[0m20:23:51.798127 [error] [Thread-1  ]: 1 of 1 ERROR STALE freshness of jaffle_shop.jaffle_shop_orders ................. [[31mERROR STALE[0m in 1.68s]
[0m20:23:51.799450 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:23:51.802186 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:23:51.802759 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:23:51.816634 [info ] [MainThread]: 
[0m20:23:51.817124 [info ] [MainThread]: Done.
[0m20:23:51.817590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ccf15e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ccf16a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ccf17c0>]}
[0m20:23:51.817960 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:24:08.978798 | be049967-5c2e-431f-8174-496818abfa45 ==============================
[0m20:24:08.978888 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:08.980250 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:24:08.980596 [debug] [MainThread]: Tracking: tracking
[0m20:24:09.005024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123531f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123531220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235310d0>]}
[0m20:24:09.084658 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:24:09.095167 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:24:09.095870 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:09.096263 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:24:09.115481 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:24:09.128920 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:24:09.131818 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:24:09.136437 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:24:09.142247 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql


============================== 2022-10-13 20:24:09.175050 | 60634571-820c-4e9a-abae-3ca5ae751b60 ==============================
[0m20:24:09.175157 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:09.177335 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:24:09.178149 [debug] [MainThread]: Tracking: tracking
[0m20:24:09.183508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be049967-5c2e-431f-8174-496818abfa45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1237770a0>]}
[0m20:24:09.195627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be049967-5c2e-431f-8174-496818abfa45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123618370>]}
[0m20:24:09.196287 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:09.196977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be049967-5c2e-431f-8174-496818abfa45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123618400>]}
[0m20:24:09.198367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236183d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123618400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123618160>]}
[0m20:24:09.198938 [debug] [MainThread]: Flushing usage events
[0m20:24:09.201099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b32eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b32310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b32160>]}
[0m20:24:09.263255 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:24:09.263548 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:24:09.270569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '60634571-820c-4e9a-abae-3ca5ae751b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117ccf0d0>]}
[0m20:24:09.278363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '60634571-820c-4e9a-abae-3ca5ae751b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c1a3a0>]}
[0m20:24:09.278822 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:09.279185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60634571-820c-4e9a-abae-3ca5ae751b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b291f0>]}
[0m20:24:09.280850 [info ] [MainThread]: 
[0m20:24:09.281514 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:24:09.282977 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:24:09.297376 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:24:09.297759 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:24:09.298341 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:24:09.298567 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:24:11.358725 [debug] [ThreadPool]: SQL status: OK in 2.06 seconds
[0m20:24:11.379311 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:24:11.390916 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:24:11.391344 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close


============================== 2022-10-13 20:24:11.741141 | 3fca9ef6-e6e6-4255-9afe-8bbb333c78de ==============================
[0m20:24:11.741212 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:11.743334 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:24:11.743899 [debug] [MainThread]: Tracking: tracking
[0m20:24:11.763618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118863f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118863220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1188630d0>]}
[0m20:24:11.823278 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:24:11.823614 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:24:11.830445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3fca9ef6-e6e6-4255-9afe-8bbb333c78de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a090d0>]}
[0m20:24:11.839820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3fca9ef6-e6e6-4255-9afe-8bbb333c78de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11894a370>]}
[0m20:24:11.840300 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:11.840620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fca9ef6-e6e6-4255-9afe-8bbb333c78de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11894a400>]}
[0m20:24:11.841425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11894a3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11894a400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11894a160>]}
[0m20:24:11.841667 [debug] [MainThread]: Flushing usage events
[0m20:24:11.867403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60634571-820c-4e9a-abae-3ca5ae751b60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117c9b850>]}
[0m20:24:11.868112 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:24:11.868414 [info ] [MainThread]: 
[0m20:24:11.872138 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:11.872425 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:24:11.872928 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:11.873090 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:11.873251 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:11.873387 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:11.873763 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:11.873919 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:24:11.874041 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:24:11.885108 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:11.885488 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:24:11.885781 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:24:13.042190 [debug] [Thread-1  ]: SQL status: OK in 1.16 seconds
[0m20:24:13.049883 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:24:13.050474 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:24:13.050826 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:24:13.498867 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:13.500099 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.63s]
[0m20:24:13.501246 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:13.504182 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:24:13.504564 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:24:13.516537 [info ] [MainThread]: Done.
[0m20:24:13.517039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117d7e160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117d7e0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117d80940>]}
[0m20:24:13.517351 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:24:32.652763 | 920021b2-b93e-44de-9cb0-054e398215a0 ==============================
[0m20:24:32.652844 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:32.653971 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:24:32.654255 [debug] [MainThread]: Tracking: tracking
[0m20:24:32.674710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11491be20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11491be80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11491b340>]}
[0m20:24:32.733591 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:24:32.734278 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:24:32.734650 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:32.734831 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:24:32.748566 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:24:32.767058 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:24:32.771613 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:24:32.775480 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:24:32.778577 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:24:32.807899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '920021b2-b93e-44de-9cb0-054e398215a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b49e80>]}
[0m20:24:32.817979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '920021b2-b93e-44de-9cb0-054e398215a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149e5400>]}
[0m20:24:32.818366 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:32.818674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '920021b2-b93e-44de-9cb0-054e398215a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149e5490>]}
[0m20:24:32.819444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149e5460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149e5490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149e50a0>]}
[0m20:24:32.819726 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:24:32.887882 | e76635cf-d2f6-4205-8a5d-350ddecaf6b0 ==============================
[0m20:24:32.887955 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:32.888963 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:24:32.889346 [debug] [MainThread]: Tracking: tracking
[0m20:24:32.908253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147faeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147fac40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147fa160>]}
[0m20:24:32.978497 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:24:32.978811 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:24:32.985847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e76635cf-d2f6-4205-8a5d-350ddecaf6b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149970d0>]}
[0m20:24:32.994135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e76635cf-d2f6-4205-8a5d-350ddecaf6b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148e2490>]}
[0m20:24:32.994545 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:32.994899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e76635cf-d2f6-4205-8a5d-350ddecaf6b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1147f11f0>]}
[0m20:24:32.996381 [info ] [MainThread]: 
[0m20:24:32.996951 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:24:32.998043 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:24:33.013324 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:24:33.013602 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:24:33.013760 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:24:33.013897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:24:34.266009 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m20:24:34.273487 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:24:34.273734 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:24:34.273934 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:24:34.739834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e76635cf-d2f6-4205-8a5d-350ddecaf6b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114963850>]}
[0m20:24:34.740428 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:24:34.740737 [info ] [MainThread]: 
[0m20:24:34.746011 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:34.746283 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:24:34.746783 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:34.746962 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:34.747127 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:34.747267 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:34.747626 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:34.747777 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:24:34.747901 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:24:34.761058 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:34.761329 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:24:34.761515 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:24:35.711709 | e7ca7b09-ce4c-4980-8372-cfcf087f54d4 ==============================
[0m20:24:35.711758 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:35.713227 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:24:35.713628 [debug] [MainThread]: Tracking: tracking
[0m20:24:35.730821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1189bae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1189bae80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1189ba340>]}
[0m20:24:35.781280 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:24:35.781543 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:24:35.788289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7ca7b09-ce4c-4980-8372-cfcf087f54d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118b4e0d0>]}
[0m20:24:35.795834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7ca7b09-ce4c-4980-8372-cfcf087f54d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a8f400>]}
[0m20:24:35.796254 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:35.796518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7ca7b09-ce4c-4980-8372-cfcf087f54d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a8f490>]}
[0m20:24:35.797251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a8f460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a8f490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a8f0a0>]}
[0m20:24:35.797476 [debug] [MainThread]: Flushing usage events
[0m20:24:35.984891 [debug] [Thread-1  ]: SQL status: OK in 1.22 seconds
[0m20:24:35.987790 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:24:35.988071 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:24:35.988218 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:24:36.522618 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:36.523162 [error] [Thread-1  ]: 1 of 1 ERROR STALE freshness of jaffle_shop.jaffle_shop_orders ................. [[31mERROR STALE[0m in 1.78s]
[0m20:24:36.523733 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:36.525302 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:24:36.525548 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:24:36.533527 [info ] [MainThread]: 
[0m20:24:36.533945 [info ] [MainThread]: Done.
[0m20:24:36.534259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148e2250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148e20a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1148e20d0>]}
[0m20:24:36.534528 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:24:46.276015 | e2ee1d84-d6e5-4ee3-9445-1d82a2ddcfc5 ==============================
[0m20:24:46.276077 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:46.277664 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:24:46.278203 [debug] [MainThread]: Tracking: tracking
[0m20:24:46.294880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d76e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d76c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d76130>]}
[0m20:24:46.354427 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:24:46.355231 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:24:46.355872 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:46.356108 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:24:46.371663 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:24:46.386441 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:24:46.389303 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:24:46.392219 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:24:46.395070 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:24:46.426326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e2ee1d84-d6e5-4ee3-9445-1d82a2ddcfc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f700d0>]}
[0m20:24:46.434438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e2ee1d84-d6e5-4ee3-9445-1d82a2ddcfc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dad970>]}
[0m20:24:46.434893 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:46.435223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2ee1d84-d6e5-4ee3-9445-1d82a2ddcfc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dad9d0>]}
[0m20:24:46.436727 [info ] [MainThread]: 
[0m20:24:46.437328 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:24:46.438310 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:24:46.447462 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:24:46.447812 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:24:46.448024 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:24:46.448196 [debug] [ThreadPool]: Opening a new connection, currently in state init


============================== 2022-10-13 20:24:47.293835 | 67ef0e2e-a70d-4a81-badd-a9c94d743788 ==============================
[0m20:24:47.293913 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:24:47.294794 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:24:47.295036 [debug] [MainThread]: Tracking: tracking
[0m20:24:47.310581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115e73e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115e73040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115e739a0>]}
[0m20:24:47.371101 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:24:47.371462 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:24:47.380046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '67ef0e2e-a70d-4a81-badd-a9c94d743788', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1160070d0>]}
[0m20:24:47.388192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '67ef0e2e-a70d-4a81-badd-a9c94d743788', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f47400>]}
[0m20:24:47.388590 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:24:47.388868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '67ef0e2e-a70d-4a81-badd-a9c94d743788', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f47490>]}
[0m20:24:47.389642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f47460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f47490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f470a0>]}
[0m20:24:47.389884 [debug] [MainThread]: Flushing usage events
[0m20:24:47.748677 [debug] [ThreadPool]: SQL status: OK in 1.3 seconds
[0m20:24:47.757546 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:24:47.757838 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:24:47.758035 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:24:48.214489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2ee1d84-d6e5-4ee3-9445-1d82a2ddcfc5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d6cc70>]}
[0m20:24:48.215102 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:24:48.215551 [info ] [MainThread]: 
[0m20:24:48.221104 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:48.221373 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:24:48.221902 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:48.222145 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:48.222310 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:48.222445 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:48.222781 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:48.222928 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:24:48.223051 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:24:48.231694 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:24:48.231922 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:24:48.232069 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:24:49.363751 [debug] [Thread-1  ]: SQL status: OK in 1.13 seconds
[0m20:24:49.371585 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:24:49.372072 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:24:49.372445 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:24:49.818662 [debug] [Thread-1  ]: finished collecting timing info
[0m20:24:49.819937 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.60s]
[0m20:24:49.821089 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:24:49.824216 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:24:49.824782 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:24:49.839117 [info ] [MainThread]: Done.
[0m20:24:49.839820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d76160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a005bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a005940>]}
[0m20:24:49.840239 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:27:30.312495 | 742a1b81-a9ed-4f36-9f6c-049e04012a6d ==============================
[0m20:27:30.312596 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:27:30.314244 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:27:30.314556 [debug] [MainThread]: Tracking: tracking
[0m20:27:30.333948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12049e6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12049ea30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12049e520>]}
[0m20:27:30.393629 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:27:30.394302 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:27:30.394631 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:27:30.394782 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:27:30.409857 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:27:30.426068 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:27:30.429828 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:27:30.434171 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:27:30.437094 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:27:30.470455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '742a1b81-a9ed-4f36-9f6c-049e04012a6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1206d60d0>]}
[0m20:27:30.480223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '742a1b81-a9ed-4f36-9f6c-049e04012a6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1205825e0>]}
[0m20:27:30.480628 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:27:30.480956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '742a1b81-a9ed-4f36-9f6c-049e04012a6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120582670>]}
[0m20:27:30.482435 [info ] [MainThread]: 
[0m20:27:30.483167 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:27:30.484276 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:27:30.494908 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:27:30.495188 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:27:30.495343 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:27:30.495474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:27:32.301529 [debug] [ThreadPool]: SQL status: OK in 1.81 seconds
[0m20:27:32.309810 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:27:32.310046 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:27:32.310217 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close


============================== 2022-10-13 20:27:32.336194 | bb8a0d37-88c9-4cae-9e75-4f889538770f ==============================
[0m20:27:32.336248 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:27:32.337553 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:27:32.337812 [debug] [MainThread]: Tracking: tracking
[0m20:27:32.353550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c5f040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c5fdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c5fac0>]}
[0m20:27:32.406531 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:27:32.406818 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:27:32.414460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bb8a0d37-88c9-4cae-9e75-4f889538770f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120df20d0>]}
[0m20:27:32.423319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bb8a0d37-88c9-4cae-9e75-4f889538770f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d3f430>]}
[0m20:27:32.423765 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:27:32.424073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb8a0d37-88c9-4cae-9e75-4f889538770f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d3f4c0>]}
[0m20:27:32.424916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d3f490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d3f4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d3f100>]}
[0m20:27:32.425143 [debug] [MainThread]: Flushing usage events
[0m20:27:32.776413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '742a1b81-a9ed-4f36-9f6c-049e04012a6d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12049e970>]}
[0m20:27:32.778196 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:27:32.779197 [info ] [MainThread]: 
[0m20:27:32.787283 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:27:32.787732 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:27:32.788543 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:27:32.788828 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:27:32.789121 [debug] [Thread-1  ]: finished collecting timing info
[0m20:27:32.789361 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:27:32.789952 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:27:32.790221 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:27:32.790445 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:27:32.803769 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:27:32.804078 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:27:32.804535 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:27:34.095251 [debug] [Thread-1  ]: SQL status: OK in 1.29 seconds
[0m20:27:34.100849 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:27:34.101324 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:27:34.101713 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:27:34.635208 [debug] [Thread-1  ]: finished collecting timing info
[0m20:27:34.636351 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.85s]
[0m20:27:34.637365 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:27:34.639521 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:27:34.639984 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:27:34.652387 [info ] [MainThread]: Done.
[0m20:27:34.652988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120491dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120491580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120729a30>]}
[0m20:27:34.653381 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:28:12.609888 | 4a869567-5119-46f5-8488-753e7b894899 ==============================
[0m20:28:12.609941 [info ] [MainThread]: Running with dbt=1.2.1


============================== 2022-10-13 20:28:12.609963 | d968fd0f-1952-4fde-97e2-1e7c81e33e92 ==============================
[0m20:28:12.610145 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:28:12.611571 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:28:12.611757 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:28:12.612137 [debug] [MainThread]: Tracking: tracking
[0m20:28:12.612143 [debug] [MainThread]: Tracking: tracking
[0m20:28:12.641508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1154ddeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1154dd310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1154dd160>]}
[0m20:28:12.640870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122a8d790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122a8d3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122a8d2b0>]}
[0m20:28:12.750407 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:28:12.750480 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:28:12.751177 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:28:12.751192 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:28:12.751520 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:12.751541 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:12.751701 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:28:12.751731 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:28:12.767185 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:28:12.768205 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:28:12.782799 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:28:12.783930 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:28:12.786469 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:28:12.787111 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:28:12.789560 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:28:12.790149 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:28:12.792566 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:28:12.792907 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:28:12.824286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd968fd0f-1952-4fde-97e2-1e7c81e33e92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1156d60d0>]}
[0m20:28:12.824365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a869567-5119-46f5-8488-753e7b894899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122cd30a0>]}
[0m20:28:12.832972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a869567-5119-46f5-8488-753e7b894899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b74340>]}
[0m20:28:12.833382 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:28:12.833499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd968fd0f-1952-4fde-97e2-1e7c81e33e92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1154d4430>]}
[0m20:28:12.833694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a869567-5119-46f5-8488-753e7b894899', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b743d0>]}
[0m20:28:12.833970 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:28:12.834317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd968fd0f-1952-4fde-97e2-1e7c81e33e92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1154bfa90>]}
[0m20:28:12.834504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b743a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b743d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b74130>]}
[0m20:28:12.834883 [debug] [MainThread]: Flushing usage events
[0m20:28:12.835932 [info ] [MainThread]: 
[0m20:28:12.836635 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:28:12.837734 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:28:12.850866 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:12.851292 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:28:12.851495 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:28:12.851748 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:28:14.561302 [debug] [ThreadPool]: SQL status: OK in 1.71 seconds
[0m20:28:14.586841 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:28:14.587298 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:28:14.587615 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:28:15.045251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd968fd0f-1952-4fde-97e2-1e7c81e33e92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1155c56d0>]}
[0m20:28:15.046511 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:28:15.047247 [info ] [MainThread]: 
[0m20:28:15.057116 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:15.057744 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:28:15.059301 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:28:15.059938 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:15.060475 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:15.060889 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:15.061967 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:28:15.062347 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:15.062656 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:28:15.080642 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:28:15.080962 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:28:15.081173 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:16.397485 [debug] [Thread-1  ]: SQL status: OK in 1.32 seconds
[0m20:28:16.405732 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:28:16.406261 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:28:16.406670 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:28:16.857954 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:16.859592 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.80s]
[0m20:28:16.861372 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:16.864591 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:28:16.865135 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:28:16.879865 [info ] [MainThread]: Done.
[0m20:28:16.880488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11576cbe0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11576c970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11576cb50>]}
[0m20:28:16.880897 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:28:25.577510 | 415974ed-28b9-4444-b4df-d075228a9bc8 ==============================
[0m20:28:25.577567 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:28:25.578825 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:28:25.579374 [debug] [MainThread]: Tracking: tracking
[0m20:28:25.595751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9b3fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9b3f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d9b3430>]}
[0m20:28:25.651138 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:28:25.651854 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:28:25.652192 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:25.652360 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:28:25.668597 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:28:25.685554 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:28:25.688845 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:28:25.691871 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:28:25.694757 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:28:25.724725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '415974ed-28b9-4444-b4df-d075228a9bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dbe2e50>]}
[0m20:28:25.733488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '415974ed-28b9-4444-b4df-d075228a9bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da893d0>]}
[0m20:28:25.733941 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:28:25.734212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '415974ed-28b9-4444-b4df-d075228a9bc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da89460>]}
[0m20:28:25.734935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da89430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da89460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11da891c0>]}
[0m20:28:25.735182 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:28:27.118537 | fddff5a1-2053-48ee-ba11-14413606fe24 ==============================
[0m20:28:27.118621 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:28:27.120213 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:28:27.120778 [debug] [MainThread]: Tracking: tracking
[0m20:28:27.139843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b7c490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b7ca30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120b7c520>]}
[0m20:28:27.192535 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:28:27.192799 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:28:27.199808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fddff5a1-2053-48ee-ba11-14413606fe24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120d130d0>]}
[0m20:28:27.207176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fddff5a1-2053-48ee-ba11-14413606fe24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c5e5b0>]}
[0m20:28:27.207526 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:28:27.207810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fddff5a1-2053-48ee-ba11-14413606fe24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c5e640>]}
[0m20:28:27.209283 [info ] [MainThread]: 
[0m20:28:27.209895 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:28:27.210899 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:28:27.221351 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:27.221647 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:28:27.221844 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:28:27.222082 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:28:28.473603 [debug] [ThreadPool]: SQL status: OK in 1.25 seconds
[0m20:28:28.485526 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:28:28.485830 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:28:28.486042 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:28:28.942394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fddff5a1-2053-48ee-ba11-14413606fe24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ce0fa0>]}
[0m20:28:28.943483 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:28:28.944057 [info ] [MainThread]: 
[0m20:28:28.951734 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:28.952311 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:28:28.953324 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:28:28.953679 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:28.953984 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:28.954244 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:28.954896 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:28:28.955394 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:28:28.955678 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:28:28.977505 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:28:28.977747 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:28:28.977920 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:28:30.213436 [debug] [Thread-1  ]: SQL status: OK in 1.24 seconds
[0m20:28:30.238527 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:28:30.239040 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:28:30.239415 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:28:30.719868 [debug] [Thread-1  ]: finished collecting timing info
[0m20:28:30.721077 [info ] [Thread-1  ]: 1 of 1 PASS freshness of jaffle_shop.jaffle_shop_orders ........................ [[32mPASS[0m in 1.77s]
[0m20:28:30.722374 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:28:30.725436 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:28:30.725981 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:28:30.740224 [info ] [MainThread]: Done.
[0m20:28:30.740927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120dc20a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120dc2fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120dc2e80>]}
[0m20:28:30.741408 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:29:20.655081 | 65763f4a-55ed-40e4-96e9-36e8880c99a9 ==============================
[0m20:29:20.655133 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:29:20.656178 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:29:20.656483 [debug] [MainThread]: Tracking: tracking
[0m20:29:20.676632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fea6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120feaa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fea520>]}
[0m20:29:20.760476 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:29:20.761260 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:29:20.761696 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:20.761920 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:29:20.777597 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:29:20.792838 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:29:20.796170 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:29:20.799121 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:29:20.801931 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:29:20.836283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65763f4a-55ed-40e4-96e9-36e8880c99a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1212220d0>]}
[0m20:29:20.844013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65763f4a-55ed-40e4-96e9-36e8880c99a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12101d190>]}
[0m20:29:20.844386 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:29:20.844700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65763f4a-55ed-40e4-96e9-36e8880c99a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12101da60>]}
[0m20:29:20.846188 [info ] [MainThread]: 
[0m20:29:20.846744 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:29:20.847779 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:29:20.857939 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:20.858430 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:29:20.858660 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:29:20.858848 [debug] [ThreadPool]: Opening a new connection, currently in state init


============================== 2022-10-13 20:29:21.536914 | 3bc723aa-9639-41b0-928c-1e98f3a0c860 ==============================
[0m20:29:21.536978 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:29:21.537845 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:29:21.538124 [debug] [MainThread]: Tracking: tracking
[0m20:29:21.554743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a384040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a384820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a384af0>]}
[0m20:29:21.606732 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:21.606999 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:21.614680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3bc723aa-9639-41b0-928c-1e98f3a0c860', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a5170d0>]}
[0m20:29:21.623439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3bc723aa-9639-41b0-928c-1e98f3a0c860', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a464460>]}
[0m20:29:21.623914 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:29:21.624265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3bc723aa-9639-41b0-928c-1e98f3a0c860', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4644f0>]}
[0m20:29:21.625082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4644c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a464070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a464280>]}
[0m20:29:21.625342 [debug] [MainThread]: Flushing usage events
[0m20:29:22.297430 [debug] [ThreadPool]: SQL status: OK in 1.44 seconds
[0m20:29:22.323579 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:29:22.323981 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:29:22.324284 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:29:22.783849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65763f4a-55ed-40e4-96e9-36e8880c99a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fb8d90>]}
[0m20:29:22.785323 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:29:22.786080 [info ] [MainThread]: 
[0m20:29:22.793804 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:22.794261 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:29:22.795140 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:22.795457 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:22.795762 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:22.796015 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:22.796659 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:22.796939 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:22.797169 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:29:22.810400 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:22.810708 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:29:22.810934 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:29:24.042944 [debug] [Thread-1  ]: SQL status: OK in 1.23 seconds
[0m20:29:24.049742 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:29:24.050193 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:24.050509 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:29:24.498047 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:24.498774 [info ] [Thread-1  ]: 1 of 1 PASS freshness of jaffle_shop.jaffle_shop_orders ........................ [[32mPASS[0m in 1.70s]
[0m20:29:24.499459 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:24.501205 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:29:24.501529 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:29:24.512672 [info ] [MainThread]: Done.
[0m20:29:24.513239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fdddc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121276910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121276b80>]}
[0m20:29:24.513569 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:29:39.488745 | fe5842d4-a2b3-4d91-85d6-8bf4da34ab40 ==============================
[0m20:29:39.488846 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:29:39.490592 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:29:39.492070 [debug] [MainThread]: Tracking: tracking
[0m20:29:39.529543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116318eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116318c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116318160>]}
[0m20:29:39.621894 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:29:39.623316 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:29:39.624132 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:39.624508 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:29:39.648096 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:29:39.674575 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:29:39.681132 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:29:39.686307 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:29:39.690121 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:29:39.735005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe5842d4-a2b3-4d91-85d6-8bf4da34ab40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1165530d0>]}
[0m20:29:39.744762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe5842d4-a2b3-4d91-85d6-8bf4da34ab40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116350970>]}
[0m20:29:39.745258 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:29:39.745744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe5842d4-a2b3-4d91-85d6-8bf4da34ab40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163509d0>]}
[0m20:29:39.747578 [info ] [MainThread]: 
[0m20:29:39.748416 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:29:39.749621 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:29:39.764440 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:39.764857 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:29:39.765207 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:29:39.765480 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:41.198278 [debug] [ThreadPool]: SQL status: OK in 1.43 seconds
[0m20:29:41.206503 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:29:41.206799 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:29:41.206955 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:29:41.750329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe5842d4-a2b3-4d91-85d6-8bf4da34ab40', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11630fc70>]}
[0m20:29:41.751003 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:29:41.751423 [info ] [MainThread]: 
[0m20:29:41.756339 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:41.756641 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:29:41.757283 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:41.757519 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:41.757726 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:41.757943 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:41.758455 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:41.758760 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:41.759007 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:29:41.771750 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:41.772137 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:29:41.772410 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:29:42.310322 | 1892069f-9291-41da-8d11-24b3a874fead ==============================
[0m20:29:42.310412 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:29:42.311323 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:29:42.311573 [debug] [MainThread]: Tracking: tracking
[0m20:29:42.333629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c53d040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c53d850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c53d550>]}
[0m20:29:42.385976 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:42.386330 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:42.393618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1892069f-9291-41da-8d11-24b3a874fead', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c6d00d0>]}
[0m20:29:42.401360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1892069f-9291-41da-8d11-24b3a874fead', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c61d460>]}
[0m20:29:42.401754 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:29:42.402013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1892069f-9291-41da-8d11-24b3a874fead', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c61d4f0>]}
[0m20:29:42.402726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c61d4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c61d070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c61d190>]}
[0m20:29:42.402942 [debug] [MainThread]: Flushing usage events
[0m20:29:43.021254 [debug] [Thread-1  ]: SQL status: OK in 1.25 seconds
[0m20:29:43.027675 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:29:43.028188 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:43.028552 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:29:43.464680 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:43.465603 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.71s]
[0m20:29:43.467033 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:43.469068 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:29:43.469507 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:29:43.481974 [info ] [MainThread]: Done.
[0m20:29:43.482950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116318d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1165a7bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1165a7940>]}
[0m20:29:43.483367 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:29:49.767916 | 32e1f584-fad3-4b45-b5a9-2eb8535c9ee8 ==============================
[0m20:29:49.768026 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:29:49.769592 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:29:49.770016 [debug] [MainThread]: Tracking: tracking
[0m20:29:49.795421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e6c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e6c370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e6c160>]}
[0m20:29:49.853337 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:29:49.854020 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:29:49.854512 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:49.854777 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:29:49.871844 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:29:49.888306 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:29:49.891318 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:29:49.894379 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:29:49.897095 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:29:49.929225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '32e1f584-fad3-4b45-b5a9-2eb8535c9ee8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1190650d0>]}
[0m20:29:49.938415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '32e1f584-fad3-4b45-b5a9-2eb8535c9ee8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118f54730>]}
[0m20:29:49.938834 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:29:49.939157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32e1f584-fad3-4b45-b5a9-2eb8535c9ee8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e4e1f0>]}
[0m20:29:49.940474 [info ] [MainThread]: 
[0m20:29:49.941134 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:29:49.942225 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:29:49.952711 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:49.953030 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:29:49.953215 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:29:49.953434 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:29:51.153097 [debug] [ThreadPool]: SQL status: OK in 1.2 seconds
[0m20:29:51.161575 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:29:51.161830 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:29:51.161988 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:29:51.626540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '32e1f584-fad3-4b45-b5a9-2eb8535c9ee8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e63160>]}
[0m20:29:51.627367 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:29:51.627857 [info ] [MainThread]: 
[0m20:29:51.633294 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:51.633671 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:29:51.634370 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:51.634634 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:51.634907 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:51.635083 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:51.635472 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:51.635679 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:29:51.635870 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:29:51.644553 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:29:51.644798 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:29:51.644944 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:29:51.915189 | 49a908c1-790f-4db9-aa9b-da0b8c08be92 ==============================
[0m20:29:51.915256 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:29:51.916171 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:29:51.916456 [debug] [MainThread]: Tracking: tracking
[0m20:29:51.933721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163c2e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163c2b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163c2430>]}
[0m20:29:51.987677 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:51.987955 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:51.994829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '49a908c1-790f-4db9-aa9b-da0b8c08be92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1165570d0>]}
[0m20:29:52.002690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '49a908c1-790f-4db9-aa9b-da0b8c08be92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116498400>]}
[0m20:29:52.003041 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:29:52.003314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49a908c1-790f-4db9-aa9b-da0b8c08be92', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116498490>]}
[0m20:29:52.004023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116498460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116498490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1164980a0>]}
[0m20:29:52.004253 [debug] [MainThread]: Flushing usage events
[0m20:29:52.783860 [debug] [Thread-1  ]: SQL status: OK in 1.14 seconds
[0m20:29:52.787459 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:29:52.787737 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:29:52.787909 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:29:53.228732 [debug] [Thread-1  ]: finished collecting timing info
[0m20:29:53.229761 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.60s]
[0m20:29:53.230962 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:29:53.233337 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:29:53.233750 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:29:53.245786 [info ] [MainThread]: Done.
[0m20:29:53.246431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11911ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11911a9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11911ab80>]}
[0m20:29:53.246885 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:30:02.612885 | b84d51aa-8013-431b-9266-06b79f6e5369 ==============================
[0m20:30:02.613032 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:02.615221 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:30:02.615792 [debug] [MainThread]: Tracking: tracking
[0m20:30:02.636743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116816eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116816310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116816160>]}
[0m20:30:02.695690 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:30:02.696343 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:30:02.696681 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:02.696842 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:30:02.710551 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:30:02.731185 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:30:02.734644 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:30:02.737526 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:30:02.740658 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:30:02.772329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b84d51aa-8013-431b-9266-06b79f6e5369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116a520d0>]}
[0m20:30:02.783314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b84d51aa-8013-431b-9266-06b79f6e5369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11680d430>]}
[0m20:30:02.783781 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:02.784161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b84d51aa-8013-431b-9266-06b79f6e5369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1167f8160>]}
[0m20:30:02.785539 [info ] [MainThread]: 
[0m20:30:02.786130 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:30:02.787123 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:02.796678 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:02.796974 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:02.797133 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:30:02.797268 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:03.942999 [debug] [ThreadPool]: SQL status: OK in 1.15 seconds
[0m20:30:03.949716 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:30:03.949992 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:30:03.950168 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:30:04.402065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b84d51aa-8013-431b-9266-06b79f6e5369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169006d0>]}
[0m20:30:04.402629 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:04.402919 [info ] [MainThread]: 
[0m20:30:04.407253 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:04.407528 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:30:04.408246 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:04.408631 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:04.408950 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:04.409188 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:04.409861 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:04.410209 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:04.410439 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:30:04.425619 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:04.425959 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:30:04.426322 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:30:05.330326 | 05b94ece-ceb6-4527-938a-e050cf2193ab ==============================
[0m20:30:05.330382 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:05.331338 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:30:05.331568 [debug] [MainThread]: Tracking: tracking
[0m20:30:05.347699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e457040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e457df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e457ac0>]}
[0m20:30:05.397323 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:05.397620 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:05.404120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '05b94ece-ceb6-4527-938a-e050cf2193ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e5eb0d0>]}
[0m20:30:05.411988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '05b94ece-ceb6-4527-938a-e050cf2193ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e538430>]}
[0m20:30:05.412344 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:05.412626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '05b94ece-ceb6-4527-938a-e050cf2193ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e5384c0>]}
[0m20:30:05.413375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e538490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e5384c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e538100>]}
[0m20:30:05.413637 [debug] [MainThread]: Flushing usage events
[0m20:30:05.580511 [debug] [Thread-1  ]: SQL status: OK in 1.15 seconds
[0m20:30:05.585188 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:30:05.585549 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:05.585805 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:30:06.040111 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:06.040816 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.63s]
[0m20:30:06.041416 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:06.042608 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:30:06.042867 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:30:06.051088 [info ] [MainThread]: Done.
[0m20:30:06.051616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116aa3be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116aa3970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116aa3b50>]}
[0m20:30:06.051915 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:30:11.664586 | 2e84823c-1484-4d39-910d-f46f8a70bd8c ==============================
[0m20:30:11.664735 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:11.666150 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:30:11.666684 [debug] [MainThread]: Tracking: tracking
[0m20:30:11.685798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc406a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc40a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc40520>]}
[0m20:30:11.741514 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:30:11.742145 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:30:11.742471 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:11.742624 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:30:11.755995 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:30:11.777560 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:30:11.781851 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:30:11.784795 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:30:11.787596 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:30:11.816449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2e84823c-1484-4d39-910d-f46f8a70bd8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11de750d0>]}
[0m20:30:11.825140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2e84823c-1484-4d39-910d-f46f8a70bd8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc71190>]}
[0m20:30:11.825511 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:11.825820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e84823c-1484-4d39-910d-f46f8a70bd8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc71a60>]}
[0m20:30:11.827176 [info ] [MainThread]: 
[0m20:30:11.827741 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:30:11.828835 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:11.839118 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:11.839546 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:11.839741 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:30:11.839881 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:13.029808 [debug] [ThreadPool]: SQL status: OK in 1.19 seconds
[0m20:30:13.036687 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:30:13.036943 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:30:13.037112 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:30:13.514745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e84823c-1484-4d39-910d-f46f8a70bd8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc0cd90>]}
[0m20:30:13.515570 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:13.516116 [info ] [MainThread]: 
[0m20:30:13.521031 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:13.521405 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:30:13.522396 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:13.522752 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:13.523038 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:13.523254 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:13.523895 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:13.524238 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:13.524584 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:30:13.535722 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:13.536009 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:30:13.536175 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:30:14.105055 | 99d82425-0110-4afa-80c6-fc620d04344c ==============================
[0m20:30:14.105115 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:14.105923 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:30:14.106193 [debug] [MainThread]: Tracking: tracking
[0m20:30:14.124153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11690d280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11690d2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11690dca0>]}
[0m20:30:14.174126 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:14.174476 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:14.181997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99d82425-0110-4afa-80c6-fc620d04344c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116ab30d0>]}
[0m20:30:14.190066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99d82425-0110-4afa-80c6-fc620d04344c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169f43a0>]}
[0m20:30:14.190425 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:14.190686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99d82425-0110-4afa-80c6-fc620d04344c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169f4430>]}
[0m20:30:14.191433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169f4400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169f4430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169f4190>]}
[0m20:30:14.191665 [debug] [MainThread]: Flushing usage events
[0m20:30:14.768518 [debug] [Thread-1  ]: SQL status: OK in 1.23 seconds
[0m20:30:14.775285 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:30:14.775718 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:14.776050 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:30:15.547698 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:15.549236 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 2.03s]
[0m20:30:15.550251 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:15.552389 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:30:15.552911 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:30:15.594965 [info ] [MainThread]: Done.
[0m20:30:15.595745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc315b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dec9910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dec9b80>]}
[0m20:30:15.596203 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:30:22.611425 | 09376f82-e9b7-4891-984a-300865a8099c ==============================
[0m20:30:22.611534 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:22.613118 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:30:22.613768 [debug] [MainThread]: Tracking: tracking
[0m20:30:22.635242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2562b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f256370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f256160>]}
[0m20:30:22.692940 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:30:22.693676 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:30:22.694022 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:22.694184 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:30:22.707188 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:30:22.727061 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:30:22.730806 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:30:22.733887 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:30:22.736574 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:30:22.764476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09376f82-e9b7-4891-984a-300865a8099c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f44f0d0>]}
[0m20:30:22.773623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09376f82-e9b7-4891-984a-300865a8099c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f340730>]}
[0m20:30:22.774043 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:22.774408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09376f82-e9b7-4891-984a-300865a8099c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f238b50>]}
[0m20:30:22.775814 [info ] [MainThread]: 
[0m20:30:22.776441 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:30:22.777444 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:22.787767 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:22.788068 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:22.788488 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:30:22.788858 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:23.929504 [debug] [ThreadPool]: SQL status: OK in 1.14 seconds
[0m20:30:23.936618 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:30:23.936898 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:30:23.937050 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:30:24.394893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09376f82-e9b7-4891-984a-300865a8099c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f24d160>]}
[0m20:30:24.395522 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:24.395828 [info ] [MainThread]: 
[0m20:30:24.399450 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:24.399722 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:30:24.400267 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:24.400500 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:24.400698 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:24.400850 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:24.401218 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:24.401378 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:24.401512 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:30:24.410872 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:24.411158 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:30:24.411331 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-13 20:30:25.579353 | 6e63bbb3-a60e-4b38-bf84-909c53aec90c ==============================
[0m20:30:25.579413 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:25.580506 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:30:25.581243 [debug] [MainThread]: Tracking: tracking
[0m20:30:25.596463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad1fe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad1feb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ad1f9a0>]}
[0m20:30:25.648023 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:25.648301 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:25.654897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e63bbb3-a60e-4b38-bf84-909c53aec90c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11aeb20d0>]}
[0m20:30:25.662821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e63bbb3-a60e-4b38-bf84-909c53aec90c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf4400>]}
[0m20:30:25.663486 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:25.663781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e63bbb3-a60e-4b38-bf84-909c53aec90c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf4490>]}
[0m20:30:25.664649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf4460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf40a0>]}
[0m20:30:25.664963 [debug] [MainThread]: Flushing usage events
[0m20:30:25.700321 [debug] [Thread-1  ]: SQL status: OK in 1.29 seconds
[0m20:30:25.704769 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:30:25.705127 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:25.705380 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:30:26.199043 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:26.199687 [warn ] [Thread-1  ]: 1 of 1 WARN freshness of jaffle_shop.jaffle_shop_orders ........................ [[33mWARN[0m in 1.80s]
[0m20:30:26.200305 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:26.201614 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:30:26.201886 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:30:26.212140 [info ] [MainThread]: Done.
[0m20:30:26.212879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f503c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f5039a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f503b80>]}
[0m20:30:26.213291 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:30:33.156352 | a9551095-9ec2-45b4-b8a3-83b54a368093 ==============================
[0m20:30:33.156500 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:33.157543 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:30:33.157784 [debug] [MainThread]: Tracking: tracking
[0m20:30:33.177569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c8e2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c8eb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c8e160>]}
[0m20:30:33.232342 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:30:33.233057 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:30:33.233393 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:33.233561 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:30:33.247000 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:30:33.260402 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:30:33.264656 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:30:33.268297 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:30:33.272377 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:30:33.302090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a9551095-9ec2-45b4-b8a3-83b54a368093', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122e870d0>]}
[0m20:30:33.310013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a9551095-9ec2-45b4-b8a3-83b54a368093', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122d76730>]}
[0m20:30:33.310397 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:33.310946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a9551095-9ec2-45b4-b8a3-83b54a368093', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c70dc0>]}
[0m20:30:33.312783 [info ] [MainThread]: 
[0m20:30:33.313472 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:30:33.314676 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:33.329691 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:33.329974 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:33.330130 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:30:33.330263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:34.525922 [debug] [ThreadPool]: SQL status: OK in 1.2 seconds
[0m20:30:34.533418 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:30:34.533669 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:30:34.533823 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:30:34.999501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a9551095-9ec2-45b4-b8a3-83b54a368093', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122c85160>]}
[0m20:30:35.000166 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:35.000552 [info ] [MainThread]: 
[0m20:30:35.003894 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:35.004168 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:30:35.004681 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:35.004880 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:35.005063 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:35.005209 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:35.005568 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:35.005722 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:35.005843 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:30:35.016177 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:35.016429 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:30:35.016630 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:36.145663 [debug] [Thread-1  ]: SQL status: OK in 1.13 seconds
[0m20:30:36.148686 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:30:36.148916 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:36.149063 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close


============================== 2022-10-13 20:30:36.213292 | 9465b5de-1b3b-44bc-8be1-bf057603d8e3 ==============================
[0m20:30:36.213353 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:36.214327 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:30:36.214755 [debug] [MainThread]: Tracking: tracking
[0m20:30:36.232861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e58f040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e58fca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e58fac0>]}
[0m20:30:36.288072 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:36.288347 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:36.294894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9465b5de-1b3b-44bc-8be1-bf057603d8e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e7230d0>]}
[0m20:30:36.303545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9465b5de-1b3b-44bc-8be1-bf057603d8e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e670430>]}
[0m20:30:36.303924 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:36.304185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9465b5de-1b3b-44bc-8be1-bf057603d8e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e6704c0>]}
[0m20:30:36.304928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e670490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e6704c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e670100>]}
[0m20:30:36.305167 [debug] [MainThread]: Flushing usage events
[0m20:30:36.604795 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:36.606296 [info ] [Thread-1  ]: 1 of 1 PASS freshness of jaffle_shop.jaffle_shop_orders ........................ [[32mPASS[0m in 1.60s]
[0m20:30:36.607364 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:36.609645 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:30:36.610145 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:30:36.628624 [info ] [MainThread]: Done.
[0m20:30:36.629497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122f3ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122f3a9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122f3ab80>]}
[0m20:30:36.630056 [debug] [MainThread]: Flushing usage events


============================== 2022-10-13 20:30:45.131699 | de41295b-7a05-4925-bbc6-cc29731d882a ==============================
[0m20:30:45.131746 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:45.132985 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'source-freshness', 'rpc_method': 'source-freshness', 'indirect_selection': 'eager'}
[0m20:30:45.133340 [debug] [MainThread]: Tracking: tracking
[0m20:30:45.148920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf7a580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf7aa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf7a520>]}
[0m20:30:45.201327 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:30:45.202019 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_customers
[0m20:30:45.202337 [debug] [MainThread]: Partial parsing: deleted source source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:45.202492 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/src_jaffle_shop.yml
[0m20:30:45.215960 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m20:30:45.230630 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m20:30:45.233430 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/orders.sql
[0m20:30:45.236574 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m20:30:45.239241 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m20:30:45.266417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de41295b-7a05-4925-bbc6-cc29731d882a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1b20d0>]}
[0m20:30:45.274603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de41295b-7a05-4925-bbc6-cc29731d882a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bfad190>]}
[0m20:30:45.275023 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:45.275314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de41295b-7a05-4925-bbc6-cc29731d882a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bfada00>]}
[0m20:30:45.276651 [info ] [MainThread]: 
[0m20:30:45.277269 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m20:30:45.278209 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:45.288926 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:45.289208 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m20:30:45.289364 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m20:30:45.289526 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:30:46.511284 [debug] [ThreadPool]: SQL status: OK in 1.22 seconds
[0m20:30:46.521424 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m20:30:46.521672 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m20:30:46.521985 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m20:30:46.987461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de41295b-7a05-4925-bbc6-cc29731d882a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf48f70>]}
[0m20:30:46.988074 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:30:46.988368 [info ] [MainThread]: 
[0m20:30:46.991912 [debug] [Thread-1  ]: Began running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:46.992200 [info ] [Thread-1  ]: 1 of 1 START freshness of jaffle_shop.jaffle_shop_orders ....................... [RUN]
[0m20:30:46.992794 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:46.992994 [debug] [Thread-1  ]: Began compiling node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:46.993166 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:46.993299 [debug] [Thread-1  ]: Began executing node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:46.993772 [debug] [Thread-1  ]: Acquiring new databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:46.993958 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m20:30:46.994128 [debug] [Thread-1  ]: Spark adapter: NotImplemented: commit
[0m20:30:47.003903 [debug] [Thread-1  ]: Using databricks connection "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"
[0m20:30:47.004144 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "source.jaffle_shop.jaffle_shop.jaffle_shop_orders"} */
select
      max(ORDER_DATE) as max_loaded_at,
      current_timestamp() as snapshotted_at
    from default.jaffle_shop_orders
    
  
[0m20:30:47.004285 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m20:30:48.118669 [debug] [Thread-1  ]: SQL status: OK in 1.11 seconds
[0m20:30:48.123731 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: ROLLBACK
[0m20:30:48.124102 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m20:30:48.124325 [debug] [Thread-1  ]: On source.jaffle_shop.jaffle_shop.jaffle_shop_orders: Close
[0m20:30:48.573488 [debug] [Thread-1  ]: finished collecting timing info
[0m20:30:48.574280 [info ] [Thread-1  ]: 1 of 1 PASS freshness of jaffle_shop.jaffle_shop_orders ........................ [[32mPASS[0m in 1.58s]
[0m20:30:48.574966 [debug] [Thread-1  ]: Finished running node source.jaffle_shop.jaffle_shop.jaffle_shop_orders
[0m20:30:48.576144 [debug] [MainThread]: Connection 'master' was properly closed.


============================== 2022-10-13 20:30:48.575229 | dd4a81c2-4093-41be-b612-dccbd219b524 ==============================
[0m20:30:48.575310 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:30:48.576385 [debug] [MainThread]: Connection 'source.jaffle_shop.jaffle_shop.jaffle_shop_orders' was properly closed.
[0m20:30:48.576618 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:30:48.576896 [debug] [MainThread]: Tracking: tracking
[0m20:30:48.584597 [info ] [MainThread]: Done.
[0m20:30:48.585111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf6d340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c223910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c223b80>]}
[0m20:30:48.585502 [debug] [MainThread]: Flushing usage events
[0m20:30:48.594247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11824bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11824b850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11824ba90>]}
[0m20:30:48.653684 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:48.653976 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:48.661281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dd4a81c2-4093-41be-b612-dccbd219b524', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183de0d0>]}
[0m20:30:48.673770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd4a81c2-4093-41be-b612-dccbd219b524', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118320400>]}
[0m20:30:48.674291 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:30:48.674609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd4a81c2-4093-41be-b612-dccbd219b524', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118320490>]}
[0m20:30:48.675475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118320460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118320490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183200a0>]}
[0m20:30:48.675820 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 14:44:37.325192 | b94baf7f-56b2-4dca-a00d-b0fbb95dd09c ==============================
[0m14:44:37.325245 [info ] [MainThread]: Running with dbt=1.2.1
[0m14:44:37.327080 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m14:44:37.327418 [debug] [MainThread]: Tracking: tracking
[0m14:44:37.363602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173bee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173beeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1173be430>]}
[0m14:44:37.460603 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m14:44:37.461065 [debug] [MainThread]: Partial parsing: added file: jaffle_shop://models/staging/jaffle_shop/customers.sql
[0m14:44:37.461265 [debug] [MainThread]: Partial parsing: deleted file: jaffle_shop://models/staging/jaffle_shop/orders.sql
[0m14:44:37.478257 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m14:44:37.511972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b94baf7f-56b2-4dca-a00d-b0fbb95dd09c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175492b0>]}
[0m14:44:37.523470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b94baf7f-56b2-4dca-a00d-b0fbb95dd09c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117520eb0>]}
[0m14:44:37.523999 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m14:44:37.524307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b94baf7f-56b2-4dca-a00d-b0fbb95dd09c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117509fa0>]}
[0m14:44:37.525229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117509fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117520c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117520b50>]}
[0m14:44:37.525823 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:45:38.191849 | 54bff9fe-91b0-4bb9-836a-39b502b2dfba ==============================
[0m15:45:38.191913 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:45:38.195083 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:45:38.195622 [debug] [MainThread]: Tracking: tracking
[0m15:45:38.226256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11717bc70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11717b8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11717b550>]}
[0m15:45:38.258653 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m15:45:38.259073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '54bff9fe-91b0-4bb9-836a-39b502b2dfba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171549d0>]}
[0m15:45:38.325735 [debug] [MainThread]: Parsing macros/statement.sql
[0m15:45:38.329298 [debug] [MainThread]: Parsing macros/catalog.sql
[0m15:45:38.331599 [debug] [MainThread]: Parsing macros/adapters.sql
[0m15:45:38.355660 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m15:45:38.365158 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m15:45:38.365983 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m15:45:38.370140 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m15:45:38.389673 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m15:45:38.397568 [debug] [MainThread]: Parsing macros/adapters.sql
[0m15:45:38.449428 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m15:45:38.475311 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m15:45:38.487182 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m15:45:38.488311 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m15:45:38.492766 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m15:45:38.520488 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m15:45:38.525799 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m15:45:38.533930 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m15:45:38.541463 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m15:45:38.542759 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m15:45:38.545335 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m15:45:38.552027 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m15:45:38.554090 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m15:45:38.568326 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m15:45:38.569215 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m15:45:38.569861 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m15:45:38.571213 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m15:45:38.575408 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m15:45:38.578009 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m15:45:38.579603 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m15:45:38.597734 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m15:45:38.615165 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m15:45:38.628469 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m15:45:38.633138 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m15:45:38.635041 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m15:45:38.637077 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m15:45:38.641731 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m15:45:38.662689 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m15:45:38.664488 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m15:45:38.690102 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m15:45:38.723459 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m15:45:38.731484 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m15:45:38.736591 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m15:45:38.747532 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m15:45:38.751000 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m15:45:38.758895 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m15:45:38.764227 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m15:45:38.777557 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m15:45:38.812453 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m15:45:38.815879 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m15:45:38.821883 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m15:45:38.825402 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m15:45:38.826551 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m15:45:38.828137 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m15:45:38.829345 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m15:45:38.832149 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m15:45:38.840235 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m15:45:38.855401 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m15:45:38.857068 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m15:45:38.859804 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m15:45:38.861919 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m15:45:38.864000 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m15:45:38.866620 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m15:45:38.868381 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m15:45:38.870386 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m15:45:38.872898 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m15:45:38.877653 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m15:45:38.880099 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m15:45:38.882238 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m15:45:38.884111 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m15:45:38.885426 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m15:45:38.886454 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m15:45:38.888120 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m15:45:38.889532 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m15:45:38.896351 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m15:45:38.897920 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m15:45:38.901361 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m15:45:38.904182 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m15:45:38.905822 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m15:45:38.909077 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m15:45:38.913118 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m15:45:38.933680 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m15:45:38.936954 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m15:45:38.959759 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m15:45:38.969533 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m15:45:38.981073 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m15:45:38.998661 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m15:45:39.436904 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m15:45:39.450933 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m15:45:39.454280 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:45:39.457836 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m15:45:39.462964 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m15:45:39.467288 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m15:45:39.527549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54bff9fe-91b0-4bb9-836a-39b502b2dfba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1172a7250>]}
[0m15:45:39.538294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54bff9fe-91b0-4bb9-836a-39b502b2dfba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117169280>]}
[0m15:45:39.538730 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:45:39.538985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54bff9fe-91b0-4bb9-836a-39b502b2dfba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171b3af0>]}
[0m15:45:39.539686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1172c5d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171b3af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171da220>]}
[0m15:45:39.539907 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:45:57.535436 | b9a46e69-c68f-42e8-9c05-0c5b8ae117a2 ==============================
[0m15:45:57.535532 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:45:57.537269 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:45:57.537814 [debug] [MainThread]: Tracking: tracking
[0m15:45:57.558445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ba9280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ba92b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ba9ca0>]}
[0m15:45:57.600660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bebcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d1ee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c1b0a0>]}
[0m15:45:57.601203 [debug] [MainThread]: Flushing usage events
[0m15:45:58.196465 [error] [MainThread]: Encountered an error:
Parsing Error
  The schema file at models/staging/jaffle_shop/stg_jaffle_shop.yml is invalid because a list element for 'models' is not a dictionary
[0m15:45:58.206264 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 57, in load_source_file
    validate_yaml(source_file.path.original_file_path, dfy)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 83, in validate_yaml
    raise ParsingException(msg)
dbt.exceptions.ParsingException: Parsing Error
  The schema file at models/staging/jaffle_shop/stg_jaffle_shop.yml is invalid because a list element for 'models' is not a dictionary



============================== 2022-10-14 15:46:04.054974 | c76be3c7-eb16-461c-8aaa-d41d2d6de6e7 ==============================
[0m15:46:04.055052 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:04.056154 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:04.056449 [debug] [MainThread]: Tracking: tracking
[0m15:46:04.075314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113bbe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113bba30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113bb610>]}
[0m15:46:04.131838 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:46:04.132745 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:46:04.153877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c76be3c7-eb16-461c-8aaa-d41d2d6de6e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115950d0>]}
[0m15:46:04.163887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c76be3c7-eb16-461c-8aaa-d41d2d6de6e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11154c160>]}
[0m15:46:04.164347 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:46:04.164666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c76be3c7-eb16-461c-8aaa-d41d2d6de6e7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11154c220>]}
[0m15:46:04.165446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11154c2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11154c220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11154b160>]}
[0m15:46:04.165670 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:46:09.265625 | 02f98357-31cc-4802-b6ad-c95d8de4f4c2 ==============================
[0m15:46:09.265696 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:09.267258 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:09.267873 [debug] [MainThread]: Tracking: tracking
[0m15:46:09.287551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119befe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bef040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119befa90>]}
[0m15:46:09.330491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119bef6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c55160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c551c0>]}
[0m15:46:09.330920 [debug] [MainThread]: Flushing usage events
[0m15:46:09.982717 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns:
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:46:09.991383 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns:
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns:
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:46:15.701726 | eaa252ac-f2e0-4670-9ee5-752daadd39bc ==============================
[0m15:46:15.701811 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:15.703187 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:15.703490 [debug] [MainThread]: Tracking: tracking
[0m15:46:15.724036 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fdbfee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fdbfe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fdbfaf0>]}
[0m15:46:15.769868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ff22f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fe1f130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fe1f190>]}
[0m15:46:15.770345 [debug] [MainThread]: Flushing usage events
[0m15:46:16.368671 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns:
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:46:16.371051 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns:
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns:
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:46:24.693766 | 8ffd20d9-6a4d-426e-9abc-b7e1bd80f513 ==============================
[0m15:46:24.693892 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:24.695069 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:24.695421 [debug] [MainThread]: Tracking: tracking
[0m15:46:24.717035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7b61c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7b62b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7b6370>]}
[0m15:46:24.757087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a7b6dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a81d160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a81d1c0>]}
[0m15:46:24.757492 [debug] [MainThread]: Flushing usage events
[0m15:46:25.369329 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:46:25.373003 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:46:35.882755 | 8c4ff037-2892-40cf-997c-5fb2b335750d ==============================
[0m15:46:35.882826 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:35.884307 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:35.884651 [debug] [MainThread]: Tracking: tracking
[0m15:46:35.901572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b95eb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b95e340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b95e790>]}
[0m15:46:35.938752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b95e430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9bf190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9bf1f0>]}
[0m15:46:35.939323 [debug] [MainThread]: Flushing usage events
[0m15:46:36.534209 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:46:36.536502 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:46:46.802259 | d79f80bc-c5a9-444c-a71a-dc6c467e4b1a ==============================
[0m15:46:46.802329 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:46.803705 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:46.804086 [debug] [MainThread]: Tracking: tracking
[0m15:46:46.822375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235f5b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235f5df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235f5340>]}
[0m15:46:46.860733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235f5430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123656190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236561f0>]}
[0m15:46:46.861063 [debug] [MainThread]: Flushing usage events
[0m15:46:47.524665 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:46:47.526798 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:46:50.676843 | c899db13-13da-4bd4-8a33-1aebae680e4c ==============================
[0m15:46:50.676951 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:46:50.678274 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:46:50.678647 [debug] [MainThread]: Tracking: tracking
[0m15:46:50.695790 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b890b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b890160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b890430>]}
[0m15:46:50.734131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b890340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b8f1190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b8f11f0>]}
[0m15:46:50.734535 [debug] [MainThread]: Flushing usage events
[0m15:46:51.338331 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:46:51.343746 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:47:30.408696 | 7b7cbc09-b3eb-49ce-a061-877e150b2b30 ==============================
[0m15:47:30.408836 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:47:30.410337 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:47:30.410815 [debug] [MainThread]: Tracking: tracking
[0m15:47:30.428542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11856df40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11856d220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11856d0d0>]}
[0m15:47:30.470791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185b01f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185df040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185df0a0>]}
[0m15:47:30.471110 [debug] [MainThread]: Flushing usage events
[0m15:47:31.176254 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:47:31.179392 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:47:37.984792 | 95922422-3410-4343-b2f4-b8f6bd8a23ce ==============================
[0m15:47:37.984908 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:47:37.993892 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:47:37.994331 [debug] [MainThread]: Tracking: tracking
[0m15:47:38.018207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ca9040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ca9df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ca99d0>]}
[0m15:47:38.055104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ca9ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122d0a190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122d0a1f0>]}
[0m15:47:38.055511 [debug] [MainThread]: Flushing usage events
[0m15:47:38.694754 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:47:38.697402 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:47:47.176088 | c6abb27b-b3e9-4017-b7fd-36356556864b ==============================
[0m15:47:47.176135 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:47:47.177353 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:47:47.177646 [debug] [MainThread]: Tracking: tracking
[0m15:47:47.197976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bed6e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bed6e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bed6340>]}
[0m15:47:47.244600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bed6dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf3d160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf3d1c0>]}
[0m15:47:47.245078 [debug] [MainThread]: Flushing usage events
[0m15:47:47.846325 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:47:47.848940 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:47:50.490919 | 981a4ef2-e10d-464e-adcb-da1608bc792c ==============================
[0m15:47:50.490980 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:47:50.491993 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:47:50.492487 [debug] [MainThread]: Tracking: tracking
[0m15:47:50.510374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1238c1fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1238c1f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1238c1430>]}
[0m15:47:50.546003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123a25f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123922130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123922190>]}
[0m15:47:50.546493 [debug] [MainThread]: Flushing usage events
[0m15:47:51.332715 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:47:51.337304 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:48:42.174419 | 1b862925-b3f2-462f-bfa2-e91edb8ae721 ==============================
[0m15:48:42.174500 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:48:42.176230 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:48:42.176730 [debug] [MainThread]: Tracking: tracking
[0m15:48:42.197024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121616eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121616310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121616160>]}
[0m15:48:42.235261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121616880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121693430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121693280>]}
[0m15:48:42.235630 [debug] [MainThread]: Flushing usage events
[0m15:48:42.888172 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:48:42.892977 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       - tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       - tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:22.161049 | 09b6a7b2-81f9-433f-8fa3-ad381badb46a ==============================
[0m15:49:22.161200 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:22.162994 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:49:22.163463 [debug] [MainThread]: Tracking: tracking
[0m15:49:22.181314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dfe1b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dfe1df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dfe1340>]}
[0m15:49:22.227451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dfe1430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e043190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e0431f0>]}
[0m15:49:22.227830 [debug] [MainThread]: Flushing usage events
[0m15:49:23.024379 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:23.027302 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:27.246231 | d7b52831-25cf-4198-91af-bb2e76d70ebd ==============================
[0m15:49:27.246291 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:27.247269 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:49:27.247541 [debug] [MainThread]: Tracking: tracking
[0m15:49:27.267131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11697be20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11697beb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11697b9a0>]}
[0m15:49:27.312730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11697bdc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169dc160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1169dc1c0>]}
[0m15:49:27.313165 [debug] [MainThread]: Flushing usage events
[0m15:49:27.926947 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:27.930566 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:30.849974 | 1bbde094-4f31-482c-9a8e-e3e487b44062 ==============================
[0m15:49:30.850094 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:30.851216 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:49:30.851595 [debug] [MainThread]: Tracking: tracking
[0m15:49:30.870606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d49ee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d49e580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d49e2b0>]}
[0m15:49:30.907698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d602f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d506130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d506190>]}
[0m15:49:30.908044 [debug] [MainThread]: Flushing usage events
[0m15:49:31.512838 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:31.519007 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:35.202353 | eb0f78a2-b5ad-45d8-b370-dcc1fb192373 ==============================
[0m15:49:35.202417 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:35.204036 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:49:35.204473 [debug] [MainThread]: Tracking: tracking
[0m15:49:35.223819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c8df40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c8d220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120c8d0d0>]}
[0m15:49:35.259413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ccfca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120cff040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120cff0a0>]}
[0m15:49:35.259821 [debug] [MainThread]: Flushing usage events
[0m15:49:35.860960 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:35.865717 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:40.820955 | d0a84444-d77c-4de0-96a4-234800cb9618 ==============================
[0m15:49:40.821125 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:40.822977 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:49:40.824194 [debug] [MainThread]: Tracking: tracking
[0m15:49:40.849474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d656b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6565e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d656370>]}
[0m15:49:40.908179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d656340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6b8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6b81f0>]}
[0m15:49:40.908896 [debug] [MainThread]: Flushing usage events
[0m15:49:41.522547 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:41.526576 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:50.440973 | 874264d5-13b8-4ef7-ae2f-e6a5b0353d67 ==============================
[0m15:49:50.441074 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:50.443890 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:49:50.444486 [debug] [MainThread]: Tracking: tracking
[0m15:49:50.467231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112de6eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112de6310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112de6160>]}
[0m15:49:50.511149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112de6bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e57430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e57280>]}
[0m15:49:50.511649 [debug] [MainThread]: Flushing usage events
[0m15:49:51.096779 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:51.099675 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:49:52.603367 | 5947aec0-8f25-4e7e-bcab-82f573338b16 ==============================
[0m15:49:52.603409 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:49:52.604634 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:49:52.604985 [debug] [MainThread]: Tracking: tracking
[0m15:49:52.623425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7bbe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7bb820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7bb610>]}
[0m15:49:52.664503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d91ef70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d81b130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d81b190>]}
[0m15:49:52.664958 [debug] [MainThread]: Flushing usage events
[0m15:49:53.259597 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:49:53.262900 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:50:07.866084 | cacf9125-5b7a-4d0d-99de-a46f10b89929 ==============================
[0m15:50:07.866147 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:50:07.868592 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:50:07.869034 [debug] [MainThread]: Tracking: tracking
[0m15:50:07.888432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab562b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab56370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab56160>]}
[0m15:50:07.932652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab56a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120cb4460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120cb42b0>]}
[0m15:50:07.932981 [debug] [MainThread]: Flushing usage events
[0m15:50:08.588139 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:50:08.590559 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:50:10.741317 | e2935486-2826-4972-bb21-80c39d107490 ==============================
[0m15:50:10.741520 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:50:10.742944 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:50:10.743259 [debug] [MainThread]: Tracking: tracking
[0m15:50:10.761747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fc5f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fc5220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fc50d0>]}
[0m15:50:10.803013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1230084f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123037070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1230370a0>]}
[0m15:50:10.803402 [debug] [MainThread]: Flushing usage events
[0m15:50:11.411528 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:50:11.416463 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:50:26.355070 | 2f72f0bf-7916-4072-bfc0-d4dad9b7636b ==============================
[0m15:50:26.355212 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:50:26.356826 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:50:26.357551 [debug] [MainThread]: Tracking: tracking
[0m15:50:26.376238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab35eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab35310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab35160>]}
[0m15:50:26.417933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ab35880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abb9430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11abb9280>]}
[0m15:50:26.418371 [debug] [MainThread]: Flushing usage events
[0m15:50:26.999305 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:50:27.001729 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:50:28.786246 | 2c2a6cab-be1d-4678-bea6-446847d63d96 ==============================
[0m15:50:28.786304 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:50:28.802618 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:50:28.803005 [debug] [MainThread]: Tracking: tracking
[0m15:50:28.819797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f7ae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f7ab20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f7a340>]}
[0m15:50:28.861608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120f7a730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fdb160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120fdb1c0>]}
[0m15:50:28.874806 [debug] [MainThread]: Flushing usage events
[0m15:50:29.480598 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3
[0m15:50:29.504454 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 4, column 2
did not find expected '-' indicator
  in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 5
  ------------------------------
  2  | 
  3  | models:
  4  |  - name: stg_customers
  5  |   columns: 
  6  |     - name: customer_id
  7  |       tests:
  8  |         - unique
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 4, column 2
  did not find expected '-' indicator
    in "<unicode string>", line 5, column 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 5
    ------------------------------
    2  | 
    3  | models:
    4  |  - name: stg_customers
    5  |   columns: 
    6  |     - name: customer_id
    7  |       tests:
    8  |         - unique
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 4, column 2
    did not find expected '-' indicator
      in "<unicode string>", line 5, column 3



============================== 2022-10-14 15:51:11.217612 | ff4dc1ae-c672-47cf-abf6-fffedc8e5b8f ==============================
[0m15:51:11.217737 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:11.218961 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:11.219240 [debug] [MainThread]: Tracking: tracking
[0m15:51:11.235569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e8dee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e8deb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e8de340>]}
[0m15:51:11.288765 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:11.289430 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:51:11.306410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff4dc1ae-c672-47cf-abf6-fffedc8e5b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eaaffd0>]}
[0m15:51:11.315777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff4dc1ae-c672-47cf-abf6-fffedc8e5b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea702e0>]}
[0m15:51:11.316226 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:11.316509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff4dc1ae-c672-47cf-abf6-fffedc8e5b8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea70220>]}
[0m15:51:11.317415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea708b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea70a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ea70970>]}
[0m15:51:11.317793 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:17.068717 | fdea37f0-cc14-473e-93ab-20e5d72d4962 ==============================
[0m15:51:17.068826 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:17.070197 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:17.070577 [debug] [MainThread]: Tracking: tracking
[0m15:51:17.086229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115075e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115075c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115075af0>]}
[0m15:51:17.170161 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:17.171547 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:51:17.189126 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m15:51:17.227339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fdea37f0-cc14-473e-93ab-20e5d72d4962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1152000d0>]}
[0m15:51:17.234939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fdea37f0-cc14-473e-93ab-20e5d72d4962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1151d8f40>]}
[0m15:51:17.235303 [info ] [MainThread]: Found 6 models, 0 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:17.235576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdea37f0-cc14-473e-93ab-20e5d72d4962', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1151d8eb0>]}
[0m15:51:17.236315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11504e310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11504e640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11504efd0>]}
[0m15:51:17.236549 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:23.001410 | bd463be2-9cc7-4d1a-9041-5efadd53eb8e ==============================
[0m15:51:23.001458 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:23.002845 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:23.003355 [debug] [MainThread]: Tracking: tracking
[0m15:51:23.024777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9bab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9ba160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9ba430>]}
[0m15:51:23.082503 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:23.083133 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:51:23.098460 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m15:51:23.149992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd463be2-9cc7-4d1a-9041-5efadd53eb8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ebe90d0>]}
[0m15:51:23.159880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd463be2-9cc7-4d1a-9041-5efadd53eb8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb1eeb0>]}
[0m15:51:23.160300 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:23.160612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd463be2-9cc7-4d1a-9041-5efadd53eb8e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11eb1efd0>]}
[0m15:51:23.161448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9a9340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9a9c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e9a9cd0>]}
[0m15:51:23.161921 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:25.841990 | 0ca5fbed-4ebd-4692-b60c-9323208d6007 ==============================
[0m15:51:25.842161 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:25.843631 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:25.844105 [debug] [MainThread]: Tracking: tracking
[0m15:51:25.864798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e34d280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e34d2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e34dca0>]}
[0m15:51:25.923519 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:25.924038 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:51:25.940551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ca5fbed-4ebd-4692-b60c-9323208d6007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e53a0d0>]}
[0m15:51:25.950735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ca5fbed-4ebd-4692-b60c-9323208d6007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4c7ee0>]}
[0m15:51:25.951146 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:25.951418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ca5fbed-4ebd-4692-b60c-9323208d6007', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e33d190>]}
[0m15:51:25.952246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4c7f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4f0c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e4f0a00>]}
[0m15:51:25.952555 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:28.989965 | 5d17abf8-e967-4235-8e92-72f17e9673ea ==============================
[0m15:51:28.990023 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:28.991295 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:28.991515 [debug] [MainThread]: Tracking: tracking
[0m15:51:29.009747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222f58b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222f5550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1222f5670>]}
[0m15:51:29.062511 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:51:29.062920 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:51:29.070610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d17abf8-e967-4235-8e92-72f17e9673ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1224930d0>]}
[0m15:51:29.078284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d17abf8-e967-4235-8e92-72f17e9673ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1223d6a00>]}
[0m15:51:29.078662 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:29.078926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d17abf8-e967-4235-8e92-72f17e9673ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1223d6a90>]}
[0m15:51:29.079667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1223d6a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1223d6b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1223d6520>]}
[0m15:51:29.079903 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:32.538272 | 18f634be-6f5a-4512-82b2-36059e8c51ee ==============================
[0m15:51:32.538346 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:32.539318 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:32.539934 [debug] [MainThread]: Tracking: tracking
[0m15:51:32.560540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184fab80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184fa5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184fa8e0>]}
[0m15:51:32.618762 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:51:32.619123 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:51:32.625839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '18f634be-6f5a-4512-82b2-36059e8c51ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186980d0>]}
[0m15:51:32.633677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '18f634be-6f5a-4512-82b2-36059e8c51ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e2a00>]}
[0m15:51:32.634040 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:32.634297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '18f634be-6f5a-4512-82b2-36059e8c51ee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e2a90>]}
[0m15:51:32.635027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e2a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e2b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e2520>]}
[0m15:51:32.635264 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:41.433881 | 5c09243a-f523-4f34-b809-98466d45f410 ==============================
[0m15:51:41.434029 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:41.435745 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:41.436189 [debug] [MainThread]: Tracking: tracking
[0m15:51:41.457067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f13e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f13040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f139a0>]}
[0m15:51:41.518185 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:41.518852 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:51:41.531915 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:51:41.570563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5c09243a-f523-4f34-b809-98466d45f410', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1160a30d0>]}
[0m15:51:41.578613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5c09243a-f523-4f34-b809-98466d45f410', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1160cfdf0>]}
[0m15:51:41.579005 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:41.579330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5c09243a-f523-4f34-b809-98466d45f410', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11607af40>]}
[0m15:51:41.580047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f44490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f444f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f44550>]}
[0m15:51:41.580273 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:44.005867 | 5d4d8a4e-7ce2-475d-a9df-4db6d55d24cf ==============================
[0m15:51:44.005926 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:44.006997 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:44.007324 [debug] [MainThread]: Tracking: tracking
[0m15:51:44.023706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a30ff70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a30f220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a30f0d0>]}
[0m15:51:44.079070 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:51:44.079323 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:51:44.086903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d4d8a4e-7ce2-475d-a9df-4db6d55d24cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4be0d0>]}
[0m15:51:44.096143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d4d8a4e-7ce2-475d-a9df-4db6d55d24cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a404940>]}
[0m15:51:44.096614 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:44.096904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d4d8a4e-7ce2-475d-a9df-4db6d55d24cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4049d0>]}
[0m15:51:44.097724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4049a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a404a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a404460>]}
[0m15:51:44.098007 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:47.503246 | 6b01ffe3-3c1d-4d80-8e1b-788b6e394089 ==============================
[0m15:51:47.503298 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:47.504454 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:47.505005 [debug] [MainThread]: Tracking: tracking
[0m15:51:47.521610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bfbbe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bfbb820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bfbb970>]}
[0m15:51:47.576134 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:51:47.576390 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:51:47.584344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b01ffe3-3c1d-4d80-8e1b-788b6e394089', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c1580d0>]}
[0m15:51:47.594193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b01ffe3-3c1d-4d80-8e1b-788b6e394089', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c09d9a0>]}
[0m15:51:47.594679 [info ] [MainThread]: Found 6 models, 2 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:47.594986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b01ffe3-3c1d-4d80-8e1b-788b6e394089', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c09da30>]}
[0m15:51:47.595869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c09da00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c09dac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c09d4c0>]}
[0m15:51:47.596162 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:51:55.214085 | a6479b69-b4f0-49e4-979b-aad5062e0577 ==============================
[0m15:51:55.214145 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:51:55.215130 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:51:55.215479 [debug] [MainThread]: Tracking: tracking
[0m15:51:55.231379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119caafa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119caae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119caa430>]}
[0m15:51:55.285460 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:55.286227 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:51:55.301500 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:51:55.324898 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'status' in the 'models' section of file 'models/staging/jaffle_shop/stg_jaffle_shop.yml'
[0m15:51:55.351063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a6479b69-b4f0-49e4-979b-aad5062e0577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119edc0d0>]}
[0m15:51:55.360161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a6479b69-b4f0-49e4-979b-aad5062e0577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c84310>]}
[0m15:51:55.360637 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:51:55.360952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6479b69-b4f0-49e4-979b-aad5062e0577', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e12f40>]}
[0m15:51:55.361678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e12fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c996a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c99bb0>]}
[0m15:51:55.361936 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:52:00.785332 | aaaf77e2-2133-41c7-9bc6-18db7f422400 ==============================
[0m15:52:00.785397 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:52:00.786763 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:52:00.787271 [debug] [MainThread]: Tracking: tracking
[0m15:52:00.807265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e97fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e97a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116e97580>]}
[0m15:52:00.871224 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:52:00.871754 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:52:00.888898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aaaf77e2-2133-41c7-9bc6-18db7f422400', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11706f0d0>]}
[0m15:52:00.900680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aaaf77e2-2133-41c7-9bc6-18db7f422400', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11702cdc0>]}
[0m15:52:00.901195 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:52:00.901514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aaaf77e2-2133-41c7-9bc6-18db7f422400', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117004eb0>]}
[0m15:52:00.902370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117004eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11702cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11702c880>]}
[0m15:52:00.902631 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:52:03.862672 | 2669e1e0-c295-4019-ba74-ce0e8044cb49 ==============================
[0m15:52:03.862789 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:52:03.863777 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:52:03.864038 [debug] [MainThread]: Tracking: tracking
[0m15:52:03.880009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121474b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121474340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121474790>]}
[0m15:52:03.933114 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:52:03.933421 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:52:03.941519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2669e1e0-c295-4019-ba74-ce0e8044cb49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12161fee0>]}
[0m15:52:03.950753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2669e1e0-c295-4019-ba74-ce0e8044cb49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121560be0>]}
[0m15:52:03.951172 [info ] [MainThread]: Found 6 models, 4 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:52:03.951470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2669e1e0-c295-4019-ba74-ce0e8044cb49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1215e1f10>]}
[0m15:52:03.952477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1215e1f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1215609d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121560790>]}
[0m15:52:03.952737 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:52:25.401224 | 53db48c3-32e7-4a76-8f93-03808dc398bd ==============================
[0m15:52:25.401291 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:52:25.402554 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:52:25.402848 [debug] [MainThread]: Tracking: tracking
[0m15:52:25.421591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b7eb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b7e820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122b7e580>]}
[0m15:52:25.460153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122ceaee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122cee9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122cee790>]}
[0m15:52:25.460495 [debug] [MainThread]: Flushing usage events
[0m15:52:26.109233 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 21
    ------------------------------
    18 |     - name: status
    19 |       tests:
    20 |         - accepted_values:
    21 |     values:
    22 |     - completed
    23 |     - skipped
    24 |     - returned
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 13, column 5
    did not find expected '-' indicator
      in "<unicode string>", line 21, column 5
[0m15:52:26.114674 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 808, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a block collection
  in "<unicode string>", line 13, column 5
did not find expected '-' indicator
  in "<unicode string>", line 21, column 5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 21
  ------------------------------
  18 |     - name: status
  19 |       tests:
  20 |         - accepted_values:
  21 |     values:
  22 |     - completed
  23 |     - skipped
  24 |     - returned
  
  Raw Error:
  ------------------------------
  while parsing a block collection
    in "<unicode string>", line 13, column 5
  did not find expected '-' indicator
    in "<unicode string>", line 21, column 5

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 21
    ------------------------------
    18 |     - name: status
    19 |       tests:
    20 |         - accepted_values:
    21 |     values:
    22 |     - completed
    23 |     - skipped
    24 |     - returned
    
    Raw Error:
    ------------------------------
    while parsing a block collection
      in "<unicode string>", line 13, column 5
    did not find expected '-' indicator
      in "<unicode string>", line 21, column 5



============================== 2022-10-14 15:52:36.321025 | f63dc889-bcec-4a8d-983d-f44b89b69ecd ==============================
[0m15:52:36.321073 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:52:36.322118 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:52:36.322415 [debug] [MainThread]: Tracking: tracking
[0m15:52:36.340575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af69f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af69220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af690d0>]}
[0m15:52:36.397211 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:52:36.397925 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:52:36.413224 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:52:36.477383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f63dc889-bcec-4a8d-983d-f44b89b69ecd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b1ad0d0>]}
[0m15:52:36.485727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f63dc889-bcec-4a8d-983d-f44b89b69ecd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0edac0>]}
[0m15:52:36.486159 [info ] [MainThread]: Found 6 models, 5 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:52:36.486422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f63dc889-bcec-4a8d-983d-f44b89b69ecd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0e8e50>]}
[0m15:52:36.487340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0e8e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0ed790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b0edbe0>]}
[0m15:52:36.487613 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:52:42.343411 | 90be9b62-93c3-4fd6-bfc9-eb35a78d11c8 ==============================
[0m15:52:42.343457 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:52:42.344840 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:52:42.345216 [debug] [MainThread]: Tracking: tracking
[0m15:52:42.362923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1143976a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114397a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114397520>]}
[0m15:52:42.418658 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:52:42.418905 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:52:42.425678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '90be9b62-93c3-4fd6-bfc9-eb35a78d11c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11453f0d0>]}
[0m15:52:42.433422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '90be9b62-93c3-4fd6-bfc9-eb35a78d11c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11448e220>]}
[0m15:52:42.433823 [info ] [MainThread]: Found 6 models, 5 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:52:42.434114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90be9b62-93c3-4fd6-bfc9-eb35a78d11c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11448e100>]}
[0m15:52:42.436143 [info ] [MainThread]: 
[0m15:52:42.436740 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:52:42.437917 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m15:52:42.449185 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:52:42.449540 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m15:52:42.449751 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m15:52:42.449899 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:52:47.580662 [debug] [ThreadPool]: SQL status: OK in 5.13 seconds
[0m15:52:47.609310 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m15:52:47.609784 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:52:47.610105 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m15:52:48.200643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90be9b62-93c3-4fd6-bfc9-eb35a78d11c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1143727c0>]}
[0m15:52:48.201429 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:52:48.201785 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:52:48.202623 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:52:48.203278 [info ] [MainThread]: 
[0m15:52:48.211463 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315
[0m15:52:48.212080 [info ] [Thread-1  ]: 1 of 5 START test accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed  [RUN]
[0m15:52:48.212809 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315"
[0m15:52:48.213082 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315
[0m15:52:48.213320 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315
[0m15:52:48.233824 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315"
[0m15:52:48.235176 [debug] [Thread-1  ]: finished collecting timing info
[0m15:52:48.235480 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315
[0m15:52:48.254777 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315"
[0m15:52:48.255464 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:52:48.255700 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315"
[0m15:52:48.255883 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop_dbt.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'completed','skipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m15:52:48.256049 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:00.578995 [debug] [Thread-1  ]: SQL status: OK in 12.32 seconds
[0m15:53:01.038180 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:01.039106 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315: ROLLBACK
[0m15:53:01.039771 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:01.040212 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315: Close
[0m15:53:01.494747 [error] [Thread-1  ]: 1 of 5 FAIL 1 accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed  [[31mFAIL 1[0m in 13.28s]
[0m15:53:01.496510 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed.85092ca315
[0m15:53:01.497580 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:01.498621 [info ] [Thread-1  ]: 2 of 5 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m15:53:01.499886 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:01.500446 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:01.500797 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:01.517515 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:01.518145 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:01.518386 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:01.521245 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:01.521814 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:01.522013 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:01.522184 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop_dbt.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m15:53:01.522352 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:03.393862 [debug] [Thread-1  ]: SQL status: OK in 1.87 seconds
[0m15:53:03.397350 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:03.397627 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m15:53:03.397803 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:03.398167 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m15:53:03.847130 [info ] [Thread-1  ]: 2 of 5 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 2.35s]
[0m15:53:03.848039 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:03.848504 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:03.848951 [info ] [Thread-1  ]: 3 of 5 START test not_null_stg_orders_order_id ................................. [RUN]
[0m15:53:03.849625 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:03.849987 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:03.850239 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:03.858164 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:03.858887 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:03.859240 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:03.861532 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:03.862279 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:03.862686 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:03.862948 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop_dbt.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m15:53:03.863163 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:05.601869 [debug] [Thread-1  ]: SQL status: OK in 1.74 seconds
[0m15:53:05.608120 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:05.608805 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m15:53:05.609307 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:05.609625 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m15:53:06.072947 [info ] [Thread-1  ]: 3 of 5 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 2.22s]
[0m15:53:06.073920 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:06.074346 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:06.074749 [info ] [Thread-1  ]: 4 of 5 START test unique_stg_customers_customer_id ............................. [RUN]
[0m15:53:06.076196 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:06.076598 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:06.076907 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:06.090408 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:06.091333 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:06.091829 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:06.095475 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:06.096136 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:06.096376 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:06.096545 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:53:06.096695 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:08.625702 [debug] [Thread-1  ]: SQL status: OK in 2.53 seconds
[0m15:53:08.631060 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:08.631560 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m15:53:08.631894 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:08.632218 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m15:53:09.086069 [info ] [Thread-1  ]: 4 of 5 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 3.01s]
[0m15:53:09.087570 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:09.088631 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:53:09.089636 [info ] [Thread-1  ]: 5 of 5 START test unique_stg_orders_order_id ................................... [RUN]
[0m15:53:09.091425 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:53:09.092018 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:53:09.092349 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:53:09.101125 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:53:09.101826 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:09.102118 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:53:09.105132 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:53:09.105822 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:09.106375 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:53:09.106649 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:53:09.106847 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:10.977661 [debug] [Thread-1  ]: SQL status: OK in 1.87 seconds
[0m15:53:10.981690 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:10.982042 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m15:53:10.982283 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:10.982500 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m15:53:11.432708 [info ] [Thread-1  ]: 5 of 5 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 2.34s]
[0m15:53:11.433348 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:53:11.435356 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:53:11.435685 [debug] [MainThread]: On master: ROLLBACK
[0m15:53:11.435967 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:53:11.902225 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:53:11.902644 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:11.902887 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:53:11.903035 [debug] [MainThread]: On master: ROLLBACK
[0m15:53:11.903165 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:53:11.903287 [debug] [MainThread]: On master: Close
[0m15:53:12.355989 [info ] [MainThread]: 
[0m15:53:12.356484 [info ] [MainThread]: Finished running 5 tests in 0 hours 0 minutes and 29.92 seconds (29.92s).
[0m15:53:12.356812 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:53:12.357154 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m15:53:12.369925 [info ] [MainThread]: 
[0m15:53:12.370402 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m15:53:12.370953 [info ] [MainThread]: 
[0m15:53:12.371261 [error] [MainThread]: [31mFailure in test accepted_values_stg_orders_status__completed__skipped__returned__return_pending__placed (models/staging/jaffle_shop/stg_jaffle_shop.yml)[0m
[0m15:53:12.371540 [error] [MainThread]:   Got 1 result, configured to fail if != 0
[0m15:53:12.371795 [info ] [MainThread]: 
[0m15:53:12.372066 [info ] [MainThread]:   compiled SQL at target/compiled/jaffle_shop/models/staging/jaffle_shop/stg_jaffle_shop.yml/accepted_values_stg_orders_4042e6062551b36b74c2b61215fa9d87.sql
[0m15:53:12.372397 [info ] [MainThread]: 
[0m15:53:12.372719 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=1 SKIP=0 TOTAL=5
[0m15:53:12.373184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11464a6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1146bfa90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1145dad90>]}
[0m15:53:12.373437 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:53:41.964582 | 15c3ccfd-84d5-4443-b046-656f270e26ed ==============================
[0m15:53:41.964648 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:53:41.965784 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:53:41.966046 [debug] [MainThread]: Tracking: tracking
[0m15:53:41.983273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c26b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c265e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c26370>]}
[0m15:53:42.038349 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:53:42.039090 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:53:42.054129 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:53:42.124390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15c3ccfd-84d5-4443-b046-656f270e26ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114e580d0>]}
[0m15:53:42.134099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15c3ccfd-84d5-4443-b046-656f270e26ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114da6e50>]}
[0m15:53:42.134535 [info ] [MainThread]: Found 6 models, 5 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:53:42.135012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15c3ccfd-84d5-4443-b046-656f270e26ed', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114da6e20>]}
[0m15:53:42.135804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114da6fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114da6fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114da6a60>]}
[0m15:53:42.136034 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:53:47.847152 | 26c5b6a6-7ec1-4404-acb4-1cb5a8ef8d96 ==============================
[0m15:53:47.847231 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:53:47.848878 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:53:47.849336 [debug] [MainThread]: Tracking: tracking
[0m15:53:47.867637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122edfe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122edfe80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122edfa30>]}
[0m15:53:47.926491 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:53:47.926811 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:53:47.938526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '26c5b6a6-7ec1-4404-acb4-1cb5a8ef8d96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1230860d0>]}
[0m15:53:47.948461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '26c5b6a6-7ec1-4404-acb4-1cb5a8ef8d96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12305fdc0>]}
[0m15:53:47.949153 [info ] [MainThread]: Found 6 models, 5 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:53:47.949564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26c5b6a6-7ec1-4404-acb4-1cb5a8ef8d96', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fd2070>]}
[0m15:53:47.950985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fd2130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12305fb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12305f940>]}
[0m15:53:47.951408 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:53:50.831223 | c241dd50-703c-4e35-abfc-3929189698a9 ==============================
[0m15:53:50.831315 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:53:50.832770 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:53:50.833272 [debug] [MainThread]: Tracking: tracking
[0m15:53:50.855329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d48d2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d48d370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d48d160>]}
[0m15:53:50.916159 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:53:50.916409 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:53:50.923496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c241dd50-703c-4e35-abfc-3929189698a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1237200d0>]}
[0m15:53:50.932728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c241dd50-703c-4e35-abfc-3929189698a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12366b2b0>]}
[0m15:53:50.933127 [info ] [MainThread]: Found 6 models, 5 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:53:50.933454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c241dd50-703c-4e35-abfc-3929189698a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12366b190>]}
[0m15:53:50.935583 [info ] [MainThread]: 
[0m15:53:50.936360 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:53:50.937837 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m15:53:50.954407 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:50.955063 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m15:53:50.955352 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m15:53:50.955633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:53:52.306741 [debug] [ThreadPool]: SQL status: OK in 1.35 seconds
[0m15:53:52.314377 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m15:53:52.314629 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:53:52.314794 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m15:53:52.766710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c241dd50-703c-4e35-abfc-3929189698a9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235a99a0>]}
[0m15:53:52.767170 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:52.767340 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:53:52.767823 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:53:52.768126 [info ] [MainThread]: 
[0m15:53:52.771029 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:53:52.771481 [info ] [Thread-1  ]: 1 of 5 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m15:53:52.772029 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:53:52.772253 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:53:52.772434 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:53:52.793167 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:53:52.793877 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:52.794303 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:53:52.814590 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:53:52.815241 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:52.815489 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:53:52.815625 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop_dbt.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m15:53:52.815750 [debug] [Thread-1  ]: Opening a new connection, currently in state closed


============================== 2022-10-14 15:53:53.034047 | 272c593a-6e62-487a-a284-3012baaa5ca9 ==============================
[0m15:53:53.034122 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:53:53.035679 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:53:53.035962 [debug] [MainThread]: Tracking: tracking
[0m15:53:53.056787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b8d2040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b8d2df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b8d29d0>]}
[0m15:53:53.111341 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:53:53.111638 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:53:53.118425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '272c593a-6e62-487a-a284-3012baaa5ca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba790d0>]}
[0m15:53:53.126928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '272c593a-6e62-487a-a284-3012baaa5ca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9c7160>]}
[0m15:53:53.127398 [info ] [MainThread]: Found 6 models, 5 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:53:53.127681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '272c593a-6e62-487a-a284-3012baaa5ca9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9c7040>]}
[0m15:53:53.128513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b9c70d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba51be0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ba519a0>]}
[0m15:53:53.128759 [debug] [MainThread]: Flushing usage events
[0m15:53:54.244710 [debug] [Thread-1  ]: SQL status: OK in 1.43 seconds
[0m15:53:54.255280 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:54.255767 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: ROLLBACK
[0m15:53:54.256199 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:54.256516 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: Close
[0m15:53:54.704424 [info ] [Thread-1  ]: 1 of 5 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 1.93s]
[0m15:53:54.705656 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:53:54.706739 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:54.707662 [info ] [Thread-1  ]: 2 of 5 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m15:53:54.708969 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:54.709536 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:54.710074 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:54.724193 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:54.725010 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:54.725316 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:54.728926 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:54.729603 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:54.729975 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:53:54.730153 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop_dbt.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m15:53:54.730311 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:55.964872 [debug] [Thread-1  ]: SQL status: OK in 1.23 seconds
[0m15:53:55.969700 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:55.970097 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m15:53:55.970358 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:55.970605 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m15:53:56.431583 [info ] [Thread-1  ]: 2 of 5 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.72s]
[0m15:53:56.433286 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:53:56.434510 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:56.435630 [info ] [Thread-1  ]: 3 of 5 START test not_null_stg_orders_order_id ................................. [RUN]
[0m15:53:56.437672 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:56.438199 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:56.438896 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:56.450174 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:56.451131 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:56.451466 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:56.455326 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:56.456439 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:56.456964 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m15:53:56.457209 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop_dbt.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m15:53:56.457387 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:57.685631 [debug] [Thread-1  ]: SQL status: OK in 1.23 seconds
[0m15:53:57.690225 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:57.690597 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m15:53:57.690831 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:57.691033 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m15:53:58.142796 [info ] [Thread-1  ]: 3 of 5 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.71s]
[0m15:53:58.143366 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m15:53:58.143704 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:58.144242 [info ] [Thread-1  ]: 4 of 5 START test unique_stg_customers_customer_id ............................. [RUN]
[0m15:53:58.144800 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:58.144972 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:58.145253 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:58.155665 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:58.156271 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:58.156571 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:53:58.159164 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:58.159722 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:53:58.159936 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m15:53:58.160062 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:53:58.160186 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:53:59.668634 [debug] [Thread-1  ]: SQL status: OK in 1.51 seconds
[0m15:53:59.672642 [debug] [Thread-1  ]: finished collecting timing info
[0m15:53:59.672995 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m15:53:59.673196 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:53:59.673372 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m15:54:00.285342 [info ] [Thread-1  ]: 4 of 5 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 2.14s]
[0m15:54:00.286691 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m15:54:00.287774 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:54:00.288698 [info ] [Thread-1  ]: 5 of 5 START test unique_stg_orders_order_id ................................... [RUN]
[0m15:54:00.290150 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:54:00.290860 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:54:00.291210 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:54:00.300523 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:54:00.301316 [debug] [Thread-1  ]: finished collecting timing info
[0m15:54:00.301639 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:54:00.305123 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:54:00.306140 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:00.306588 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m15:54:00.306972 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:54:00.307220 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:54:02.435195 [debug] [Thread-1  ]: SQL status: OK in 2.13 seconds
[0m15:54:02.442088 [debug] [Thread-1  ]: finished collecting timing info
[0m15:54:02.442603 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m15:54:02.442958 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:54:02.443257 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m15:54:03.250202 [info ] [Thread-1  ]: 5 of 5 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 2.96s]
[0m15:54:03.251252 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m15:54:03.254280 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:54:03.254978 [debug] [MainThread]: On master: ROLLBACK
[0m15:54:03.255504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:54:04.171672 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:54:04.172191 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:54:04.172452 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:54:04.172701 [debug] [MainThread]: On master: ROLLBACK
[0m15:54:04.172941 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:54:04.173158 [debug] [MainThread]: On master: Close
[0m15:54:04.958541 [info ] [MainThread]: 
[0m15:54:04.959563 [info ] [MainThread]: Finished running 5 tests in 0 hours 0 minutes and 14.02 seconds (14.02s).
[0m15:54:04.960594 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:54:04.961161 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m15:54:04.976631 [info ] [MainThread]: 
[0m15:54:04.977112 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:54:04.977551 [info ] [MainThread]: 
[0m15:54:04.977902 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m15:54:04.978625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d48d1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12382a4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12382a610>]}
[0m15:54:04.979375 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:57:49.209654 | bb56b16c-332b-4eff-8fcd-cf3b320b4079 ==============================
[0m15:57:49.209749 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:57:49.210714 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:57:49.210972 [debug] [MainThread]: Tracking: tracking
[0m15:57:49.231307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f14d250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f14d220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f14d0d0>]}
[0m15:57:49.262793 [info ] [MainThread]: Unable to do partial parsing because config vars, config profile, or config target have changed
[0m15:57:49.263205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bb56b16c-332b-4eff-8fcd-cf3b320b4079', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f138ac0>]}
[0m15:57:49.289192 [debug] [MainThread]: Parsing macros/statement.sql
[0m15:57:49.292592 [debug] [MainThread]: Parsing macros/catalog.sql
[0m15:57:49.295461 [debug] [MainThread]: Parsing macros/adapters.sql
[0m15:57:49.319839 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m15:57:49.329102 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m15:57:49.329869 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m15:57:49.333950 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m15:57:49.355477 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m15:57:49.362201 [debug] [MainThread]: Parsing macros/adapters.sql
[0m15:57:49.411598 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m15:57:49.414990 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m15:57:49.424587 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m15:57:49.425305 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m15:57:49.429100 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m15:57:49.456191 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m15:57:49.461367 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m15:57:49.469552 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m15:57:49.476687 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m15:57:49.477609 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m15:57:49.478971 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m15:57:49.484815 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m15:57:49.487110 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m15:57:49.500440 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m15:57:49.501149 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m15:57:49.501752 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m15:57:49.503429 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m15:57:49.507829 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m15:57:49.510208 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m15:57:49.511744 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m15:57:49.529861 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m15:57:49.544428 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m15:57:49.557382 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m15:57:49.561885 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m15:57:49.563832 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m15:57:49.565565 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m15:57:49.569622 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m15:57:49.587148 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m15:57:49.588964 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m15:57:49.601366 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m15:57:49.639801 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m15:57:49.646089 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m15:57:49.649651 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m15:57:49.656181 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m15:57:49.658504 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m15:57:49.662791 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m15:57:49.666032 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m15:57:49.673192 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m15:57:49.693730 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m15:57:49.695515 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m15:57:49.698341 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m15:57:49.700267 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m15:57:49.701247 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m15:57:49.702633 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m15:57:49.703471 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m15:57:49.705022 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m15:57:49.709869 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m15:57:49.717837 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m15:57:49.718689 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m15:57:49.720076 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m15:57:49.721071 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m15:57:49.722172 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m15:57:49.723534 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m15:57:49.724511 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m15:57:49.725581 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m15:57:49.726853 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m15:57:49.729415 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m15:57:49.731040 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m15:57:49.732526 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m15:57:49.733836 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m15:57:49.735085 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m15:57:49.736240 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m15:57:49.737558 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m15:57:49.738552 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m15:57:49.744840 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m15:57:49.746017 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m15:57:49.748023 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m15:57:49.750156 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m15:57:49.751447 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m15:57:49.754227 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m15:57:49.757576 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m15:57:49.771609 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m15:57:49.774663 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m15:57:49.789192 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m15:57:49.795505 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m15:57:49.803666 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m15:57:49.813652 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m15:57:50.208349 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m15:57:50.220399 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m15:57:50.223497 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:57:50.226840 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m15:57:50.230403 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m15:57:50.233991 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m15:57:50.352444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bb56b16c-332b-4eff-8fcd-cf3b320b4079', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f376f40>]}
[0m15:57:50.361103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bb56b16c-332b-4eff-8fcd-cf3b320b4079', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f197b80>]}
[0m15:57:50.361458 [info ] [MainThread]: Found 6 models, 6 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:57:50.361728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb56b16c-332b-4eff-8fcd-cf3b320b4079', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f3394f0>]}
[0m15:57:50.362541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f1c07c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f14d2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f2b8d90>]}
[0m15:57:50.362823 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:57:55.569256 | 59c68d88-b90f-4e58-9ab2-a221f3d1a6fa ==============================
[0m15:57:55.569363 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:57:55.570529 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:57:55.570758 [debug] [MainThread]: Tracking: tracking
[0m15:57:55.590708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117463f70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117463220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1174630d0>]}
[0m15:57:55.652766 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:57:55.653107 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:57:55.663319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59c68d88-b90f-4e58-9ab2-a221f3d1a6fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11761f0d0>]}
[0m15:57:55.672170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59c68d88-b90f-4e58-9ab2-a221f3d1a6fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1175642e0>]}
[0m15:57:55.672604 [info ] [MainThread]: Found 6 models, 6 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:57:55.672876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59c68d88-b90f-4e58-9ab2-a221f3d1a6fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117564190>]}
[0m15:57:55.673703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117564340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117564190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117564070>]}
[0m15:57:55.673940 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:58:11.245735 | 621d4ebb-6470-4f1a-bf17-107dd58e8618 ==============================
[0m15:58:11.245814 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:11.246995 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:11.247289 [debug] [MainThread]: Tracking: tracking
[0m15:58:11.265820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a33b040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a33bca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a33b9d0>]}
[0m15:58:11.323035 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:58:11.323870 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:58:11.338924 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:58:11.398069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4de5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a52c100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a4be4f0>]}
[0m15:58:11.398479 [debug] [MainThread]: Flushing usage events
[0m15:58:12.072373 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test arguments must be dict, got <class 'NoneType'> (value None)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:58:12.077704 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 319, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test arguments must be dict, got <class 'NoneType'> (value None)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test arguments must be dict, got <class 'NoneType'> (value None)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:58:15.092815 | 860f7db4-b4e9-49ca-b652-4ba7ee07d673 ==============================
[0m15:58:15.092924 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:15.094348 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:15.094626 [debug] [MainThread]: Tracking: tracking
[0m15:58:15.111854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f1ae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f1a580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f1a2b0>]}
[0m15:58:15.167435 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:58:15.168122 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:58:15.187491 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:58:15.256628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a1076a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0a0ca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0a05e0>]}
[0m15:58:15.256977 [debug] [MainThread]: Flushing usage events
[0m15:58:15.849510 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test arguments must be dict, got <class 'NoneType'> (value None)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:58:15.853474 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 319, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test arguments must be dict, got <class 'NoneType'> (value None)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test arguments must be dict, got <class 'NoneType'> (value None)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:58:23.186475 | d295a25c-a5fd-4b03-b9c2-34ed2e1f5f2c ==============================
[0m15:58:23.186587 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:23.187878 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:23.188390 [debug] [MainThread]: Tracking: tracking
[0m15:58:23.209431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1179eeb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1179ee5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1179ee370>]}
[0m15:58:23.265620 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:58:23.266305 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:58:23.279916 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:58:23.339386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b93d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117be0af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117b82820>]}
[0m15:58:23.339770 [debug] [MainThread]: Flushing usage events
[0m15:58:23.940796 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', None)] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:58:23.944357 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 312, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', None)] instead (3 keys)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', None)] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:58:35.880948 | 61468826-6b97-4027-86c4-bcf777551c82 ==============================
[0m15:58:35.881017 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:35.882608 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:35.883072 [debug] [MainThread]: Tracking: tracking
[0m15:58:35.905419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183ef040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183ef820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183efa90>]}
[0m15:58:35.964009 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:58:35.964718 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:58:35.978395 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:58:36.038658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118593400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185e0ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118576b20>]}
[0m15:58:36.039007 [debug] [MainThread]: Flushing usage events
[0m15:58:36.696910 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:58:36.698901 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 312, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', 'customer_id')] instead (3 keys)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:58:37.978002 | e0fd5e0d-1c3f-4474-b545-2aeca6846e1b ==============================
[0m15:58:37.978049 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:37.979376 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:37.979801 [debug] [MainThread]: Tracking: tracking
[0m15:58:38.000018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d57ab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d57adf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d57a430>]}
[0m15:58:38.054615 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:58:38.055502 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:58:38.068206 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:58:38.128714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d71e5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d76ec40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6ff310>]}
[0m15:58:38.129055 [debug] [MainThread]: Flushing usage events
[0m15:58:38.721631 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:58:38.724290 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 312, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', 'customer_id')] instead (3 keys)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', None), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:58:55.925475 | 3a680ae4-b717-477d-8624-01280a35e01d ==============================
[0m15:58:55.925554 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:55.926547 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:55.927050 [debug] [MainThread]: Tracking: tracking
[0m15:58:55.946585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236df040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236dfdf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236df9d0>]}
[0m15:58:55.987378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123854430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123854fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123854dc0>]}
[0m15:58:56.002670 [debug] [MainThread]: Flushing usage events
[0m15:58:56.675857 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 32
    ------------------------------
    29 |       tests:
    30 |         - relationships:
    31 |           to: {{  ref('stg_customers')
    32 |           field: customer_id
    
    Raw Error:
    ------------------------------
    while parsing a flow mapping
      in "<unicode string>", line 31, column 16
    did not find expected ',' or '}'
      in "<unicode string>", line 32, column 16
[0m15:58:56.678267 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 844, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 847, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a flow mapping
  in "<unicode string>", line 31, column 16
did not find expected ',' or '}'
  in "<unicode string>", line 32, column 16

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 32
  ------------------------------
  29 |       tests:
  30 |         - relationships:
  31 |           to: {{  ref('stg_customers')
  32 |           field: customer_id
  
  Raw Error:
  ------------------------------
  while parsing a flow mapping
    in "<unicode string>", line 31, column 16
  did not find expected ',' or '}'
    in "<unicode string>", line 32, column 16

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 32
    ------------------------------
    29 |       tests:
    30 |         - relationships:
    31 |           to: {{  ref('stg_customers')
    32 |           field: customer_id
    
    Raw Error:
    ------------------------------
    while parsing a flow mapping
      in "<unicode string>", line 31, column 16
    did not find expected ',' or '}'
      in "<unicode string>", line 32, column 16



============================== 2022-10-14 15:58:58.405769 | a0300229-e0b6-445b-b9ce-aa8a10ce8ba4 ==============================
[0m15:58:58.405851 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:58:58.407196 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:58:58.407653 [debug] [MainThread]: Tracking: tracking
[0m15:58:58.425745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b53dc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b53d820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b53da90>]}
[0m15:58:58.470049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b6b3400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b6b3fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b6b3d60>]}
[0m15:58:58.470375 [debug] [MainThread]: Flushing usage events
[0m15:58:59.120570 [error] [MainThread]: Encountered an error:
Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 32
    ------------------------------
    29 |       tests:
    30 |         - relationships:
    31 |           to: {{  ref('stg_customers')
    32 |           field: customer_id
    
    Raw Error:
    ------------------------------
    while parsing a flow mapping
      in "<unicode string>", line 31, column 16
    did not find expected ',' or '}'
      in "<unicode string>", line 32, column 16
[0m15:58:59.124119 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    return safe_load(contents)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 51, in safe_load
    return yaml.load(contents, Loader=SafeLoader)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "yaml/_yaml.pyx", line 673, in yaml._yaml.CParser.get_single_node
  File "yaml/_yaml.pyx", line 687, in yaml._yaml.CParser._compose_document
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 729, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 806, in yaml._yaml.CParser._compose_sequence_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 845, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 844, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 731, in yaml._yaml.CParser._compose_node
  File "yaml/_yaml.pyx", line 847, in yaml._yaml.CParser._compose_mapping_node
  File "yaml/_yaml.pyx", line 860, in yaml._yaml.CParser._parse_next_event
yaml.parser.ParserError: while parsing a flow mapping
  in "<unicode string>", line 31, column 16
did not find expected ',' or '}'
  in "<unicode string>", line 32, column 16

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 118, in yaml_from_file
    return load_yaml_text(source_file.contents, source_file.path)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/clients/yaml_helper.py", line 63, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 32
  ------------------------------
  29 |       tests:
  30 |         - relationships:
  31 |           to: {{  ref('stg_customers')
  32 |           field: customer_id
  
  Raw Error:
  ------------------------------
  while parsing a flow mapping
    in "<unicode string>", line 31, column 16
  did not find expected ',' or '}'
    in "<unicode string>", line 32, column 16

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 245, in load
    read_files(project, self.manifest.files, project_parser_files, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 182, in read_files
    project_files["SchemaParser"] = read_files_for_parser(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 134, in read_files_for_parser
    source_files = get_source_files(project, dirs, extension, parse_ft, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 125, in get_source_files
    file = load_source_file(fp, parse_file_type, project.project_name, saved_files)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/read_files.py", line 55, in load_source_file
    dfy = yaml_from_file(source_file)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 121, in yaml_from_file
    raise ParsingException(
dbt.exceptions.ParsingException: Parsing Error
  Error reading jaffle_shop: staging/jaffle_shop/stg_jaffle_shop.yml - Runtime Error
    Syntax error near line 32
    ------------------------------
    29 |       tests:
    30 |         - relationships:
    31 |           to: {{  ref('stg_customers')
    32 |           field: customer_id
    
    Raw Error:
    ------------------------------
    while parsing a flow mapping
      in "<unicode string>", line 31, column 16
    did not find expected ',' or '}'
      in "<unicode string>", line 32, column 16



============================== 2022-10-14 15:59:02.856948 | a63d72aa-083c-4ebc-8c71-9a438487b4a8 ==============================
[0m15:59:02.856996 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:02.858449 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:59:02.858905 [debug] [MainThread]: Tracking: tracking
[0m15:59:02.875673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c4be20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c4b820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c4b610>]}
[0m15:59:02.927821 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:59:02.928608 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:59:02.943330 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:59:02.998939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118e36670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118dd3940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118dd3f10>]}
[0m15:59:02.999346 [debug] [MainThread]: Flushing usage events
[0m15:59:03.615112 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:59:03.618197 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 312, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:59:07.029559 | b6131c24-68de-4243-8742-436753df11eb ==============================
[0m15:59:07.029617 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:07.030781 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:59:07.031381 [debug] [MainThread]: Tracking: tracking
[0m15:59:07.050887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dedfe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dedf8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dedfaf0>]}
[0m15:59:07.104102 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:59:07.104781 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:59:07.118097 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:59:07.178278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e083610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e0d0b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11e066d60>]}
[0m15:59:07.178921 [debug] [MainThread]: Flushing usage events
[0m15:59:07.812669 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:59:07.815617 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 312, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:59:13.085178 | a632da01-ffed-480f-9ce3-6348e2b17f3e ==============================
[0m15:59:13.085233 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:13.086443 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:59:13.086951 [debug] [MainThread]: Tracking: tracking
[0m15:59:13.108793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b71eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b71d00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114b71160>]}
[0m15:59:13.168234 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:59:13.168894 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:59:13.183305 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:59:13.238905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d1e7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d6c220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114d0d610>]}
[0m15:59:13.239392 [debug] [MainThread]: Flushing usage events
[0m15:59:14.051284 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)
[0m15:59:14.055350 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 288, in _parse_generic_test
    builder = TestBuilder(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 222, in __init__
    test_name, test_args = self.extract_test_args(test, column_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/generic_test_builders.py", line 312, in extract_test_args
    raise_parsing_error(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 449, in raise_parsing_error
    raise ParsingException(msg, node)
dbt.exceptions.ParsingException: Parsing Error
  test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 366, in load
    self.parse_project(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 469, in parse_project
    parser.parse_file(block, dct=dct)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 504, in parse_file
    self.parse_tests(test_block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 480, in parse_tests
    self.parse_column_tests(block, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 210, in parse_column_tests
    self.parse_test(block, test, column)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 476, in parse_test
    self.parse_node(block)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 417, in parse_node
    node = self._parse_generic_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/schemas.py", line 304, in _parse_generic_test
    raise ParsingException(msg) from exc
dbt.exceptions.ParsingException: Parsing Error
  Invalid test config given in models/staging/jaffle_shop/stg_jaffle_shop.yml:
  	test definition dictionary must have exactly one key, got [('relationships', None), ('to', "ref('stg_customers')"), ('field', 'customer_id')] instead (3 keys)
  	@: UnparsedNodeUpdate(original_file_path='model...ne)



============================== 2022-10-14 15:59:54.262158 | 97179fad-56b0-4e43-9fb3-3b080d6fcc00 ==============================
[0m15:59:54.262215 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:54.263245 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m15:59:54.263549 [debug] [MainThread]: Tracking: tracking
[0m15:59:54.279984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114301e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1143015b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114301370>]}
[0m15:59:54.351511 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:59:54.352225 [debug] [MainThread]: Partial parsing: update schema file: jaffle_shop://models/staging/jaffle_shop/stg_jaffle_shop.yml
[0m15:59:54.367671 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m15:59:54.454362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97179fad-56b0-4e43-9fb3-3b080d6fcc00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1145310d0>]}
[0m15:59:54.463755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97179fad-56b0-4e43-9fb3-3b080d6fcc00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11447db50>]}
[0m15:59:54.464343 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:59:54.464659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97179fad-56b0-4e43-9fb3-3b080d6fcc00', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11447dbb0>]}
[0m15:59:54.465478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11447dca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11447dc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11447d730>]}
[0m15:59:54.465728 [debug] [MainThread]: Flushing usage events


============================== 2022-10-14 15:59:55.436141 | eeffe1d2-3daa-4261-981a-7011e106aa41 ==============================
[0m15:59:55.436200 [info ] [MainThread]: Running with dbt=1.2.1
[0m15:59:55.437468 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'select': ['models/staging/jaffle_shop/stg_jaffle_shop.yml'], 'which': 'test', 'rpc_method': 'test'}
[0m15:59:55.437962 [debug] [MainThread]: Tracking: tracking
[0m15:59:55.459344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ffe62b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ffe6b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ffe6160>]}
[0m15:59:55.513845 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:59:55.514085 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:59:55.520807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eeffe1d2-3daa-4261-981a-7011e106aa41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12019d0d0>]}
[0m15:59:55.528998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eeffe1d2-3daa-4261-981a-7011e106aa41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200e3640>]}
[0m15:59:55.529340 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m15:59:55.529666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eeffe1d2-3daa-4261-981a-7011e106aa41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200e38b0>]}
[0m15:59:55.532161 [info ] [MainThread]: 
[0m15:59:55.532861 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m15:59:55.534130 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m15:59:55.546571 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:55.546853 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m15:59:55.547017 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m15:59:55.547208 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:59:57.465239 [debug] [ThreadPool]: SQL status: OK in 1.92 seconds
[0m15:59:57.476526 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m15:59:57.476974 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:59:57.477230 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m15:59:57.932974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eeffe1d2-3daa-4261-981a-7011e106aa41', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200e31c0>]}
[0m15:59:57.934167 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:57.934648 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:59:57.935535 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:59:57.936398 [info ] [MainThread]: 
[0m15:59:57.945842 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:59:57.946655 [info ] [Thread-1  ]: 1 of 6 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m15:59:57.948075 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:59:57.948583 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:59:57.948943 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:59:57.972983 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:59:57.973570 [debug] [Thread-1  ]: finished collecting timing info
[0m15:59:57.973819 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:59:57.992418 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:59:57.993104 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:57.993390 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m15:59:57.993533 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop_dbt.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m15:59:57.993680 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m15:59:59.410129 [debug] [Thread-1  ]: SQL status: OK in 1.42 seconds
[0m15:59:59.417767 [debug] [Thread-1  ]: finished collecting timing info
[0m15:59:59.418193 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: ROLLBACK
[0m15:59:59.418457 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m15:59:59.418690 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: Close
[0m15:59:59.882809 [info ] [Thread-1  ]: 1 of 6 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 1.94s]
[0m15:59:59.884277 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m15:59:59.885226 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:59:59.886241 [info ] [Thread-1  ]: 2 of 6 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m15:59:59.888127 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:59:59.888859 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:59:59.889316 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:59:59.907382 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:59:59.908280 [debug] [Thread-1  ]: finished collecting timing info
[0m15:59:59.908629 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m15:59:59.911505 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:59:59.912160 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m15:59:59.912387 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m15:59:59.912583 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop_dbt.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m15:59:59.912757 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:01.194690 [debug] [Thread-1  ]: SQL status: OK in 1.28 seconds
[0m16:00:01.199154 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:01.199516 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m16:00:01.199750 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:01.199955 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m16:00:01.664203 [info ] [Thread-1  ]: 2 of 6 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.78s]
[0m16:00:01.665130 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m16:00:01.665594 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m16:00:01.666187 [info ] [Thread-1  ]: 3 of 6 START test not_null_stg_orders_order_id ................................. [RUN]
[0m16:00:01.666976 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m16:00:01.667220 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m16:00:01.667437 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m16:00:01.674547 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m16:00:01.675266 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:01.675650 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m16:00:01.678722 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m16:00:01.679369 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:01.679581 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m16:00:01.679746 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop_dbt.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m16:00:01.679902 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:02.990246 [debug] [Thread-1  ]: SQL status: OK in 1.31 seconds
[0m16:00:02.997125 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:02.997628 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m16:00:02.997946 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:02.998227 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m16:00:03.450517 [info ] [Thread-1  ]: 3 of 6 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.78s]
[0m16:00:03.452087 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m16:00:03.452992 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m16:00:03.454067 [info ] [Thread-1  ]: 4 of 6 START test relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [RUN]
[0m16:00:03.455601 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m16:00:03.456149 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m16:00:03.456488 [debug] [Thread-1  ]: Compiling test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m16:00:03.473457 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m16:00:03.474177 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:03.474433 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m16:00:03.477704 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m16:00:03.478403 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:03.478652 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m16:00:03.478841 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from jaffle_shop_dbt.stg_orders
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from jaffle_shop_dbt.stg_customers
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m16:00:03.479025 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:05.653130 [debug] [Thread-1  ]: SQL status: OK in 2.17 seconds
[0m16:00:05.659714 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:05.660186 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: ROLLBACK
[0m16:00:05.660533 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:05.661103 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: Close
[0m16:00:06.119726 [info ] [Thread-1  ]: 4 of 6 PASS relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [[32mPASS[0m in 2.66s]
[0m16:00:06.121167 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m16:00:06.121842 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m16:00:06.122731 [info ] [Thread-1  ]: 5 of 6 START test unique_stg_customers_customer_id ............................. [RUN]
[0m16:00:06.123903 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m16:00:06.124333 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m16:00:06.124709 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m16:00:06.143210 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m16:00:06.144273 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:06.144697 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m16:00:06.148263 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m16:00:06.148989 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:06.149537 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m16:00:06.149777 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:00:06.149964 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:07.622635 [debug] [Thread-1  ]: SQL status: OK in 1.47 seconds
[0m16:00:07.629348 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:07.629834 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m16:00:07.630160 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:07.630498 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m16:00:08.082441 [info ] [Thread-1  ]: 5 of 6 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.96s]
[0m16:00:08.084076 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m16:00:08.085048 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m16:00:08.086121 [info ] [Thread-1  ]: 6 of 6 START test unique_stg_orders_order_id ................................... [RUN]
[0m16:00:08.088068 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m16:00:08.088583 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m16:00:08.089131 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m16:00:08.099466 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m16:00:08.100275 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:08.100743 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m16:00:08.104018 [debug] [Thread-1  ]: Writing runtime SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m16:00:08.105077 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:08.105373 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m16:00:08.105587 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:00:08.105792 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m16:00:09.622875 [debug] [Thread-1  ]: SQL status: OK in 1.52 seconds
[0m16:00:09.629446 [debug] [Thread-1  ]: finished collecting timing info
[0m16:00:09.629965 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m16:00:09.630368 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m16:00:09.630830 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m16:00:10.078667 [info ] [Thread-1  ]: 6 of 6 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.99s]
[0m16:00:10.080326 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m16:00:10.083466 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m16:00:10.084045 [debug] [MainThread]: On master: ROLLBACK
[0m16:00:10.084601 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:00:10.555121 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:00:10.555831 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:00:10.556244 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:00:10.556640 [debug] [MainThread]: On master: ROLLBACK
[0m16:00:10.557176 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:00:10.557594 [debug] [MainThread]: On master: Close
[0m16:00:11.021735 [info ] [MainThread]: 
[0m16:00:11.022749 [info ] [MainThread]: Finished running 6 tests in 0 hours 0 minutes and 15.49 seconds (15.49s).
[0m16:00:11.023445 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:00:11.023812 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m16:00:11.043689 [info ] [MainThread]: 
[0m16:00:11.044427 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:00:11.044882 [info ] [MainThread]: 
[0m16:00:11.045247 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m16:00:11.045876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1200e38b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12028b070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120272ee0>]}
[0m16:00:11.046281 [debug] [MainThread]: Flushing usage events


============================== 2022-10-17 12:28:59.550463 | bd98ee02-82a2-4cba-a976-ed26181acb4c ==============================
[0m12:28:59.550514 [info ] [MainThread]: Running with dbt=1.2.1
[0m12:28:59.551374 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m12:28:59.551626 [debug] [MainThread]: Tracking: tracking
[0m12:28:59.582710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11623fd30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11623fd00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116237850>]}
[0m12:28:59.624613 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:28:59.625251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bd98ee02-82a2-4cba-a976-ed26181acb4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116282af0>]}
[0m12:28:59.658800 [debug] [MainThread]: Parsing macros/statement.sql
[0m12:28:59.664787 [debug] [MainThread]: Parsing macros/catalog.sql
[0m12:28:59.670535 [debug] [MainThread]: Parsing macros/adapters.sql
[0m12:28:59.701451 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m12:28:59.709670 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m12:28:59.710315 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m12:28:59.717517 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m12:28:59.743372 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m12:28:59.750074 [debug] [MainThread]: Parsing macros/adapters.sql
[0m12:28:59.812810 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m12:28:59.820714 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m12:28:59.837021 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m12:28:59.837892 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m12:28:59.841653 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m12:28:59.876247 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m12:28:59.885109 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m12:28:59.896267 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m12:28:59.906054 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m12:28:59.907165 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m12:28:59.909221 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m12:28:59.919359 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m12:28:59.923283 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m12:28:59.943170 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m12:28:59.944025 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m12:28:59.944693 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m12:28:59.946161 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m12:28:59.951478 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m12:28:59.954593 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m12:28:59.956353 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m12:28:59.986965 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m12:29:00.034057 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m12:29:00.065397 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m12:29:00.076395 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m12:29:00.080412 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m12:29:00.085040 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m12:29:00.093305 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m12:29:00.125193 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m12:29:00.128162 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m12:29:00.141045 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m12:29:00.159549 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m12:29:00.169169 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m12:29:00.175168 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m12:29:00.184313 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m12:29:00.186777 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m12:29:00.191587 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m12:29:00.194056 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m12:29:00.202039 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m12:29:00.232133 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m12:29:00.236003 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m12:29:00.239494 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m12:29:00.241690 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m12:29:00.243362 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m12:29:00.245261 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m12:29:00.246180 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m12:29:00.249183 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m12:29:00.256052 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m12:29:00.273704 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m12:29:00.275674 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m12:29:00.278478 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m12:29:00.280770 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m12:29:00.283224 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m12:29:00.285180 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m12:29:00.286394 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m12:29:00.287698 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m12:29:00.288809 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m12:29:00.291091 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m12:29:00.292261 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m12:29:00.293431 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m12:29:00.294546 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m12:29:00.295644 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m12:29:00.296557 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m12:29:00.297575 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m12:29:00.298456 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m12:29:00.305376 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m12:29:00.306402 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m12:29:00.308153 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m12:29:00.310126 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m12:29:00.311910 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m12:29:00.316657 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m12:29:00.322613 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m12:29:00.341918 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m12:29:00.345032 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m12:29:00.360443 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m12:29:00.369214 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m12:29:00.381813 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m12:29:00.398319 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m12:29:00.860248 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m12:29:00.883222 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m12:29:00.887754 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m12:29:00.892221 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m12:29:00.894971 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m12:29:00.898353 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m12:29:01.182897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bd98ee02-82a2-4cba-a976-ed26181acb4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1164680d0>]}
[0m12:29:01.195913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bd98ee02-82a2-4cba-a976-ed26181acb4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116237a60>]}
[0m12:29:01.196294 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m12:29:01.196517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bd98ee02-82a2-4cba-a976-ed26181acb4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11622e2b0>]}
[0m12:29:01.197268 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11645b9a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163eed60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163fc580>]}
[0m12:29:01.197470 [debug] [MainThread]: Flushing usage events


============================== 2022-10-18 20:14:30.108362 | 9035c38b-e83c-47ca-99ee-f668ec70e2ea ==============================
[0m20:14:30.108453 [info ] [MainThread]: Running with dbt=1.2.1
[0m20:14:30.110864 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m20:14:30.111555 [debug] [MainThread]: Tracking: tracking
[0m20:14:30.144884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11615ab50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11615a430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11615a790>]}
[0m20:14:30.301449 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:14:30.302582 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:14:30.322084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9035c38b-e83c-47ca-99ee-f668ec70e2ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1163090d0>]}
[0m20:14:30.349678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9035c38b-e83c-47ca-99ee-f668ec70e2ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116247370>]}
[0m20:14:30.351250 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m20:14:30.352176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9035c38b-e83c-47ca-99ee-f668ec70e2ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1162475e0>]}
[0m20:14:30.353551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1162475b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116247670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116247040>]}
[0m20:14:30.354310 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 17:42:31.176289 | 5a27c921-758a-4eb5-b4ec-eb897cc740cb ==============================
[0m17:42:31.176363 [info ] [MainThread]: Running with dbt=1.2.1
[0m17:42:31.177749 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m17:42:31.178287 [debug] [MainThread]: Tracking: tracking
[0m17:42:31.215515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcfcb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcfc430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcfc2b0>]}
[0m17:42:31.276471 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:42:31.277001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5a27c921-758a-4eb5-b4ec-eb897cc740cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcd6640>]}
[0m17:42:31.333425 [debug] [MainThread]: Parsing macros/statement.sql
[0m17:42:31.338895 [debug] [MainThread]: Parsing macros/catalog.sql
[0m17:42:31.345910 [debug] [MainThread]: Parsing macros/adapters.sql
[0m17:42:31.390733 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m17:42:31.411219 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m17:42:31.413111 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m17:42:31.419772 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m17:42:31.452637 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m17:42:31.468848 [debug] [MainThread]: Parsing macros/adapters.sql
[0m17:42:31.554108 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m17:42:31.564555 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m17:42:31.582548 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m17:42:31.584412 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m17:42:31.591107 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m17:42:31.644603 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m17:42:31.658180 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m17:42:31.677420 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m17:42:31.689664 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m17:42:31.691831 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m17:42:31.694607 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:42:31.714163 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:42:31.719528 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:42:31.744464 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:42:31.746292 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:42:31.748250 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:42:31.753031 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m17:42:31.765905 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m17:42:31.770554 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m17:42:31.774006 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m17:42:31.811669 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m17:42:31.834933 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m17:42:31.864003 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m17:42:31.872704 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m17:42:31.877845 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m17:42:31.882830 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m17:42:31.892641 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m17:42:31.928457 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m17:42:31.933897 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m17:42:31.952316 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m17:42:31.982183 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m17:42:31.989756 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m17:42:31.996008 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m17:42:32.007425 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m17:42:32.012538 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m17:42:32.020282 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m17:42:32.024993 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m17:42:32.038138 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m17:42:32.077734 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m17:42:32.080955 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m17:42:32.086900 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m17:42:32.091120 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m17:42:32.092835 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m17:42:32.094874 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m17:42:32.097152 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m17:42:32.101160 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m17:42:32.115992 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m17:42:32.130016 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m17:42:32.132006 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m17:42:32.135007 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m17:42:32.137417 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m17:42:32.139476 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:42:32.142165 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m17:42:32.143914 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m17:42:32.147219 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m17:42:32.151300 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:42:32.157537 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:42:32.161943 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m17:42:32.164116 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m17:42:32.166840 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m17:42:32.169220 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:42:32.171508 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m17:42:32.173390 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m17:42:32.174803 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m17:42:32.185100 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:42:32.188553 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:42:32.191491 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:42:32.194648 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m17:42:32.197210 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m17:42:32.202022 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m17:42:32.208159 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m17:42:32.237823 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m17:42:32.243270 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m17:42:32.271431 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m17:42:32.280971 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m17:42:32.293117 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m17:42:32.321496 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m17:42:33.123722 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m17:42:33.144386 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m17:42:33.149873 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m17:42:33.155771 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m17:42:33.165100 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:42:33.174362 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:42:33.402528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5a27c921-758a-4eb5-b4ec-eb897cc740cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf140d0>]}
[0m17:42:33.419551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5a27c921-758a-4eb5-b4ec-eb897cc740cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be0ba60>]}
[0m17:42:33.420059 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:42:33.420402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5a27c921-758a-4eb5-b4ec-eb897cc740cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bed85b0>]}
[0m17:42:33.421469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bf0b070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bcecfa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11be9fee0>]}
[0m17:42:33.421810 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:38:27.210622 | 7006df1c-0ecb-4d12-909e-649f15ac35fb ==============================
[0m18:38:27.210856 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:38:27.214276 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:38:27.215152 [debug] [MainThread]: Tracking: tracking
[0m18:38:27.246885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f7ae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f7ab20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f7a340>]}
[0m18:38:27.319778 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m18:38:27.320404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7006df1c-0ecb-4d12-909e-649f15ac35fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116f54bb0>]}
[0m18:38:27.393530 [debug] [MainThread]: Parsing macros/statement.sql
[0m18:38:27.401404 [debug] [MainThread]: Parsing macros/catalog.sql
[0m18:38:27.410339 [debug] [MainThread]: Parsing macros/adapters.sql
[0m18:38:27.468500 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m18:38:27.489043 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m18:38:27.491595 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m18:38:27.500003 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m18:38:27.545136 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m18:38:27.566331 [debug] [MainThread]: Parsing macros/adapters.sql
[0m18:38:27.675315 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m18:38:27.681970 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m18:38:27.712882 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m18:38:27.715840 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m18:38:27.728896 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m18:38:27.803570 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m18:38:27.822005 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m18:38:27.851043 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m18:38:27.872241 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m18:38:27.874272 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m18:38:27.877210 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m18:38:27.891905 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m18:38:27.895590 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m18:38:27.934851 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m18:38:27.936263 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m18:38:27.937430 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m18:38:27.940357 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m18:38:27.949943 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m18:38:27.956394 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m18:38:27.960635 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m18:38:28.009048 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m18:38:28.039425 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m18:38:28.068607 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m18:38:28.077471 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m18:38:28.080560 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m18:38:28.085279 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m18:38:28.100856 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m18:38:28.135476 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m18:38:28.140489 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m18:38:28.172340 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m18:38:28.201167 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m18:38:28.217005 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m18:38:28.225575 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m18:38:28.236854 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m18:38:28.239633 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m18:38:28.247383 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m18:38:28.252963 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m18:38:28.273858 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m18:38:28.326857 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m18:38:28.329734 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m18:38:28.336600 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m18:38:28.339355 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m18:38:28.341542 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m18:38:28.344426 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m18:38:28.346278 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m18:38:28.350202 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m18:38:28.363546 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m18:38:28.381395 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m18:38:28.386330 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m18:38:28.388954 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m18:38:28.391350 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m18:38:28.393911 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m18:38:28.397114 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m18:38:28.400477 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m18:38:28.406603 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m18:38:28.410675 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m18:38:28.419308 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m18:38:28.422056 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m18:38:28.425122 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m18:38:28.428761 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m18:38:28.430654 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m18:38:28.433894 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m18:38:28.436886 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m18:38:28.438924 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m18:38:28.456795 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m18:38:28.460952 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m18:38:28.466326 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m18:38:28.470082 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m18:38:28.472951 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m18:38:28.478555 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m18:38:28.489032 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m18:38:28.529745 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m18:38:28.540020 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m18:38:28.577517 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m18:38:28.591184 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m18:38:28.611316 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m18:38:28.633099 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m18:38:29.517296 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m18:38:29.540565 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m18:38:29.546861 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m18:38:29.558767 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m18:38:29.572940 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m18:38:29.580071 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:38:29.920745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7006df1c-0ecb-4d12-909e-649f15ac35fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171920d0>]}
[0m18:38:29.936547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7006df1c-0ecb-4d12-909e-649f15ac35fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11700c790>]}
[0m18:38:29.937383 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:38:29.937949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7006df1c-0ecb-4d12-909e-649f15ac35fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117147c10>]}
[0m18:38:29.939424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11718a040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117062250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117127580>]}
[0m18:38:29.940087 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:42:06.485698 | d3ba7bba-12a0-40bd-8ecb-71cbbb812a62 ==============================
[0m18:42:06.485869 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:42:06.489399 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:42:06.490267 [debug] [MainThread]: Tracking: tracking
[0m18:42:06.516831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119aaee20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119aaeeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119aae430>]}
[0m18:42:06.641721 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:42:06.642393 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:42:06.668328 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:42:06.729727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd3ba7bba-12a0-40bd-8ecb-71cbbb812a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c45a60>]}
[0m18:42:06.744570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd3ba7bba-12a0-40bd-8ecb-71cbbb812a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c25f40>]}
[0m18:42:06.745101 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:42:06.745751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd3ba7bba-12a0-40bd-8ecb-71cbbb812a62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c14130>]}
[0m18:42:06.747307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a88a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a88730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c14100>]}
[0m18:42:06.748065 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:42:18.216549 | eb15f09c-0e95-4f9a-9a4b-ff95820b62c9 ==============================
[0m18:42:18.216761 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:42:18.229881 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:42:18.230676 [debug] [MainThread]: Tracking: tracking
[0m18:42:18.264812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1157f6e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1157f6430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1157f6820>]}
[0m18:42:18.398736 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:42:18.399737 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:42:18.434259 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:42:18.502908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb15f09c-0e95-4f9a-9a4b-ff95820b62c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11598dfd0>]}
[0m18:42:18.523444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb15f09c-0e95-4f9a-9a4b-ff95820b62c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11596ed90>]}
[0m18:42:18.524222 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:42:18.524784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb15f09c-0e95-4f9a-9a4b-ff95820b62c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11595d100>]}
[0m18:42:18.526564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11595d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11596ec10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11596ebe0>]}
[0m18:42:18.527099 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:42:31.694157 | 94889ec8-5b80-40be-898a-c8c67311dd11 ==============================
[0m18:42:31.694300 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:42:31.696982 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:42:31.698176 [debug] [MainThread]: Tracking: tracking
[0m18:42:31.730079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114acefa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114ace3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114ace2b0>]}
[0m18:42:31.852870 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:42:31.854085 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:42:31.880686 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:42:31.932549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '94889ec8-5b80-40be-898a-c8c67311dd11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c640a0>]}
[0m18:42:31.947320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '94889ec8-5b80-40be-898a-c8c67311dd11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c56eb0>]}
[0m18:42:31.948387 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:42:31.949099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94889ec8-5b80-40be-898a-c8c67311dd11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c3ffd0>]}
[0m18:42:31.950874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c45070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c56c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114c569d0>]}
[0m18:42:31.951420 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:42:39.339195 | 435aff5d-4d17-4f2f-bcf3-46cfaed482c7 ==============================
[0m18:42:39.339328 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:42:39.340949 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:42:39.341653 [debug] [MainThread]: Tracking: tracking
[0m18:42:39.377152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c810790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c8102b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c810ca0>]}
[0m18:42:39.498165 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:42:39.498882 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:42:39.519276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '435aff5d-4d17-4f2f-bcf3-46cfaed482c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c9cf0d0>]}
[0m18:42:39.537498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '435aff5d-4d17-4f2f-bcf3-46cfaed482c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c90d1f0>]}
[0m18:42:39.538042 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:42:39.538571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '435aff5d-4d17-4f2f-bcf3-46cfaed482c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c90d550>]}
[0m18:42:39.540494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c90d520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c90d5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c90d0d0>]}
[0m18:42:39.541184 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:43:13.920369 | 5eb86d3e-a87e-4e9e-96d0-36406bf7dd49 ==============================
[0m18:43:13.920719 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:43:13.926441 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:43:13.926980 [debug] [MainThread]: Tracking: tracking
[0m18:43:13.953732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122029790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1220293d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1220292b0>]}
[0m18:43:14.057474 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:43:14.058618 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:43:14.098706 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:43:14.152009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5eb86d3e-a87e-4e9e-96d0-36406bf7dd49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221c0b80>]}
[0m18:43:14.167679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5eb86d3e-a87e-4e9e-96d0-36406bf7dd49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221b1eb0>]}
[0m18:43:14.168213 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:43:14.168735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5eb86d3e-a87e-4e9e-96d0-36406bf7dd49', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12219afd0>]}
[0m18:43:14.170437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221a0070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221b1c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1221b19d0>]}
[0m18:43:14.170933 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:43:22.051502 | 3d0b87d7-f387-4d8a-96fb-2d0806167c3d ==============================
[0m18:43:22.051584 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:43:22.053461 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:43:22.054262 [debug] [MainThread]: Tracking: tracking
[0m18:43:22.091810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fefde20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fefd040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fefda90>]}
[0m18:43:22.219806 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:43:22.221473 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:43:22.251119 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:43:22.311378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d0b87d7-f387-4d8a-96fb-2d0806167c3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120095ca0>]}
[0m18:43:22.329154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d0b87d7-f387-4d8a-96fb-2d0806167c3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120074dc0>]}
[0m18:43:22.329845 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:43:22.330370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d0b87d7-f387-4d8a-96fb-2d0806167c3d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120063130>]}
[0m18:43:22.332002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fed6610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fed6730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120063100>]}
[0m18:43:22.332561 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:43:27.555023 | bf5fced2-5529-4b6e-8325-e74c6c03ac27 ==============================
[0m18:43:27.555109 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:43:27.558272 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:43:27.559102 [debug] [MainThread]: Tracking: tracking
[0m18:43:27.613231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12131eb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12131e5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12131e850>]}
[0m18:43:27.760594 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:43:27.761247 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:43:27.774745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bf5fced2-5529-4b6e-8325-e74c6c03ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214cd0d0>]}
[0m18:43:27.794409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bf5fced2-5529-4b6e-8325-e74c6c03ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12140b370>]}
[0m18:43:27.795193 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:43:27.797037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf5fced2-5529-4b6e-8325-e74c6c03ac27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12140b5e0>]}
[0m18:43:27.800512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12140b5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12140b670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12140b040>]}
[0m18:43:27.801509 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:43:33.966882 | e51f987d-c25b-42c6-8c1a-d0d180dd4940 ==============================
[0m18:43:33.966986 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:43:33.969377 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:43:33.969879 [debug] [MainThread]: Tracking: tracking
[0m18:43:34.006505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd6afd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd6a2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd6aca0>]}
[0m18:43:34.136108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:43:34.137463 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:43:34.172948 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:43:34.231894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e51f987d-c25b-42c6-8c1a-d0d180dd4940', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11ff0ffa0>]}
[0m18:43:34.252690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e51f987d-c25b-42c6-8c1a-d0d180dd4940', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fef2f70>]}
[0m18:43:34.253410 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:43:34.254788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e51f987d-c25b-42c6-8c1a-d0d180dd4940', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fee10d0>]}
[0m18:43:34.257921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fee10a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fef2b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fef2be0>]}
[0m18:43:34.258967 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:43:41.848136 | 98cfa2f0-06cc-4439-bc98-16c9e828abab ==============================
[0m18:43:41.848278 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:43:41.852038 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:43:41.853171 [debug] [MainThread]: Tracking: tracking
[0m18:43:41.901954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b0efa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b0e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b0e0d0>]}
[0m18:43:42.045767 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:43:42.046160 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:43:42.073657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '98cfa2f0-06cc-4439-bc98-16c9e828abab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ccd0d0>]}
[0m18:43:42.091709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '98cfa2f0-06cc-4439-bc98-16c9e828abab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b2b0>]}
[0m18:43:42.092348 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:43:42.092954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '98cfa2f0-06cc-4439-bc98-16c9e828abab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b520>]}
[0m18:43:42.095527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c0b520>]}
[0m18:43:42.096272 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:44:02.796213 | 9ebe4e0b-35be-43b6-8aae-172357e6d5cf ==============================
[0m18:44:02.796301 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:44:02.798649 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:44:02.799164 [debug] [MainThread]: Tracking: tracking
[0m18:44:02.831392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dafd040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dafdc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dafd970>]}
[0m18:44:02.966832 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:44:02.967729 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m18:44:02.995315 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m18:44:03.043100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ebe4e0b-35be-43b6-8aae-172357e6d5cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc94d30>]}
[0m18:44:03.061771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ebe4e0b-35be-43b6-8aae-172357e6d5cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc74ee0>]}
[0m18:44:03.062726 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:44:03.063278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ebe4e0b-35be-43b6-8aae-172357e6d5cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc63100>]}
[0m18:44:03.064616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc630d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc74c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11dc74be0>]}
[0m18:44:03.065350 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:44:05.117289 | 624accb4-6457-4946-880e-477bb84aa44b ==============================
[0m18:44:05.117401 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:44:05.119349 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m18:44:05.119723 [debug] [MainThread]: Tracking: tracking
[0m18:44:05.145383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e02fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e02370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117e02790>]}
[0m18:44:05.248860 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:44:05.249392 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:44:05.266815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '624accb4-6457-4946-880e-477bb84aa44b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117fb10d0>]}
[0m18:44:05.279652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '624accb4-6457-4946-880e-477bb84aa44b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117eef310>]}
[0m18:44:05.280287 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:44:05.280894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '624accb4-6457-4946-880e-477bb84aa44b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117eef580>]}
[0m18:44:05.282288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117eef550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117eef610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x117eef040>]}
[0m18:44:05.282670 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 18:44:39.308470 | e2ad8279-847e-4c91-bf5f-5ba0387577a3 ==============================
[0m18:44:39.308614 [info ] [MainThread]: Running with dbt=1.2.1
[0m18:44:39.310264 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/marts/core/fct_orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:44:39.311018 [debug] [MainThread]: Tracking: tracking
[0m18:44:39.333664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1217b6eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1217b6310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1217b6160>]}
[0m18:44:39.423877 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:44:39.424695 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:44:39.435136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e2ad8279-847e-4c91-bf5f-5ba0387577a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12196c0d0>]}
[0m18:44:39.445115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e2ad8279-847e-4c91-bf5f-5ba0387577a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1218b35b0>]}
[0m18:44:39.445572 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:44:39.446029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2ad8279-847e-4c91-bf5f-5ba0387577a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1218b3820>]}
[0m18:44:39.448942 [info ] [MainThread]: 
[0m18:44:39.449981 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m18:44:39.451562 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m18:44:39.472858 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m18:44:39.473206 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m18:44:39.473457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:44:40.995366 [debug] [ThreadPool]: SQL status: OK in 1.52 seconds
[0m18:44:41.003449 [debug] [ThreadPool]: On list_schemas: Close
[0m18:44:41.622813 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m18:44:41.630068 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m18:44:41.630391 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m18:44:41.630684 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m18:44:41.630958 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:44:43.367247 [debug] [ThreadPool]: SQL status: OK in 1.74 seconds
[0m18:44:43.372399 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m18:44:43.372967 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m18:44:43.373238 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m18:44:43.830099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2ad8279-847e-4c91-bf5f-5ba0387577a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12179aa00>]}
[0m18:44:43.830547 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:44:43.830731 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:44:43.831092 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:44:43.831394 [info ] [MainThread]: 
[0m18:44:43.842017 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m18:44:43.842854 [info ] [Thread-1  ]: 1 of 1 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m18:44:43.843757 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m18:44:43.844458 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m18:44:43.845008 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m18:44:43.852226 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m18:44:43.854589 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:43.855087 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m18:44:43.954854 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m18:44:43.957643 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m18:44:43.957959 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m18:44:43.958155 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount,
        case 
            when orders_payments.amount > 2000
                then 'Valores altos'
            when orders_payments.amount > 3000
                then 'Valores muito altos'
            else 'Valores baixos'
        end as tipo
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m18:44:43.958427 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:44:51.554412 [debug] [Thread-1  ]: SQL status: OK in 7.6 seconds
[0m18:44:52.074861 [debug] [Thread-1  ]: finished collecting timing info
[0m18:44:52.075514 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m18:44:52.075798 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m18:44:52.075972 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m18:44:54.355800 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e2ad8279-847e-4c91-bf5f-5ba0387577a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121a71d00>]}
[0m18:44:54.356861 [info ] [Thread-1  ]: 1 of 1 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 10.51s]
[0m18:44:54.357546 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m18:44:54.359627 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m18:44:54.359978 [debug] [MainThread]: On master: ROLLBACK
[0m18:44:54.360188 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:44:56.655130 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:44:56.655884 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m18:44:56.656303 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m18:44:56.656792 [debug] [MainThread]: On master: ROLLBACK
[0m18:44:56.657642 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m18:44:56.658619 [debug] [MainThread]: On master: Close
[0m18:44:57.352496 [info ] [MainThread]: 
[0m18:44:57.353399 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 17.90 seconds (17.90s).
[0m18:44:57.355503 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:44:57.356285 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m18:44:57.377598 [info ] [MainThread]: 
[0m18:44:57.378411 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:44:57.378804 [info ] [MainThread]: 
[0m18:44:57.379102 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:44:57.379515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1219c5c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121a43e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121a44e80>]}
[0m18:44:57.379840 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 19:28:18.991299 | b4bf8194-7ade-4114-bd18-f9f0c7e6cd2d ==============================
[0m19:28:18.991408 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:28:18.994384 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['/Users/tcsilva/workspace/pessoal/estudos/dbt/jaffle_shop/models/marts/core/fct_orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:28:18.995623 [debug] [MainThread]: Tracking: tracking
[0m19:28:19.045292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d4687c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d468a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d468520>]}
[0m19:28:19.183968 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:28:19.184319 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:28:19.195261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b4bf8194-7ade-4114-bd18-f9f0c7e6cd2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6180d0>]}
[0m19:28:19.221103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b4bf8194-7ade-4114-bd18-f9f0c7e6cd2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d559430>]}
[0m19:28:19.221723 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:28:19.222481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4bf8194-7ade-4114-bd18-f9f0c7e6cd2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d559790>]}
[0m19:28:19.224076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d5e2ee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d5e2f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d5e2e50>]}
[0m19:28:19.224745 [debug] [MainThread]: Flushing usage events
[0m19:28:19.949252 [error] [MainThread]: Encountered an error:
Non-relative patterns are unsupported
[0m19:28:19.956283 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 163, in _runtime_initialize
    self.job_queue = self.get_graph_queue()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 156, in get_graph_queue
    return selector.get_graph_queue(spec)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 273, in get_graph_queue
    selected_nodes = self.get_selected(spec)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 264, in get_selected
    selected_nodes, indirect_only = self.select_nodes(spec)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 157, in select_nodes
    direct_nodes, indirect_nodes = self.select_nodes_recursively(spec)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 131, in select_nodes_recursively
    bundles = [self.select_nodes_recursively(component) for component in spec]
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 131, in <listcomp>
    bundles = [self.select_nodes_recursively(component) for component in spec]
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 131, in select_nodes_recursively
    bundles = [self.select_nodes_recursively(component) for component in spec]
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 131, in <listcomp>
    bundles = [self.select_nodes_recursively(component) for component in spec]
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 131, in select_nodes_recursively
    bundles = [self.select_nodes_recursively(component) for component in spec]
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 131, in <listcomp>
    bundles = [self.select_nodes_recursively(component) for component in spec]
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 129, in select_nodes_recursively
    direct_nodes, indirect_nodes = self.get_nodes_from_criteria(spec)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 86, in get_nodes_from_criteria
    collected = self.select_included(nodes, spec)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector.py", line 72, in select_included
    return set(method.search(included_nodes, spec.value))
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector_methods.py", line 287, in search
    paths = set(p.relative_to(root) for p in root.glob(selector))
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/graph/selector_methods.py", line 287, in <genexpr>
    paths = set(p.relative_to(root) for p in root.glob(selector))
  File "/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/pathlib.py", line 1175, in glob
    raise NotImplementedError("Non-relative patterns are unsupported")
NotImplementedError: Non-relative patterns are unsupported



============================== 2022-10-19 19:28:59.892721 | 09203b55-abd6-426d-ac74-9716ada11f03 ==============================
[0m19:28:59.892855 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:28:59.895898 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:28:59.896709 [debug] [MainThread]: Tracking: tracking
[0m19:28:59.949201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115a5ae20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115a5aeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115a5a430>]}
[0m19:29:00.113088 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:29:00.113746 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:29:00.133362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09203b55-abd6-426d-ac74-9716ada11f03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115c090d0>]}
[0m19:29:00.153919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09203b55-abd6-426d-ac74-9716ada11f03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b48340>]}
[0m19:29:00.154700 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:29:00.155246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09203b55-abd6-426d-ac74-9716ada11f03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b485b0>]}
[0m19:29:00.156822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b48580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b48640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b480a0>]}
[0m19:29:00.158199 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 19:29:51.537391 | 430d7059-b5cc-4bd1-a237-c54dea86ebad ==============================
[0m19:29:51.537481 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:29:51.540722 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/marts/core/fct_orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:29:51.541578 [debug] [MainThread]: Tracking: tracking
[0m19:29:51.579126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c5dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c5d1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c5d940>]}
[0m19:29:51.696428 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:29:51.697005 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:29:51.717421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '430d7059-b5cc-4bd1-a237-c54dea86ebad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119e0c0d0>]}
[0m19:29:51.730660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '430d7059-b5cc-4bd1-a237-c54dea86ebad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d52460>]}
[0m19:29:51.731396 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:29:51.731988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '430d7059-b5cc-4bd1-a237-c54dea86ebad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119d526d0>]}
[0m19:29:51.736507 [info ] [MainThread]: 
[0m19:29:51.737980 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:29:51.740481 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m19:29:51.766162 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:29:51.766909 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:29:51.767360 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:29:53.770233 [debug] [ThreadPool]: SQL status: OK in 2.0 seconds
[0m19:29:53.788912 [debug] [ThreadPool]: On list_schemas: Close
[0m19:29:54.529618 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m19:29:54.539505 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:29:54.539854 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m19:29:54.540098 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m19:29:54.540447 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:29:55.819822 [debug] [ThreadPool]: SQL status: OK in 1.28 seconds
[0m19:29:55.829522 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m19:29:55.830111 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:29:55.830492 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m19:29:56.276350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '430d7059-b5cc-4bd1-a237-c54dea86ebad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119dd6ee0>]}
[0m19:29:56.276959 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:29:56.277256 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:29:56.278001 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:29:56.279182 [info ] [MainThread]: 
[0m19:29:56.291087 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m19:29:56.291885 [info ] [Thread-1  ]: 1 of 1 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m19:29:56.292939 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m19:29:56.293325 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m19:29:56.293592 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m19:29:56.299765 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m19:29:56.302824 [debug] [Thread-1  ]: finished collecting timing info
[0m19:29:56.303414 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m19:29:56.410749 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m19:29:56.411907 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:29:56.412284 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m19:29:56.412642 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount,
        case 
            when orders_payments.amount > 2000
                then 'Valores altos'
            when orders_payments.amount > 3000
                then 'Valores muito altos'
            else 'Valores baixos'
        end as tipo
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m19:29:56.412870 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:30:00.580962 [debug] [Thread-1  ]: SQL status: OK in 4.17 seconds
[0m19:30:00.628338 [debug] [Thread-1  ]: finished collecting timing info
[0m19:30:00.628789 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m19:30:00.629062 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:30:00.629268 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m19:30:01.087301 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '430d7059-b5cc-4bd1-a237-c54dea86ebad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f11fa0>]}
[0m19:30:01.088908 [info ] [Thread-1  ]: 1 of 1 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.79s]
[0m19:30:01.090565 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m19:30:01.093594 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:30:01.093950 [debug] [MainThread]: On master: ROLLBACK
[0m19:30:01.094166 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:30:01.593113 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:30:01.593776 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:30:01.595700 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:30:01.596222 [debug] [MainThread]: On master: ROLLBACK
[0m19:30:01.596582 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:30:01.597204 [debug] [MainThread]: On master: Close
[0m19:30:02.064061 [info ] [MainThread]: 
[0m19:30:02.064989 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.33 seconds (10.33s).
[0m19:30:02.065598 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:30:02.065994 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m19:30:02.081410 [info ] [MainThread]: 
[0m19:30:02.081905 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:30:02.082439 [info ] [MainThread]: 
[0m19:30:02.083153 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:30:02.083883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ee14f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ee1520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ee4910>]}
[0m19:30:02.084386 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 19:31:29.159023 | c4cb5082-8e8f-458b-a592-b28821430f5e ==============================
[0m19:31:29.159386 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:31:29.162952 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/marts/core/fct_orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:31:29.163935 [debug] [MainThread]: Tracking: tracking
[0m19:31:29.205377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f796a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f79a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f79520>]}
[0m19:31:29.355667 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:31:29.356301 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:31:29.378799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c4cb5082-8e8f-458b-a592-b28821430f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1161280d0>]}
[0m19:31:29.397489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c4cb5082-8e8f-458b-a592-b28821430f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116067520>]}
[0m19:31:29.398449 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:31:29.399867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4cb5082-8e8f-458b-a592-b28821430f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116067790>]}
[0m19:31:29.405954 [info ] [MainThread]: 
[0m19:31:29.408346 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:31:29.410915 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m19:31:29.427122 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:31:29.427504 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:31:29.427812 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:31:30.658181 [debug] [ThreadPool]: SQL status: OK in 1.23 seconds
[0m19:31:30.669517 [debug] [ThreadPool]: On list_schemas: Close
[0m19:31:31.144518 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m19:31:31.161544 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:31:31.162070 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m19:31:31.162611 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m19:31:31.162958 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:31:32.563417 [debug] [ThreadPool]: SQL status: OK in 1.4 seconds
[0m19:31:32.573393 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m19:31:32.573811 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:31:32.574116 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m19:31:33.030622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4cb5082-8e8f-458b-a592-b28821430f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115f55100>]}
[0m19:31:33.031242 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:31:33.031535 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:31:33.032268 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:31:33.032806 [info ] [MainThread]: 
[0m19:31:33.041796 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m19:31:33.042404 [info ] [Thread-1  ]: 1 of 1 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m19:31:33.043273 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m19:31:33.043585 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m19:31:33.043906 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m19:31:33.052116 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m19:31:33.053996 [debug] [Thread-1  ]: finished collecting timing info
[0m19:31:33.054800 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m19:31:33.184159 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m19:31:33.185049 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:31:33.185383 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m19:31:33.185635 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount,
        case 
            when orders_payments.amount > 2000
                then 'Valores altos'
            when orders_payments.amount > 3000
                then 'Valores muito altos'
            else 'Valores baixos'
        end as tipo
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m19:31:33.185859 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:31:36.975067 [debug] [Thread-1  ]: SQL status: OK in 3.79 seconds
[0m19:31:37.022065 [debug] [Thread-1  ]: finished collecting timing info
[0m19:31:37.022438 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m19:31:37.022763 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:31:37.023060 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m19:31:37.696166 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4cb5082-8e8f-458b-a592-b28821430f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11622ec70>]}
[0m19:31:37.697065 [info ] [Thread-1  ]: 1 of 1 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 4.65s]
[0m19:31:37.698486 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m19:31:37.702788 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:31:37.703345 [debug] [MainThread]: On master: ROLLBACK
[0m19:31:37.703787 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:31:39.183039 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:31:39.183521 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:31:39.184121 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:31:39.184766 [debug] [MainThread]: On master: ROLLBACK
[0m19:31:39.185377 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:31:39.186003 [debug] [MainThread]: On master: Close
[0m19:31:39.629140 [info ] [MainThread]: 
[0m19:31:39.630235 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.22 seconds (10.22s).
[0m19:31:39.631511 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:31:39.632226 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m19:31:39.646704 [info ] [MainThread]: 
[0m19:31:39.648122 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:31:39.648806 [info ] [MainThread]: 
[0m19:31:39.649631 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:31:39.650668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x116182b80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1161bc4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1161fef70>]}
[0m19:31:39.651335 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 19:32:37.573040 | 05051a3a-9aa7-4e3e-90d6-77d3a1fa9bee ==============================
[0m19:32:37.573115 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:32:37.574918 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:32:37.575347 [debug] [MainThread]: Tracking: tracking
[0m19:32:37.608723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d65fc40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d65fe20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d65fa30>]}
[0m19:32:37.744944 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:32:37.746555 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m19:32:37.783973 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m19:32:37.821565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7faf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d74b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d830460>]}
[0m19:32:37.822449 [debug] [MainThread]: Flushing usage events
[0m19:32:38.501955 [error] [MainThread]: Encountered an error:
Compilation Error in model fct_orders (models/marts/core/fct_orders.sql)
  Model 'model.jaffle_shop.fct_orders' (models/marts/core/fct_orders.sql) depends on a node named 'stg_paymen' which was not found
[0m19:32:38.510648 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/list.py", line 135, in run
    ManifestTask._runtime_initialize(self)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 391, in load
    self.process_refs(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 830, in process_refs
    _process_refs_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1230, in _process_refs_for_node
    invalid_ref_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 939, in invalid_ref_fail_unless_test
    ref_target_not_found(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 631, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model fct_orders (models/marts/core/fct_orders.sql)
  Model 'model.jaffle_shop.fct_orders' (models/marts/core/fct_orders.sql) depends on a node named 'stg_paymen' which was not found



============================== 2022-10-19 19:32:52.291045 | fd73c8b2-73f1-4e8c-8873-245abca07c2f ==============================
[0m19:32:52.291128 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:32:52.304871 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/marts/core/fct_orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:32:52.322420 [debug] [MainThread]: Tracking: tracking
[0m19:32:52.355699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b7c550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b7ca30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b7c520>]}
[0m19:32:52.525109 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:32:52.530083 [debug] [MainThread]: Partial parsing: updated file: jaffle_shop://models/marts/core/fct_orders.sql
[0m19:32:52.582661 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m19:32:52.614696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121cecac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c6d910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121d2f130>]}
[0m19:32:52.615971 [debug] [MainThread]: Flushing usage events
[0m19:32:53.222483 [error] [MainThread]: Encountered an error:
Compilation Error in model fct_orders (models/marts/core/fct_orders.sql)
  Model 'model.jaffle_shop.fct_orders' (models/marts/core/fct_orders.sql) depends on a node named 'stg_paymen' which was not found
[0m19:32:53.225006 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 129, in main
    results, succeeded = handle_and_check(args)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 191, in handle_and_check
    task, res = run_from_args(parsed)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/main.py", line 238, in run_from_args
    results = task.run()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 451, in run
    self._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 159, in _runtime_initialize
    super()._runtime_initialize()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/task/runnable.py", line 81, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 219, in get_full_manifest
    manifest = loader.load()
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 391, in load
    self.process_refs(self.root_project.project_name)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 830, in process_refs
    _process_refs_for_node(self.manifest, current_project, node)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 1230, in _process_refs_for_node
    invalid_ref_fail_unless_test(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/parser/manifest.py", line 939, in invalid_ref_fail_unless_test
    ref_target_not_found(
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 631, in ref_target_not_found
    raise_compiler_error(msg, model)
  File "/Users/tcsilva/.virtualenvs/dbt/lib/python3.9/site-packages/dbt/exceptions.py", line 445, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model fct_orders (models/marts/core/fct_orders.sql)
  Model 'model.jaffle_shop.fct_orders' (models/marts/core/fct_orders.sql) depends on a node named 'stg_paymen' which was not found



============================== 2022-10-19 19:33:41.129951 | 14ae8284-0285-4c98-ab16-d8573e1631c3 ==============================
[0m19:33:41.130018 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:33:41.132083 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'select': ['models/marts/core/fct_orders.sql'], 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:33:41.132540 [debug] [MainThread]: Tracking: tracking
[0m19:33:41.168729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120806970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120806a30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120806520>]}
[0m19:33:41.303175 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:33:41.303865 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:33:41.319418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14ae8284-0285-4c98-ab16-d8573e1631c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1209b50d0>]}
[0m19:33:41.335065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14ae8284-0285-4c98-ab16-d8573e1631c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208f64f0>]}
[0m19:33:41.335984 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:33:41.336967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14ae8284-0285-4c98-ab16-d8573e1631c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1208f6760>]}
[0m19:33:41.341723 [info ] [MainThread]: 
[0m19:33:41.342843 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:33:41.345396 [debug] [ThreadPool]: Acquiring new databricks connection "list_schemas"
[0m19:33:41.368496 [debug] [ThreadPool]: Using databricks connection "list_schemas"
[0m19:33:41.369248 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  
[0m19:33:41.369587 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:33:42.726404 [debug] [ThreadPool]: SQL status: OK in 1.36 seconds
[0m19:33:42.741234 [debug] [ThreadPool]: On list_schemas: Close


============================== 2022-10-19 19:33:43.446258 | 96a6cc19-e4a6-4226-9401-3d37ea7172b5 ==============================
[0m19:33:43.446524 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:33:43.449417 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m19:33:43.450250 [debug] [MainThread]: Tracking: tracking
[0m19:33:43.475162 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m19:33:43.491933 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:33:43.492466 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m19:33:43.492971 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m19:33:43.493867 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:33:43.491781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11834ef40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11834e2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11834eca0>]}
[0m19:33:43.629769 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:33:43.630327 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:33:43.646102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '96a6cc19-e4a6-4226-9401-3d37ea7172b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11850d0d0>]}
[0m19:33:43.668168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '96a6cc19-e4a6-4226-9401-3d37ea7172b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11844b2e0>]}
[0m19:33:43.669320 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:33:43.671576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96a6cc19-e4a6-4226-9401-3d37ea7172b5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11844b550>]}
[0m19:33:43.673308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11844b520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11844b5e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11844b0d0>]}
[0m19:33:43.673822 [debug] [MainThread]: Flushing usage events
[0m19:33:44.840560 [debug] [ThreadPool]: SQL status: OK in 1.35 seconds
[0m19:33:44.849442 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m19:33:44.850045 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m19:33:44.850640 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m19:33:45.309490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14ae8284-0285-4c98-ab16-d8573e1631c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207e2910>]}
[0m19:33:45.310271 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:33:45.310883 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:33:45.311781 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:33:45.312772 [info ] [MainThread]: 
[0m19:33:45.326970 [debug] [Thread-1  ]: Began running node model.jaffle_shop.fct_orders
[0m19:33:45.327608 [info ] [Thread-1  ]: 1 of 1 START table model jaffle_shop_dbt.fct_orders ............................ [RUN]
[0m19:33:45.329805 [debug] [Thread-1  ]: Acquiring new databricks connection "model.jaffle_shop.fct_orders"
[0m19:33:45.330416 [debug] [Thread-1  ]: Began compiling node model.jaffle_shop.fct_orders
[0m19:33:45.331617 [debug] [Thread-1  ]: Compiling model.jaffle_shop.fct_orders
[0m19:33:45.337385 [debug] [Thread-1  ]: Writing injected SQL for node "model.jaffle_shop.fct_orders"
[0m19:33:45.339528 [debug] [Thread-1  ]: finished collecting timing info
[0m19:33:45.340393 [debug] [Thread-1  ]: Began executing node model.jaffle_shop.fct_orders
[0m19:33:45.493969 [debug] [Thread-1  ]: Writing runtime SQL for node "model.jaffle_shop.fct_orders"
[0m19:33:45.495279 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m19:33:45.495621 [debug] [Thread-1  ]: Using databricks connection "model.jaffle_shop.fct_orders"
[0m19:33:45.495873 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "model.jaffle_shop.fct_orders"} */

      create or replace table jaffle_shop_dbt.fct_orders
    
    
    using delta
    
    
    
    
    
    
    as
      with orders as (

    select * from jaffle_shop_dbt.stg_orders

),
payments as (

    select * from jaffle_shop_dbt.stg_payments

),
orders_customer as (
    select 
        order_id,
        customer_id
    from orders
),
orders_payments as (
    select 
        order_id,
        amount
    from payments
),
final as (
    select 
        orders_customer.order_id, 
        orders_customer.customer_id,
        orders_payments.amount,
        case 
            when orders_payments.amount > 2000
                then 'Valores altos'
            when orders_payments.amount > 3000
                then 'Valores muito altos'
            else 'Valores baixos'
        end as tipo
    from orders_payments
    left join orders_customer using (order_id)
)

select * from final
[0m19:33:45.496201 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:33:50.394404 [debug] [Thread-1  ]: SQL status: OK in 4.9 seconds
[0m19:33:50.460716 [debug] [Thread-1  ]: finished collecting timing info
[0m19:33:50.461749 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: ROLLBACK
[0m19:33:50.462410 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m19:33:50.463086 [debug] [Thread-1  ]: On model.jaffle_shop.fct_orders: Close
[0m19:33:50.949866 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14ae8284-0285-4c98-ab16-d8573e1631c3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120abbfd0>]}
[0m19:33:50.952062 [info ] [Thread-1  ]: 1 of 1 OK created table model jaffle_shop_dbt.fct_orders ....................... [[32mOK[0m in 5.62s]
[0m19:33:50.954554 [debug] [Thread-1  ]: Finished running node model.jaffle_shop.fct_orders
[0m19:33:50.958171 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:33:50.958786 [debug] [MainThread]: On master: ROLLBACK
[0m19:33:50.959313 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:33:51.516510 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:33:51.516975 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:33:51.517251 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:33:51.517525 [debug] [MainThread]: On master: ROLLBACK
[0m19:33:51.517838 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:33:51.518143 [debug] [MainThread]: On master: Close
[0m19:33:52.000728 [info ] [MainThread]: 
[0m19:33:52.001754 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.66 seconds (10.66s).
[0m19:33:52.002953 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:33:52.003444 [debug] [MainThread]: Connection 'model.jaffle_shop.fct_orders' was properly closed.
[0m19:33:52.024056 [info ] [MainThread]: 
[0m19:33:52.024624 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:33:52.025124 [info ] [MainThread]: 
[0m19:33:52.025690 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:33:52.026421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a0fb80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a8a4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120a8d250>]}
[0m19:33:52.027220 [debug] [MainThread]: Flushing usage events


============================== 2022-10-19 21:15:46.144404 | b1519066-418e-467a-ab14-730a1398327b ==============================
[0m21:15:46.144600 [info ] [MainThread]: Running with dbt=1.2.1
[0m21:15:46.146174 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'resource_types': [], 'output': 'selector', 'indirect_selection': 'eager', 'which': 'list'}
[0m21:15:46.146639 [debug] [MainThread]: Tracking: tracking
[0m21:15:46.178220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183bfbb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183bf8b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183bfb20>]}
[0m21:15:46.238966 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m21:15:46.239615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b1519066-418e-467a-ab14-730a1398327b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183988e0>]}
[0m21:15:46.310142 [debug] [MainThread]: Parsing macros/statement.sql
[0m21:15:46.316127 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:15:46.321511 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:15:46.369418 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:15:46.382148 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:15:46.383382 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:15:46.390968 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:15:46.427266 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m21:15:46.439817 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:15:46.528244 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m21:15:46.534765 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:15:46.559415 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:15:46.560719 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:15:46.567627 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:15:46.621809 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m21:15:46.631342 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m21:15:46.647370 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m21:15:46.669472 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:15:46.671769 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m21:15:46.675071 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:15:46.687198 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:15:46.690106 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:15:46.727671 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:15:46.730055 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:15:46.731909 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:15:46.739457 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:15:46.754219 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:15:46.759917 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:15:46.769657 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:15:46.836763 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:15:46.875882 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:15:46.904197 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:15:46.915318 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:15:46.919174 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:15:46.922931 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:15:46.931519 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:15:46.964695 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:15:46.967249 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:15:46.985969 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:15:47.017599 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:15:47.025662 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:15:47.030293 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:15:47.039254 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:15:47.041413 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:15:47.048966 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:15:47.055588 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:15:47.068164 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:15:47.105084 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:15:47.109324 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:15:47.113203 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:15:47.116282 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:15:47.117916 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:15:47.119594 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:15:47.120783 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:15:47.122885 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:15:47.130154 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:15:47.149353 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:15:47.152206 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:15:47.158131 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:15:47.161101 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:15:47.164343 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:15:47.166700 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:15:47.168372 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:15:47.170205 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:15:47.172036 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:15:47.175945 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:15:47.178410 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:15:47.180751 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:15:47.183051 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:15:47.185433 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:15:47.186963 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:15:47.189360 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:15:47.191337 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:15:47.207173 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:15:47.210171 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:15:47.212804 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:15:47.216161 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:15:47.217793 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:15:47.221767 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:15:47.226614 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:15:47.260962 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:15:47.265930 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:15:47.292495 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:15:47.308549 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:15:47.324498 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:15:47.341978 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:15:48.136157 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_customers.sql
[0m21:15:48.176400 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/customers.sql
[0m21:15:48.184027 [debug] [MainThread]: 1699: static parser successfully parsed staging/jaffle_shop/stg_orders.sql
[0m21:15:48.193970 [debug] [MainThread]: 1699: static parser successfully parsed staging/stripe/stg_payments.sql
[0m21:15:48.204414 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m21:15:48.213741 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m21:15:48.601945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b1519066-418e-467a-ab14-730a1398327b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185d90d0>]}
[0m21:15:48.624130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b1519066-418e-467a-ab14-730a1398327b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1184bc850>]}
[0m21:15:48.624846 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m21:15:48.625277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b1519066-418e-467a-ab14-730a1398327b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11858e0a0>]}
[0m21:15:48.626610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185cacd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118473a00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11856c580>]}
[0m21:15:48.627447 [debug] [MainThread]: Flushing usage events


============================== 2022-10-26 19:01:58.847021 | e5bea63b-2d5c-4232-82cb-9daa7351f00a ==============================
[0m19:01:58.847096 [info ] [MainThread]: Running with dbt=1.2.1
[0m19:01:58.849743 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m19:01:58.850448 [debug] [MainThread]: Tracking: tracking
[0m19:01:58.888983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d056e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d056c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d056130>]}
[0m19:01:59.056085 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:01:59.056739 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:01:59.075305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e5bea63b-2d5c-4232-82cb-9daa7351f00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d20b0d0>]}
[0m19:01:59.096723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e5bea63b-2d5c-4232-82cb-9daa7351f00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d156580>]}
[0m19:01:59.097303 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 321 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m19:01:59.097766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e5bea63b-2d5c-4232-82cb-9daa7351f00a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d1567f0>]}
[0m19:01:59.101115 [info ] [MainThread]: 
[0m19:01:59.102286 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m19:01:59.103843 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m19:01:59.131930 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:01:59.132477 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m19:01:59.132969 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.2.1", "dbt_databricks_version": "1.2.3", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m19:01:59.133224 [debug] [ThreadPool]: Opening a new connection, currently in state init


============================== 2022-11-09 14:44:37.495996 | 85d6d6d5-9c3f-4da1-8c6a-c66ef802d08b ==============================
[0m14:44:37.496052 [info ] [MainThread]: Running with dbt=1.3.0
[0m14:44:37.497749 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/tcsilva/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m14:44:37.498299 [debug] [MainThread]: Tracking: tracking
[0m14:44:37.519959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b30fb20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b30fca0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b30fc40>]}
[0m14:44:37.630738 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:44:37.631085 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:44:37.639546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '85d6d6d5-9c3f-4da1-8c6a-c66ef802d08b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b4efdf0>]}
[0m14:44:37.651221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '85d6d6d5-9c3f-4da1-8c6a-c66ef802d08b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b3fcd30>]}
[0m14:44:37.651581 [info ] [MainThread]: Found 6 models, 7 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m14:44:37.651871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85d6d6d5-9c3f-4da1-8c6a-c66ef802d08b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b3fcdc0>]}
[0m14:44:37.653737 [info ] [MainThread]: 
[0m14:44:37.654986 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:44:37.656843 [debug] [ThreadPool]: Acquiring new databricks connection "list_None_jaffle_shop_dbt"
[0m14:44:37.674142 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:37.674545 [debug] [ThreadPool]: Using databricks connection "list_None_jaffle_shop_dbt"
[0m14:44:37.674906 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "connection_name": "list_None_jaffle_shop_dbt"} */
show table extended in jaffle_shop_dbt like '*'
  
[0m14:44:37.675104 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:39.744972 [debug] [ThreadPool]: SQL status: OK in 2.07 seconds
[0m14:44:39.755025 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: ROLLBACK
[0m14:44:39.755324 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:44:39.755639 [debug] [ThreadPool]: On list_None_jaffle_shop_dbt: Close
[0m14:44:40.308922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85d6d6d5-9c3f-4da1-8c6a-c66ef802d08b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b3fcb20>]}
[0m14:44:40.309382 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:40.309672 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:44:40.310750 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:44:40.311279 [info ] [MainThread]: 
[0m14:44:40.318173 [debug] [Thread-1  ]: Began running node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m14:44:40.318476 [info ] [Thread-1  ]: 1 of 7 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m14:44:40.319312 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m14:44:40.319669 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m14:44:40.319926 [debug] [Thread-1  ]: Compiling test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m14:44:40.344331 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m14:44:40.345911 [debug] [Thread-1  ]: finished collecting timing info
[0m14:44:40.346229 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m14:44:40.366855 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m14:44:40.368040 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:40.368254 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m14:44:40.368411 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from jaffle_shop_dbt.stg_orders
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m14:44:40.368600 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:44:52.698614 [debug] [Thread-1  ]: SQL status: OK in 12.33 seconds
[0m14:44:53.157698 [debug] [Thread-1  ]: finished collecting timing info
[0m14:44:53.158213 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: ROLLBACK
[0m14:44:53.158642 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:44:53.158892 [debug] [Thread-1  ]: On test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: Close
[0m14:44:53.602853 [info ] [Thread-1  ]: 1 of 7 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 13.28s]
[0m14:44:53.603663 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m14:44:53.604242 [debug] [Thread-1  ]: Began running node test.jaffle_shop.assert_positive_value_for_total_amount
[0m14:44:53.604667 [info ] [Thread-1  ]: 2 of 7 START test assert_positive_value_for_total_amount ....................... [RUN]
[0m14:44:53.605507 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.assert_positive_value_for_total_amount"
[0m14:44:53.605774 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.assert_positive_value_for_total_amount
[0m14:44:53.605960 [debug] [Thread-1  ]: Compiling test.jaffle_shop.assert_positive_value_for_total_amount
[0m14:44:53.612795 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.assert_positive_value_for_total_amount"
[0m14:44:53.613496 [debug] [Thread-1  ]: finished collecting timing info
[0m14:44:53.613895 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.assert_positive_value_for_total_amount
[0m14:44:53.620558 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.assert_positive_value_for_total_amount"
[0m14:44:53.621424 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:53.621782 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.assert_positive_value_for_total_amount"
[0m14:44:53.621999 [debug] [Thread-1  ]: On test.jaffle_shop.assert_positive_value_for_total_amount: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.assert_positive_value_for_total_amount"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      -- Refunds have a negative amount, so the total amount should always be >= 0.
-- Therefore return records where this isn't true to make the test fail.
select
  order_id,
	sum(amount) as total_amount
from jaffle_shop_dbt.stg_payments
group by 1
having not(total_amount >= 0)
      
    ) dbt_internal_test
[0m14:44:53.622365 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:44:56.231073 [debug] [Thread-1  ]: SQL status: OK in 2.61 seconds
[0m14:44:56.233845 [debug] [Thread-1  ]: finished collecting timing info
[0m14:44:56.234122 [debug] [Thread-1  ]: On test.jaffle_shop.assert_positive_value_for_total_amount: ROLLBACK
[0m14:44:56.234275 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:44:56.234401 [debug] [Thread-1  ]: On test.jaffle_shop.assert_positive_value_for_total_amount: Close
[0m14:44:56.688350 [info ] [Thread-1  ]: 2 of 7 PASS assert_positive_value_for_total_amount ............................. [[32mPASS[0m in 3.08s]
[0m14:44:56.689051 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.assert_positive_value_for_total_amount
[0m14:44:56.689591 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:56.689985 [info ] [Thread-1  ]: 3 of 7 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m14:44:56.690638 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:44:56.690798 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:56.690938 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:56.701581 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:44:56.702421 [debug] [Thread-1  ]: finished collecting timing info
[0m14:44:56.702807 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:44:56.706129 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:44:56.707033 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:44:56.707477 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m14:44:56.707923 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from jaffle_shop_dbt.stg_customers
where customer_id is null



      
    ) dbt_internal_test
[0m14:44:56.708329 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:45:00.514107 [debug] [Thread-1  ]: SQL status: OK in 3.81 seconds
[0m14:45:00.517803 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:00.518298 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: ROLLBACK
[0m14:45:00.518680 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:45:00.518964 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa: Close
[0m14:45:00.965644 [info ] [Thread-1  ]: 3 of 7 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 4.28s]
[0m14:45:00.966732 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m14:45:00.967364 [debug] [Thread-1  ]: Began running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:00.967841 [info ] [Thread-1  ]: 4 of 7 START test not_null_stg_orders_order_id ................................. [RUN]
[0m14:45:00.969227 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:45:00.969576 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:00.969947 [debug] [Thread-1  ]: Compiling test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:00.980268 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:45:00.980902 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:00.981180 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:00.985153 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:45:00.985872 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:00.986176 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"
[0m14:45:00.986426 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from jaffle_shop_dbt.stg_orders
where order_id is null



      
    ) dbt_internal_test
[0m14:45:00.986560 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:45:04.753553 [debug] [Thread-1  ]: SQL status: OK in 3.77 seconds
[0m14:45:04.756740 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:04.757029 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: ROLLBACK
[0m14:45:04.757222 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:45:04.757363 [debug] [Thread-1  ]: On test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64: Close
[0m14:45:05.333357 [info ] [Thread-1  ]: 4 of 7 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 4.36s]
[0m14:45:05.334114 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.not_null_stg_orders_order_id.81cfe2fe64
[0m14:45:05.334864 [debug] [Thread-1  ]: Began running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:45:05.335322 [info ] [Thread-1  ]: 5 of 7 START test relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [RUN]
[0m14:45:05.336341 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:45:05.336614 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:45:05.336783 [debug] [Thread-1  ]: Compiling test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:45:05.348686 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:45:05.349320 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:05.349777 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:45:05.353045 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:45:05.353654 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:05.353854 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"
[0m14:45:05.354021 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select customer_id as from_field
    from jaffle_shop_dbt.stg_orders
    where customer_id is not null
),

parent as (
    select customer_id as to_field
    from jaffle_shop_dbt.stg_customers
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m14:45:05.354218 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:45:07.500424 [debug] [Thread-1  ]: SQL status: OK in 2.15 seconds
[0m14:45:07.503627 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:07.503940 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: ROLLBACK
[0m14:45:07.504165 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:45:07.504307 [debug] [Thread-1  ]: On test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500: Close
[0m14:45:07.946834 [info ] [Thread-1  ]: 5 of 7 PASS relationships_stg_orders_customer_id__customer_id__ref_stg_customers_  [[32mPASS[0m in 2.61s]
[0m14:45:07.947441 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.relationships_stg_orders_customer_id__customer_id__ref_stg_customers_.430bf21500
[0m14:45:07.947881 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:07.948209 [info ] [Thread-1  ]: 6 of 7 START test unique_stg_customers_customer_id ............................. [RUN]
[0m14:45:07.948900 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:45:07.949069 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:07.949214 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:07.957838 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:45:07.958664 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:07.959024 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:07.963547 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:45:07.964262 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:07.964524 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"
[0m14:45:07.964687 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    customer_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_customers
where customer_id is not null
group by customer_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:45:07.964822 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:45:10.005189 [debug] [Thread-1  ]: SQL status: OK in 2.04 seconds
[0m14:45:10.009258 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:10.009688 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: ROLLBACK
[0m14:45:10.009925 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:45:10.010190 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_customers_customer_id.c7614daada: Close
[0m14:45:10.475285 [info ] [Thread-1  ]: 6 of 7 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 2.53s]
[0m14:45:10.476362 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_customers_customer_id.c7614daada
[0m14:45:10.476899 [debug] [Thread-1  ]: Began running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:10.477467 [info ] [Thread-1  ]: 7 of 7 START test unique_stg_orders_order_id ................................... [RUN]
[0m14:45:10.478901 [debug] [Thread-1  ]: Acquiring new databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:45:10.479336 [debug] [Thread-1  ]: Began compiling node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:10.479686 [debug] [Thread-1  ]: Compiling test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:10.488238 [debug] [Thread-1  ]: Writing injected SQL for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:45:10.488787 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:10.489012 [debug] [Thread-1  ]: Began executing node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:10.492390 [debug] [Thread-1  ]: Writing runtime sql for node "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:45:10.493020 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:10.493198 [debug] [Thread-1  ]: Using databricks connection "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"
[0m14:45:10.493351 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.0.5", "profile_name": "jaffle_shop", "target_name": "dev", "node_id": "test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from jaffle_shop_dbt.stg_orders
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:45:10.493481 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:45:11.877203 [debug] [Thread-1  ]: SQL status: OK in 1.38 seconds
[0m14:45:11.882056 [debug] [Thread-1  ]: finished collecting timing info
[0m14:45:11.882377 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: ROLLBACK
[0m14:45:11.882559 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:45:11.882705 [debug] [Thread-1  ]: On test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a: Close
[0m14:45:12.339511 [info ] [Thread-1  ]: 7 of 7 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.86s]
[0m14:45:12.340118 [debug] [Thread-1  ]: Finished running node test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a
[0m14:45:12.341731 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:45:12.342147 [debug] [MainThread]: On master: ROLLBACK
[0m14:45:12.342400 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:45:12.797343 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:45:12.797655 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:45:12.797822 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:45:12.797979 [debug] [MainThread]: On master: ROLLBACK
[0m14:45:12.798179 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:45:12.798338 [debug] [MainThread]: On master: Close
[0m14:45:13.238417 [info ] [MainThread]: 
[0m14:45:13.239167 [info ] [MainThread]: Finished running 7 tests in 0 hours 0 minutes and 35.58 seconds (35.58s).
[0m14:45:13.239670 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:45:13.239986 [debug] [MainThread]: Connection 'test.jaffle_shop.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m14:45:13.253027 [info ] [MainThread]: 
[0m14:45:13.253446 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:45:13.253753 [info ] [MainThread]: 
[0m14:45:13.254039 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:45:13.254350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b67f7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b5d63d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11b339b50>]}
[0m14:45:13.254894 [debug] [MainThread]: Flushing usage events
